{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lawrencechen/fun/autochapter/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import whisper\n",
        "import os\n",
        "import json\n",
        "import cohere\n",
        "import httpx\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "COHERE_API_KEY = \"8rLIrm60Gf9mSmeIF2RHxv1TNcjXpHFDZ9XnqjD2\"\n",
        "cohere = cohere.Client(COHERE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def timestamps2summaries(video_url: str, timestamps: List[int], output_dir: str):\n",
        "#   transcribed = model.transcribe(video_url)\n",
        "#   print(transcribed)\n",
        "#   # transcribed[\"timestamps\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Whisper\n",
            "Loaded Whisper\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading Whisper\")\n",
        "model = whisper.load_model(\"tiny\")\n",
        "print(\"Loaded Whisper\")\n",
        "\n",
        "def transcribe_video(video_url: str):\n",
        "  video_transcribed_path = os.path.join(\"../transcribed\", video_url.split(\"/\")[-1].split(\".\")[0] + \".json\")\n",
        "  print(video_transcribed_path)\n",
        "  if os.path.exists(video_transcribed_path):\n",
        "    with open(video_transcribed_path) as f:\n",
        "      return json.load(f)\n",
        "  else:\n",
        "    result = model.transcribe(video_url)\n",
        "    with open(video_transcribed_path, \"w\") as f:\n",
        "      json.dump(result, f)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# transcribed = transcribe_video(\"videos/cs61a_lec1.mkv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import wraps, partial\n",
        "\n",
        "def summarize(passage: str):\n",
        "  response = cohere.generate(\n",
        "    model='large',\n",
        "    prompt=f'Passage: Is Wordle getting tougher to solve? Players seem to be convinced that the game has gotten harder in recent weeks ever since The New York Times bought it from developer Josh Wardle in late January. The Times has come forward and shared that this likely isn’t the case. That said, the NYT did mess with the back end code a bit, removing some offensive and sexual language, as well as some obscure words There is a viral thread claiming that a confirmation bias was at play. One Twitter user went so far as to claim the game has gone to “the dusty section of the dictionary” to find its latest words.\\n\\nTLDR: Wordle has not gotten more difficult to solve.\\n--\\nPassage: ArtificialIvan, a seven-year-old, London-based payment and expense management software company, has raised $190 million in Series C funding led by ARG Global, with participation from D9 Capital Group and Boulder Capital. Earlier backers also joined the round, including Hilton Group, Roxanne Capital, Paved Roads Ventures, Brook Partners, and Plato Capital.\\n\\nTLDR: ArtificialIvan has raised $190 million in Series C funding.\\n--\\nPassage: {passage}\\n\\nTLDR:',\n",
        "    max_tokens=50,\n",
        "    temperature=0.8,\n",
        "    k=0,\n",
        "    p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop_sequences=[\"--\"],\n",
        "    return_likelihoods='NONE')\n",
        "  # print('Prediction: {}'.format(response.generations[0].text))\n",
        "  return response.generations[0].text\n",
        "\n",
        "def async_wrap(func):\n",
        "    @wraps(func)\n",
        "    async def run(*args, loop=None, executor=None, **kwargs):\n",
        "        if loop is None:\n",
        "            loop = asyncio.get_event_loop()\n",
        "        pfunc = partial(func, *args, **kwargs)\n",
        "        return await loop.run_in_executor(executor, pfunc)\n",
        "    return run \n",
        "\n",
        "@async_wrap\n",
        "def async_summarize(passage: str):\n",
        "  return summarize(passage).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize(transcribed[\"text\"][0:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_async(url):\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        return await client.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all units should be in seconds\n",
        "def get_pre_summarized_segments(transcribed, timestamps: List[int], max_duration=300):\n",
        "  pre_summarized_segments = []\n",
        "  ts = 0\n",
        "  max_ts = timestamps[-1] + max_duration\n",
        "  i_timestamp = 0\n",
        "  i_segment = 0\n",
        "  segments = transcribed[\"segments\"]\n",
        "\n",
        "  while ts < max_ts and i_segment < len(segments) and i_timestamp < len(timestamps):\n",
        "    segment = segments[i_segment]\n",
        "    next_segment = segments[i_segment + 1] if i_segment + 1 < len(segments) else None\n",
        "    segment_start = segment[\"start\"]\n",
        "\n",
        "    if len(pre_summarized_segments) <= i_timestamp:\n",
        "      pre_summarized_segments.append(\"\")\n",
        "    \n",
        "    if next_segment is None:\n",
        "      pre_summarized_segments[i_timestamp] += segment[\"text\"]\n",
        "    else:\n",
        "      if segment_start <= timestamps[i_timestamp] + max_duration and next_segment[\"start\"] > timestamps[i_timestamp]:\n",
        "        pre_summarized_segments[i_timestamp] += segment[\"text\"]\n",
        "      else:\n",
        "        i_timestamp += 1\n",
        "      ts = next_segment[\"start\"]\n",
        "    i_segment += 1\n",
        "\n",
        "  stripped_segments = [segment.strip() for segment in pre_summarized_segments]\n",
        "\n",
        "  return stripped_segments\n",
        "\n",
        "async def summarize_segments(segments: List[str]):\n",
        "  summaries = await asyncio.gather(*map(async_summarize, segments))\n",
        "  return summaries\n",
        "\n",
        "# segments = get_pre_summarized_segments(transcribed, fake_timestamps)\n",
        "# summaries = await summarize_segments(segments)\n",
        "# summaries\n",
        "# summaries = [summarize(segment) for segment in segments]\n",
        "# summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'transcribed' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m segments \u001b[39m=\u001b[39m get_pre_summarized_segments(transcribed, fake_timestamps)\n\u001b[1;32m      2\u001b[0m summaries \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m summarize_segments(segments)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transcribed' is not defined"
          ]
        }
      ],
      "source": [
        "transcribed = transcribe_video(\"videos/cs61a_lec1.mkv\")\n",
        "fake_timestamps = [0, 791]\n",
        "segments = get_pre_summarized_segments(transcribed, fake_timestamps)\n",
        "summaries = await summarize_segments(segments)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9f3691336094409b7a2e6da62f7c07d63be3cbfcc8c3e222f01ef3baf293cba1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
