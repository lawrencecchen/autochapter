{"text": " Okay, sounds like it is. I'll be telling you about adversarial examples and adversarial training today. I think you. As an overview, I'll start off by telling you what adversarial examples are. Then I'll explain why they happen, why it's possible for them to exist. I'll talk a little bit about how adversarial examples pose real world security threats, that they can actually be used to compromise systems build on machine learning. I'll tell you what the defenses are so far, but mostly defenses are an open research problem that I hope some of you will move on to tackle. And then finally, I'll tell you how to use adversarial examples to improve other machine learning algorithms, even if you want to build a machine learning algorithm that won't face a real world adversary. Look at the big picture and the context for this lecture. I think most of you are probably here because you've heard how incredibly powerful and successful machine learning is. The very many different tasks that could not be solved with software before are now solvable things to deep learning and convolutional networks and gradient descent. All of these technologies that are working really well. Well, until just a few years ago, these technologies didn't really work. In about 2013, we started to see the deep learning achieved human level performance at a lot of different tasks. We saw that convolutional nets could recognize objects and images and score about the same as people in those benchmarks. With the caveat, the part of the reason that algorithm score as well as people is that people can't tell a less than Huskies from Siberian Huskies very well. But modular, the strangeness of the benchmarks deep learning caught up to about human level performance for object recognition in about 2013. The same year, we also saw that object recognition applied to human faces caught up to about human level. That suddenly we had computers that could recognize faces about as well as you or I could recognize faces of strangers. You can recognize the faces of your friends and family better than a computer. But when you're dealing with people that you haven't had a lot of experience with the computer caught up to us in about 2013. We also saw that computers caught up to humans for reading typewritten fonts in photos in about 2013. It even got to the point that we can no longer use captures to tell whether a user of a web page is human or not, because the convolutional network is better at reading up-he-scated text than a human is. So with this context today of deep learning working really well, especially for computer vision, it's a little bit unusual to think about the computer making a mistake. Before about 2013, nobody was ever surprised if the computer made a mistake. That was the rule, not the exception. And so today's topic, which is all about unusual mistakes that deep learning algorithms make, this topic wasn't really a serious avenue of study until the algorithm started to work well most of the time. And now we will study the way that they break. Now that that's actually the exception rather than the rule. An adversarial example is an example that has been carefully computed to be misclassified. In a lot of cases we're able to make the new image indistinguishable to a human observer from the original image. Here I show you one where we start with a panda. On the left, this is a panda that has not been modified in any way. And a convolutional network trained on the image net data set is able to recognize it as being a panda. One interesting thing is that the model doesn't have a whole lot of confidence in that decision. It assigns about 60% probability to this image being a panda. If we then compute exactly the way that we could modify the image to cause the convolutional network to make a mistake, we find that the optimal direction to move all the pixels is given by this image in the middle. To a human it looks a lot like noise. It's not actually noise. It's carefully computed as a function of the parameters of the network. There's actually a lot of structure there. If we multiply that image of the structured attack by a very small coefficient and add it to the original panda, we get an image that a human can't tell from the original panda. In fact, on this slide there is no difference between the panda on the left and the panda on the right. When we present the image to the convolutional network, we use 32 bit floating point values. The monitor here can only display 8 bits of color resolution. And we've made a change that's just barely too small to affect the smallest of those 8 bits. But it affects the other 24 of the 32 bit floating point representation. That little tiny change is enough to fool the convolutional network into recognizing this image of a panda as being a given. Another interesting thing is that it doesn't just change the class. It's not that we just barely found the decision boundary and just barely stepped across it. The convolutional network actually has much more confidence in its incorrect prediction that the image on the right is a given. It's a given than it had for the original being a panda. On the right it believes that the image is a given with 99.9% probability. So before it thought that there was about a third chance that it was something other than a panda. And now it's about as certain as it can possibly be that it's a given. As a little bit of history, people have studied ways of computing attacks to fool different machine learning models. And at least about 2004 and maybe earlier. For a long time this was done in the context of fooling spam detectors. In about 2013, but used a bigio found that you could fool neural networks in this way. And around the same time my colleague Christian Zegity found that you could make this kind of attack against deep neural networks. Just by using an optimization algorithm to search on the input of the image. A lot of what I'll be telling you about today is my own follow up work on this topic. But I've spent a lot of my career over the past few years understanding why these attacks are possible and why it's so easy to fool these convolutional networks. When my colleague Christian first discovered this phenomenon independently from Bates Dabigio, but around the same time. He found that it was actually a result of a visualization he was trying to make. He wasn't studying security, he wasn't studying how to fool a neural network. Instead he had a convolutional network that could recognize objects very well and he wanted to understand how it worked. So he thought that maybe he could take an image of a scene like for example a picture of a ship. And he could gradually transform that image into something that the network would recognize as being an airplane. And over the course of that transformation he could see how the features of the input change. You might expect that maybe the background would turn blue to look like the sky behind an airplane. Or you might expect that maybe the ship would grow wings to look more like an airplane. And then you could conclude from that that the convolutional network uses the blue sky or uses the wings to recognize airplanes. That's actually not really what happened at all. So each of these panels here shows an animation that you read left to right, top to bottom. Each panel is another step of gradient a cent on the log probability that the input is an airplane according to a convolutional network model. And then the following the gradient on the input to the image, you're probably used to following the gradient on the parameters of a model. You can use the back propagation algorithm to compute the gradient on the input image using exactly the same procedure that you would use to compute the gradient on the parameters. In this animation of the ship in the upper left, we see five panels that all look basically the same. Gradient descent doesn't seem to have moved the image at all. But by the last panel the network is completely confident that this is an airplane. When you first code up this kind of experiment, especially if you don't know what's going to happen, it feels a little bit like you have a bug in your script and you're just displaying the same image over and over again. The first time I didn't, I couldn't believe it was happening and I had to open up the images and numpy and take the difference of them and make sure that there was actually a non zero difference there. But there is. I showed several different animations here of a ship, a car, a cat and a truck. The only one where I actually see any change at all is the image of the cat. The color of the cat's face changes a little bit and maybe it becomes a little bit more like the color of a metal airplane. Other than that, I don't see any changes in any of these animations and I don't see anything very suggestive of an airplane. So gradient descent, rather than turning the input into an example of an airplane, has found an image that fools the network into thinking that the input is an airplane. And if we were malicious attackers, we didn't even have to work very hard to figure out how to fool the network. We just asked the network to give us an image of an airplane and it gave us something that fools it into thinking that the input is an airplane. When Christian first published this work, a lot of articles came out with titles like the flaw lurking in every deep neural network or deep learning has deep flaws. It's important to remember that these vulnerabilities apply to essentially every machine learning algorithm that we've studied so far. Some of them like RBF networks and parsing density estimators are able to resist this effect somewhat, but even very simple machine learning algorithms are highly vulnerable to adversarial examples. In this image, I show an animation of what happens when we attack a linear model. So it's not a deep algorithm and all, it's just a shallow softmax model. You multiply by a matrix, you add a vector of bias terms, you apply the softmax function and you've got your probability distribution of the 10 M nist classes. At the upper left, I start with an image of a 9, and then as we move left to right, top to bottom, I gradually transform it to be a 0. Where I've drawn the yellow box, the model assigns high probability to it being a 0. I forget exactly what my threshold was for high probability, but I think it was around 0.9 or so. Then as we move to the second row, I transform it into a 1, and the second yellow box indicates where we've successfully fooled the model into thinking it's a 1 with high probability. And then as you read the rest of the yellow box is left to right, top to bottom, we go through the 2s, 3s, 4s and so on. Until finally at the lower right, we have a 9 that has a yellow box around it, and it actually looks like a 9. But in this case, the only reason it actually looks like a 9 is that we started the whole process with a 9. We successfully swept through all 10 classes of M nist without substantially changing the image of the digit in any way that would interfere with human recognition. So this linear model was actually extremely easy to fool. Besides linear models, we've also seen that we can fool many different kinds of linear models including logistic regression and SVM's. We've also found that we can fool decision trees and to a lesser extent nearest neighbor's classifiers. We wanted to explain exactly why this happens. Back in about 2014 after we'd published the original paper where we said that these problems exist, we were trying to figure out why they happened. When we wrote our first paper, we thought that basically this is a form of overfitting that you have a very complicated deep neural network. It learns to fit the training set. It's behavior on the test set is somewhat undefined, and then it makes random mistakes that an attacker can exploit. So let's walk through what that story looks like somewhat concretely. I have here a training set of three blue axes and three green o's, and we want to make a classifier that can recognize X's and recognize O's. We have a very complicated classifier that can easily fit the training set. So we represent everywhere it believes X's should be with blobs of blue color, and I've drawn a blob of blue around all of the training set X's. So it correctly classifies the training set. It also has a blob of green mass showing where the O's are, and it successfully fits all of the green training set O's. But then because this is a very complicated function and it has just way more parameters than it actually needs to represent the training task. It throws little blobs of probability mass around the rest of space randomly. On the left there's a blob of green space that's kind of near the training set X's, and I've drawn a red X there to show that maybe this would be an adversarial example where we expect a classification to be X but the model assigns O. And on the right I've shown that there's a red O where we have another adversarial example. We're very near the other O's. We might expect the model to assign this class to be an O, and yet because it's drawn blue mass there, it's actually assigning it to be an X. So if overfitting is really the story, then each adversarial example is more or less the result of bad luck, and also more or less unique. If we fit the model again or we fit a slightly different model, we would expect to make different random mistakes on these points that are off the training set. But that was actually not what we found at all. We found that many different models would misclassify the same adversarial examples, and they would assign the same class to them. We also found that if we took the difference between an original example and an adversarial example, then we had a direction in input space, and we could add that same offset vector to any clean example, and we would almost always get an adversarial example as a result. So we started to realize that there is a systematic effect going on here, not just a random effect. That led us to another idea, which is that adversarial examples might actually be more like underfitting rather than overfitting. They might actually come from the model being too linear. Here I draw the same task again, where we have the same manifold of O's and the same line of X's. And this time I fit a linear model to the data set, rather than fitting a high capacity nonlinear model to it. We see that we get a dividing hyperplane running in between the two classes. But this hyperplane doesn't really capture the true structure of the classes. The O's are clearly arranged in a seashaped manifold. If we keep walking past the end of the O's, we cross the decision boundary, and we've drawn a red O, where even though we're very near the decision boundary and near other O's, we believe that it is now an X. Similarly, we can take steps that go from near X's to just over the line that our classified as O's. Another thing that's somewhat unusual about this plot is that if we look at the lower left or upper right corners, these corners are very confidently classified as being X's on the lower left or O's on the upper right. Even though we've never seen any data over there at all, the linear model family forces the model to have very high confidence in these regions that are very far from the decision boundary. We've seen that linear models can actually assign really unusual confidence as you move very far from the decision boundary, even if there isn't any data there. But our deep neural networks actually anything like linear models could linear models actually explain anything about how it is that deep neural nets fail. It turns out that modern deep neural nets are actually very piecewise linear. Rather than being a single linear function, there are piecewise linear with maybe not that many linear pieces. If we use rectified linear units, then the mapping from the input image to the output logits is literally a piecewise linear function. By the logits, I mean the unnormalized log probabilities before we apply the soft backsop at the output of the model. There are other neural networks like max out networks that are also literally piecewise linear. And then there are several that come very close to it. Before rectified linear units became popular, most people used to use sigmoid units at one form or another either logistic sigmoid or hyperbolic tangent units. These sigmoid units have to be carefully tuned, especially at an initialization, so that you spend most of your time near the center of the sigmoid, or the sigmoid is approximately linear. And then finally, the LSTM, a kind of recurrent network that is one of the most popular recurrent networks today, uses addition from one time step to the next in order to accumulate and remember information over time. Addition is a particularly simple form of linearity, so we can see that the interaction from a very distant time step in the past and the present is highly linear within an LSTM. And how to be clear, I'm speaking about the mapping from the input of the model to the output of the model. That's what I'm saying is close to being linear, or is piecewise linear with relatively few pieces. The mapping from the parameters of the network to the output of the network is nonlinear, because the weight matrices at each layer of the network are multiplied together. So we actually get extremely nonlinear interactions between parameters and the output. That's what makes training an neural network so difficult. But the mapping from the input to the output is much more linear and predictable, and it means that optimization problems that aim to optimize the input to the model are much easier than optimization problems that aim to optimize the parameters. If we go and look for this happening in practice, we can take a convolutional network and trace out a one dimensional path through its input space. So what we're doing here is we're choosing a clean example. It's an image of a white car on a red background, and we are choosing a direction that will travel through space. We're going to have a coefficient epsilon that we multiply by this direction. So an epsilon is negative 30, like at the left end of the plot, we're subtracting off a lot of this unit vector direction. When epsilon is zero, like at the middle of the plot, we're visiting the original image from the dataset. And when epsilon is positive 30, like at the right end of the plot, we're adding this direction onto the input. In the panel on the left, I show you an animation where we move from epsilon equals negative 30, as up to epsilon equals positive 30. You read the animation left to right top to bottom. And everywhere that there's a yellow box, the input is correctly recognized as being a car. On the upper left, you see that it looks mostly blue on the upper, on the lower right, it's kind of hard to tell what's going on. It's, it's kind of reddish and so on. In the middle row, just after where the yellow box is and you can see pretty clearly that it's a car on a red background that the image is kind of small in these slides. What's interesting to look at here is the lojits that the model outputs. This is a deep convolutional rectified linear unit that work because it uses rectified linear units. We know that the output is a piecewise linear function of the input to the model. The main question we're asking by making this plot is how many different pieces does this piecewise linear function have if we look at one particular cross section. You might think that maybe a deep net is going to represent some extremely wiggly, complicated function with lots and lots of linear pieces no matter which cross section you look in. Or we might find that it has more or less two pieces for each function we look at. Each of the different curves on this plot is the lojits for a different class. We see that out at the tails of the plot that the frog class is the most likely. The frog class basically looks like a big v-shaped function. The lojits for the frog class become very high when epsilon is negative 30 or positive 30. They drop down and become a little bit negative when epsilon is 0. The car class listed as automobile here, it's actually high in the middle and the car is correctly recognized. As we sweep out to very negative epsilon, the lojits for the car class do increase, but they don't increase nearly as quickly as the lojits for the frog class. We found a direction that's associated with the frog class and as we follow it out to a relatively large perturbation. We find that the model extrapolates linearly and begins to make a very unreasonable prediction that the frog class is extremely likely. Just because we've moved for a long time in this direction that was locally associated with the frog class being more likely. When we actually go and construct adversarial examples, we need to remember that we're able to get quite a large perturbation without changing the image very much as far as a human being is concerned. Here I show you a hand-written digit 3 and I'm going to change it in several different ways. All of these changes have the same L2 norm perturbation. In the top row, I'm going to change the 3 into a 7 just by looking for the nearest 7 in the training set. The difference between those two is this image that looks a little bit like the 7 wrapped in some black lines. Here white pixels in the middle image in the perturbation column, the white pixels represent adding something and black pixels represent subtracting something as you move from the left column to the right column. When we take the 3 and we apply this perturbation that transforms it into a 7, we can measure the L2 norm of that perturbation. It turns out to have an L2 norm of 3.96. That gives you a kind of reference for how big these perturbations can be. In the middle row, we apply a perturbation of exactly the same size, but with the direction chosen randomly. In this case, we don't actually change the class of the 3 at all. We just get some random noise that didn't really change the class. A human can still easily read it as being a 3. And then finally, at the very bottom row, we take the 3 and we just erase a piece of it with a perturbation of the same norm. And we turn it into something that doesn't have any class at all. It's not a 3. It's not a 7. It's just a defective input. All of these changes can happen with the same L2 norm perturbation. And actually, a lot of the time with adversarial examples, you make perturbations that have even larger L2 norm. What's going on is that there are several different pixels on the image, and so small changes to individual pixels can add up to relatively large vectors. For larger datasets, like image net, where there's even more pixels, you can make very small changes to each pixel, but travel very far in vector space, as measured by the L2 norm. That means that you can actually make changes that are almost imperceptible, but actually move you really far and get a large dot product with the coefficients of the linear function that the model represents. It also means that when we're constructing adversarial examples, we need to make sure that the adversarial example procedure isn't able to do what happened in the top row of this slide here. So the top row of this slide, we took a 3 and we actually just changed it into a 7. So when the model says that the image in the upper right is a 7, it's not a mistake. We actually just change the input class. When we build adversarial examples, we want to make sure that we're measuring real mistakes. If we're experimenter studying how easy a network is to fool, we want to make sure that we're actually fooling it and not just changing the input class. And if we're in a attacker, we actually want to make sure that we're causing misbehavior in the system. To do that, when we build adversarial examples, we use the max norm to constrain the perturbation. Basically, this says that no pixel can change by more than some amount of epsilon. So the L2 norm can get really big, but you can't concentrate all the changes for that L2 norm to erase pieces of the digit. Like in the bottom row, here we erase the top of a 3. When very fast way to build an adversarial example is just to take the gradient of the cost that you used to train the network with respect to the input, and then take the sign of that gradient. The sign is essentially enforcing the max norm constraint. You're only allowed to change the input by up to epsilon at each pixel. So if you just take the sign, it tells you whether you want to add up to epsilon or subtract epsilon in order to hurt the network. You can view this as taking the observation that the network is more or less linear as we showed on this slide. Using that to motivate building a first order Taylor series approximation of the neural networks cost. And then subject to that Taylor series approximation, we want to maximize the cost following this max norm constraint. And that gives us this technique that we call the fast gradient sign method. If you want to just get your hands dirty and start making adversarial examples really quickly, or if you have an algorithm where you want to train on adversarial examples in the inner loop of learning, this method will make adversarial examples for you very, very quickly. In practice, you should also use other methods like Nicholas Carlinis attack based on multiple steps of the atom optimizer to make sure that you have a very strong attack that you bring out when you think you have a model that might be more powerful. A lot of the time people find that they can defeat the fast gradient sign method and think that they've built a successful defense, but then when you bring out a more powerful method that takes longer to evaluate, they find that they can't overcome the more computationally expensive attack. I've told you that adversarial examples happen because the model is very linear, and then I told you that you could use this linear idea assumption to build this attack, the fast gradient sign method. This method would apply to a regular neural network that doesn't have any special defenses, will get over a 99% attack success rate. So that seems to confirm somewhat this hypothesis that adversarial examples come from the model being far too linear and extrapolating in linear fashions when it shouldn't. We can actually go looking for some more evidence. My friend David Ward-Farley and I built these maps of the decision boundaries of neural networks, and we found that they are consistent with the linearity hypothesis. So the FGSM is that attack method that I described in the previous slide where we take the sign of the gradient. We'd like to build a map of a two-dimensional cross-section of input space and show which classes are assigned to the data at each point. In the grid on the right, each different cell, each little square within the grid, is a map of a C-Far10 classifier's decision boundary with each cell corresponding to a different C-Far10 test example. On the left, I show you a little legend where you can understand what each cell means. The very center of each cell corresponds to the original example from the C-Far10 data set with no modification. As we move left to right in the cell, we're moving in the direction of the fast gradient sign method attack, so just the sign of the gradient. As we move up and down within the cell, we're moving in a random direction that's orthogonal to the fast gradient sign method direction. So we get to see a cross-section, a 2D cross-section of C-Far10 decision space. At each pixel within this map, we plot a color that tells us which classes is assigned there. We use white pixels to indicate that the correct class was chosen, and then we use different colors to represent all of the other incorrect classes. You can see that in nearly all of the grid cells on the right, roughly the left half of the image is white. So, roughly the left half of the image has been correctly classified. As we move to the right, we see that there's usually a different color on the right half, and the boundaries between these regions are approximately linear. What's going on here is that the fast gradient sign method has identified a direction where if we get a large dot product with that direction, we can get an adversarial example. From this, we see that adversarial examples live more or less in linear subspaces. When we first discovered adversarial examples, we thought that they might live in little tiny pockets. And the first paper we actually speculated that maybe there are a little bit like the rational numbers, hiding out finally tile among the real numbers, with nearly every real number being near a rational number. We thought that because we were able to find an adversarial example corresponding to every clean example that we loaded into the network. After doing this further analysis, we found that what's happening is that every real example is near one of these linear decision batteries, where you cross over into an adversarial subspace. And once you're in that adversarial subspace, all the other points nearby are also adversarial examples that will be misclassified. This has security implications, because it means you only need to get the direction right. You don't need to find an exact coordinate in space. You just need to find a direction that has a large dot product with the sign of the gradient. And once you move more or less approximately in that direction, you can fool the model. We also made another cross section where after using the left right axis as the fast gradient sign method, we looked for a second direction that has high dot product with the gradient so that we could make both axes adversarial. In this case, we see that we get linear decision boundaries. They're now oriented diagonally rather than vertically, but we can see that there's actually this two dimensional subspace of adversarial examples that we can cross into. Finally, it's important to remember that adversarial examples are not noise. You can add a lot of noise to an adversarial example and it will stay adversarial. How do a lot of noise to a clean example? It'll stay clean. Here we make random cross sections where both axes are randomly chosen directions. And you see that on C410, most of the cells are completely white, meaning that they're correctly classified to start with. And when you add noise, they stay correctly classified. We also see that the model makes some mistakes, because this is the test set. And generally if a test example starts out misclassified, adding the noise doesn't change it. There are a few exceptions where if you look in the third row and third column, noise actually can make the model misclassify the example for especially large noise values. And there's even somewhere in the top row, there's one example you can see where the model is misclassifying the test example to start with. And then the noise is changing to be correctly classified. But for the most part, noise has very little effect on the classification decision compared to adversarial examples. What's going on here is that in high dimensional spaces, if you choose some reference vector and then you choose a random vector in that high dimensional space, the random vector will on average have zero dot product with the reference vector. So, if you take a first order Taylor series approximation of your cost and thinking about how your Taylor series approximation predicts that random vectors will change your cost, you see that random vectors on average have no effect on the cost. But adversarial examples are chosen to maximize it. So, we looked in two dimensions, more recently, Florio Chamey here at Stanford got interested in finding out just how many dimensions there are to these subspaces where the adversarial examples lie in a thick contiguous region. And we came up with an algorithm together where you actually look for several different orthogonal vectors that all have a large dot product with the gradient. By looking in several different orthogonal directions simultaneously, we could map out this kind of polytope where many different adversarial examples live. We found out that this adversarial region has on average about 25 dimensions. If you look at different examples, you'll find different numbers of adversarial dimensions, but on average on MNIST, we found it is about 25. Interesting here is the dimensionality actually tells you something about how likely you are to find an adversarial example by generating random noise. If every direction were adversarial, then any change would cause them as classification. If most of the directions were adversarial, then random directions would end up being adversarial just by accident most of the time. And then if there was only one adversarial direction, you'd almost never find that direction just by adding random noise. When there's 25, you have a chance of doing it sometimes. Another interesting thing is that different models will often misclassify the same adversarial examples. The subspace dimensionality of the adversarial subspace relates to that transfer property. The larger the dimensionality of the subspace, the more likely it is that the subspaces for two models will intersect. So if you have two different models that have a very large adversarial subspace, you know that you can probably transfer adversarial examples from one to the other. But if their adversarial subspaces very small, then unless there's some kind of really systematic effect forcing them to share exactly the same subspace, it seems less likely that you'll be able to transfer examples just due to the subspaces randomly aligning. A lot of the time in the adversarial example research community, we refer back to the story of Clever Hans. This comes from an essay by Bob Stern called Clever Hans Clever Algorithms, because Clever Hans is a pretty good metaphor for what's happening with machine learning algorithms. So Clever Hans was a horse that lived in the early 1900s. His owner trained him to do arithmetic problems. So you could ask him Clever Hans, what's two plus one? And he would answer by tapping his hoof. And after the third tap, everybody would start cheering and clapping and looking excited because he'd actually done an arithmetic problem. Well, it's heard that he hadn't actually learned to do arithmetic, but it was actually pretty hard to figure out what was going on. His owner was not trying to defried anybody. His owner actually believed he could do arithmetic. And presumably Clever Hans himself was not trying to trick anybody. But eventually a psychologist examined him and found that if he was put in a room alone without an audience, and the person asking the questions, war a mask, he couldn't figure out when to stop tapping. You'd ask him Clever Hans, what's one plus one? And he'd just keep staring at your face waiting for you to give him some sign that he was done tapping. So everybody in this situation was trying to do the right thing. Clever Hans is trying to do whatever it took to get the apple that his owner would give him when he answered an arithmetic problem. His owner did his best to train him correctly with real arithmetic questions and real rewards for correct answers. And what happened was that Clever Hans inadvertently focused on the wrong queue. He found this queue of people's social reactions that could reliably help him solve the problem. But then it didn't generalize to a test set where you intentionally took that queue away. It did generalize to a naturally occurring test set where he had an audience. So that's more or less what's happening with machine learning algorithms. They found these very linear patterns that can fit the training data. And these linear patterns even generalize to the test data. They've learned to handle any example that comes from the same distribution as their training data. But then if you shift the distribution that you test them on, if a malicious adversary actually creates examples that are intended to fool them, they're very easily fooled. In fact, we find that modern machine learning algorithms are wrong almost everywhere. We tend to think of them as being correct most of the time because when we run them on naturally occurring inputs, they achieve very high accuracy percentages. But if we look instead of as the percentage of samples from an IID test set, if we look at the percentage of the space in RN that is correctly classified, we find that the misclassified almost everything, and they behave reasonably only on a very thin manifold surrounding the data that we train them on. In this plot, I show you several different examples of Gaussian noise that I've run through a C410 classifier. Everywhere that there's a pink box, the classifier thinks that there is something rather than nothing. I'll come back to what that means in a second. Everywhere that there is a yellow box, one step of the fast gradient sign method was able to persuade the model that it was looking specifically at an airplane. I chose the airplane class because it was the one with the lowest success rate. It had about a 25% success rate. It means an attacker would need four chances to get noise recognized as an airplane on this model. An interesting thing and appropriate enough given the story of Clever Hans, is that this model found that about 70% of RN was classified as a horse. So I mentioned that this model will say that noise is something rather than nothing, and it's actually kind of important to think about how we evaluate that. If you have a softmax classifier, it has to give you a distribution over the end different classes that you train it on. So there's a few ways that you can argue that the model is telling you that there's something rather than nothing. One is you can say, if it assigns something like 90% to one particular class, that seems to be voting for that class being there. We'd much rather see it give us something like a uniform distribution saying, this noise doesn't look like anything in the training set, so it's equally likely to be a horse or a car. And that's not what the model does. It will say this is very definitely a horse. Another thing that you can do is you can replace the last layer of the model. For example, you can use a sigmoid output for each class. And then the model is actually capable of telling you that any subset of classes is present. It could actually tell you that an image is both a horse and a car. And what we'd like it to do for the noise is tell us that none of the classes is present. That all the sigmoids should have a value of less than one half. And one half isn't even particularly a low threshold. We could reasonably expect that all the sigmoids would be less than 0.01 for such a defective input as this. But what we find instead is that the sigmoids tend to have at least one class present, just when we run Gaussian noise of sufficient norm through the model. We've also found that we can do adversarial examples for reinforcement learning. And there's a video for this. I'll upload the slides after the talk and you can follow the link. Unfortunately, I wasn't able to get the Wi-Fi to work, so I can't show you the video animated. But I can describe basically what's going on from this still here. There's a game sequest on Atari where you can train reinforcement learning agents to play that game. And you can take the raw input pixels and you can take the fast gradient sign method or other attacks that use other norms besides the max norm. And compute perturbations that are intended to change the action that the policy would select. So the reinforcement learning policy, you can think of it as just being like a classifier that looks at a frame. And instead of categorizing the input into a particular category, it gives you a softmax distribution over actions to take. So if we just take that and say that the most likely action should have its accuracy be decreased by the adversary. So I'd have its probability be decreased by the adversary. You'll get these perturbations of input frames that you can then apply and cause the agent to play different actions that it would have otherwise. And using this you can make the agent play sequest very, very badly. It's maybe not the most interesting possible thing. What we'd really like is an environment where there are many different reward functions available for us to study. So for example, if you had a robot that was intended to cook scrambled eggs and you had a reward function measuring how well it's cooking scrambled eggs. And you had another reward function measuring how well it's cooking chocolate cake. It would be really interesting if we could make adversarial examples that cause the robot to make a chocolate cake when the user intended for it to make scrambled eggs. That's because it's very difficult to succeed at something. It's relatively straightforward to make a system fail. So right now adversarial examples for RRL are very good at showing that we can make RRL agents fail, but we haven't yet been able to hijack them and make them do a complicated task that's different from what they're unintended. It seems like it's one of the next steps in adversarial example research though. If we look at high dimensional linear models, we'd actually see that a lot of this is very simple and straightforward. Here we have a logistic regression model that classifies 7s and 3s. So the whole model can be described just by a weight vector and a single scalar bias term. We don't really need to see the bias term for this exercise. If you look on the left, I've plotted the weights that we use to discriminate 7s and 3s. The weights should look a little bit like the difference between the average 7 and the average 3. And then down at the bottom we've taken the sign of the weights. So the gradient for logistic regression model is going to be proportional to the weights. And then the sign of the weights gives you essentially the sign of the gradient. So we can do the fast gradient sign method to attack this model just by looking at its weights. In the examples in the panel, the second column from the left, we can see clean examples. And then on the right we've just added or subtracted this image of the sign of the weights off of them. To you and me and as human observers, the sign of the weights is just like garbage that's in the backrest. And more or less filter it out. It doesn't look particularly interesting to us. It doesn't grab our attention. To the logistic regression model, this image of the sign of the weights is the most salient thing that could ever appear in the image. When it's positive, it looks like the world's most quintessential 7. When it's negative, it looks like the world's most quintessential 3. And so the model makes its decision almost entirely based on this perturbation that we added to the image. But then on the backrest. You can also take the same procedure and my colleague Andre at OpenAI showed how you can modify the image on image net using the same approach and turn this goldfish into a daisy. Because image net is much higher dimensional, you don't need to use quite as large of a coefficient on the image of the weights. So we can make a more persuasive, full-length attack. You can see that the same image of the weights when applied to any different input image will actually reliably cause a misclassification. What's going on is that there are many different classes. And it means that if you choose the weights for any particular class, it's very unlikely that a new test image will belong to that class. So an image net, if we're using the weights for the daisy class, and there are a thousand different classes, then we have about a 99.9% chance that a test image will not be a daisy. If we then go ahead and add the weights for the daisy class to that image, then we get a daisy, and because that's not the correct class, it's a misclassification. So there's a paper at CVPR this year called Universal Adversarial Perterbations that expands a lot more on this observation that we had going back in 2014. But basically these weight vectors, when applied to many different images, can cause misclassification in all of them. I've spent a lot of time telling you that these linear models are just terrible, and at some point you've probably been hoping I'll give you some sort of a control experiment to convince you that there's another model that's not terrible. So it turns out that some quadratic models actually perform really well. In particular, a shallow RBF network is able to resist adversarial perturbations very well. Earlier I showed you an animation where I took a 9 and I turned it into a 0, 1, 2, and so on, without really changing its appearance at all. And I was able to fool a linear softmax regression classifier. Here I've got an RBF network where it outputs a separate probability of each class being absent or present. And that probability is given by e to the negative square of the difference between a template image and the input image. And if we actually follow the gradient of this classifier, it does actually turn the image into a 0, a 1, a 2, a 3, and so on. And we can actually recognize those changes. The problem is this classifier does not get very good accuracy on the training set. It's a shallow model. It's basically just a template matcher. It is literally a template matcher. And if you try to make it more sophisticated by making it deeper, it turns out that the gradient of these RBF units is 0, or very near 0, throughout most of our end. So they're extremely difficult to train, even with batch normalization and methods like that. I haven't managed to train a deep RBF network yet. But I think if somebody comes up with better hyper parameters or a new more powerful optimization algorithm, it might be possible to solve the adversarial example problem by training a deep RBF network. Where the model is so nonlinear and has such wide flat areas that the adversary is not able to push the cost up hill just by making small changes to the models input. One of the things that's the most alarming about adversarial examples is that they generalize from one data set to another at one model to another. Here I've trained two different models on two different training sets. The training sets are tiny in both cases. It's just Mness 3 versus 7 classification. And this is really just for the purpose of making a slide. If you train a logistic regression model on the digits shown in the left panel, you get the weights shown in the left on the lower panel. So you've got two different training sets and we learn weight vectors that look very similar to each other. That's just because Mness 3 learning algorithms generalize. You want them to learn a function that's somewhat independent of the data that you train them on. It shouldn't matter which particular training examples you choose. The training set to the test set, you've also got to expect the different training sets will give you more or less the same result. And that means that because they've learned more or less similar functions, they're vulnerable to similar adversarial examples. The same adversary, an adversary can compute an image that fools one and use it to fool the other. We can actually go ahead and measure the transfer rate between several different machine learning techniques, not just different data sets. And they found that for example, logistic regression makes adversarial examples that transfer to decision trees with 87.4% probability. Wherever you see dark squares in this matrix, that shows that there's a high amount of transfer. And it's very possible for an attacker using the model on the left to create adversarial examples for the model on the right. The procedure overall is that suppose the attacker wants to fool a model that they don't actually have access to. They don't know the architecture that's used to train the model. They may not even know which algorithm is being used. They may not know whether they're attacking a decision tree or a deep neural net. They also don't know the parameters of the model that they're going to attack. So what they can do is they can train their own model that they want to, that they'll use to build the attack. There's two different ways you can train your own model. What is you can label your own training set for the same task that you want to attack? Say that somebody is using an image net classifier and for whatever reason you don't have access to image net, you can take your own photos and label them train your own object recognizer. It's going to share adversarial examples with an image net model. The other thing you can do is say that you can't afford to gather your own training set. You can do instead is if you can get limited access to the model where you just have the ability to send inputs to the model to observe its outputs, then you can send those inputs, observe the outputs, and use those as your training set. This will work even if the output that you get from the target model is only the class label that it chooses. A lot of people read this in a assume that you need to have access to all the probability values it outputs, but even just the class labels are sufficient. Once you've used one of these two methods, either gather your own training set or observing the outputs of a target model, you can train your own model and then make adversarial examples for your model. Those adversarial examples are very likely to transfer and affect the target model, so you can then go and send those out and fool it, even if you didn't have access to it directly. We've also measured the transferability across different data sets, and for most models we find that there are kind of an intermediate zone where different data sets will result in a transfer rate of like 60 to 80%. There's a few models like SVMs that are very data dependent because SVMs that are focusing on a very small subset of the training data form their final decision boundary. But most models that we care about are somewhere in the intermediate zone. Now that's just assuming that you rely on the transfer happening naturally. You make an adversarial example and you hope that it will transfer to your target. What if you do something to stack the deck in your favor and improve the odds that you'll get your adversarial examples to transfer? Don songs group that you see Berkeley studied this. They found that if they take an ensemble of different models and they use gradient descent to search for an adversarial example that will fool every member of their ensemble. It's extremely likely that it will transfer and fool a new machine learning model. So if you have an ensemble of five models, you can get it to the point where there's essentially a 100% chance that you'll fool a sixth model out of the set of the models that they compared. They looked at things like resonance of different depths, VGG and GoogleNet. So in the labels for each of the different rows, you can see that they made ensembles, that lacked each of these different models and then they would test it on the different target models. So like if you make an ensemble that omits GoogleNet, you have only about a 5% chance of GoogleNet correctly classifying the adversarial example you make for that ensemble. If you make an ensemble that omits, resonate 152 in their experiments, they found that there is a 0% chance of resonate 152 resisting that attack. That probably in the case they should have run some more adversarial examples until they found a non-zero success rate. But it does show that the attack is very powerful and that when you go looking to intentionally cause the transfer effect, you can really make it quite strong. A lot of people often ask me if the human brain is vulnerable to adversarial examples. And for this lecture, I can't use copyrighted material, but there's some really hilarious things on the internet. If you go looking for like the fake capture with images of Mark Hamill, you'll find something that, you know, my perception system definitely can't handle. So here's another one that's actually published with a license where I was confident I'm allowed to use it. You can look at this image of different circles here and the appear to be intertwined spirals, but in fact they are concentric circles. The orientation of the edges of the squares is interfering with the edge detectors in your brain and making it look like the circles are spiraling. So you can think of these optical illusions as being adversarial examples for the human brain. What's interesting is that we don't seem to share many adversarial examples in common with machine learning models. Adversarial examples transfer extremely reliably between different machine learning models, especially if you use that ensemble trick that was developed in a UC Berkeley. But those adversarial examples don't fool us. It tells us that we must be using a very different algorithm or model family than current convolutional networks. And we don't really know what the difference is yet, but it would be very interesting to figure that out. It seems to suggest that studying adversarial examples could tell us how to significantly improve our existing machine learning models. Even if you don't care about having an adversary, we might figure out something or other about how to make machine learning algorithms deal with ambiguity and unexpected inputs. More like a human does. If we actually want to go out and do attacks in practice, there's started to be a body of research on this subject. Nikola Papernos showed that he could use the transfer effect to fool classifiers hosted by metamine, Amazon and Google. So these are all just different machine learning APIs where you can upload a dataset and the API will train a model for you. And then you don't actually know in most cases which model has trained for you. You don't have access to its weights or anything like that. So Nikola would train his own copy of a model using the API and then build a model on his own personal desktop where he could fool the API hosted model. Later, Berkeley showed you could fool clarify in this way. Oh, what's it like if we look at for example, like this picture of the panda to us it looks like a panda to most machine learning models it looks like a given. And so this this changes in interfering with our brains, but it fools reliably with lots of different machine learning models. Yeah. I saw somebody actually took this image of the perturbation out of our paper and they pasted it on their Facebook profile picture to see if it could interfere with Facebook recognizing them and they said that it did. I don't I don't think that Facebook has a given tag though so we don't know if they managed to make it think that they were a given. And one of the other things that you can do that's of fairly high practical significance is you can actually fool malware detectors. And so, you can actually see that the pattern grows at the University of Starland to wrote a paper about this and there's starting to be a few others there's a model called malgan that actually uses again to generate adversarial examples for malware detectors. Another thing that matters a lot if you are interested in using these attacks in the real world and defending against them in the real world. And so, at the time you don't actually have access to the digital input to a model if you're interested in the perception system for a self-driving car or a robot you probably don't get to actually write to the buffer on the robot itself. You just get to show the robot objects that it can see through camera lens. And I'm talking in a semi-benjiu and I wrote a paper where we studied if we can actually fool an object recognition system running on a phone where it perceives the world through a camera. Our methodology was really straightforward. We just printed out several pictures of adversarial examples and we found that the object recognition system running on the camera was fooled by them. The camera is actually different from the model that we used to generate the adversarial examples. So we're showing not just transfer across the changes that happen when you use the camera. We're also showing that there's transfer across the model that you use. So the attacker could conceivably fool a system that's deployed in a physical agent, even if they don't have access to the model on that agent. And even if they can't interface directly with the agent, but just modify subtly, model, objects that it can see in its environment. So I think a lot of that comes back to the maps that I showed earlier that if you cross over the boundary into the realm of adversarial examples, they occupy a pretty wide space and they're very densely packed in there. So if you jostle around a little bit, you're not going to recover from the adversarial attack. If the camera noise somehow or other was aligned with the negative gradient of the cost, then the camera could like take a gradient descent step downhill and rescue you from up hill step that the adversary took. But probably the camera is taking more or less something that you could model as a random direction. Like clearly when you use the camera more than once, it's going to do the same thing each time. But for the point of view of how that direction relates to the image classification problem, it's more or less a random variable that you sample once, and it seems unlikely to align exactly with the normal to this class boundary. There's a lot of different defenses that we'd like to build. And you know it's a little bit disappointing that I'm mostly here to tell you about attacks. I'd like to tell you how to make your system's more robust. But basically every attack we've tried has failed pretty badly. And in fact, even when people have published that they successfully defended, while there's been several papers on archive over the last several months, Nicholas Carlinney at Berkeley just released a paper where he shows that 10 of those defenses are broken. So this is a really, really hard problem. You can't just make it go away by using traditional regularization techniques. And in particular, generative models are not enough to solve the problem. A lot of people say, oh, the problem that's going on here is that you don't know anything about the distribution over the input pixels. If you could just tell whether the input is realistic or not, then you'd be able to resist it. And turns out that what's going on here is what matters more than getting the right distribution over the inputs X is getting the right posterior distribution over the class labels why given inputs X. So just using a generative model is not enough to solve the problem. I think a very carefully designed generative model could possibly do it. Here I showed two different modes of a bimodal distribution and we have two different generative models that try to capture these modes. On the left we have a mixture of two gousians on the right we have a mixture of two leplocians. You can not really tell the difference visually between the distribution they impose over X and the difference in the likelihood they assigned to the training data is negligible. The posterior distribution they assign over classes is extremely different. On the left we get a logistic regression classifier that has very high confidence out at the tails of the distribution where there is never any training data. On the right with a leplocian distribution we level off to more or less 50 50. Yeah. The issue is that it's a non-stationary distribution. So if you train it to recognize one kind of adversarial example then it will become vulnerable to another kind of design to flow its its detector. That's one of the category of defenses that Nicholas broke in his latest paper that he put out. So here basically the choice of exactly the family of generative model has a big effect on whether the posterior becomes deterministic or uniform as the model extrapolates. So we can design a really rich deep generative model that can generate realistic image net images and also correctly calculate its posterior distribution. Then maybe something like this approach could work. But at the moment it's really difficult to get any of those probabilistic calculations correct. And what usually happens is somewhere rather we make an approximation that causes the posterior distribution to extrapolate very linearly again. And it's it's been a difficult engineering challenge to build generative models that actually capture these distributions accurately. The universal approximator theorem tells us that whatever shape we would like our classification function to have and neural net that's big enough ought to be able to represent it. It's an open question whether we can train the neural net to have that function, but we know that we should at least be able to give the right shape. So so far we've been getting neural nets that give us these very linear decision functions and we'd like to get something that looks a little bit more like a step function. So what if we actually just train on every serial examples for every input x in the training set we also say we want you to train x plus an attack to map to the same class label as the original. It turns out that this sort of works. You can generally resist the same kind of attack that you train on and an important consideration is making sure that you can run your attack very quickly so that you can train on lots of examples. So here the green curve at the very top the one that doesn't really descend much at all that's the test center on adversarial examples if you train on clean examples only. The cyan curve that descends more or less diagonally through the middle of the plot that's the tester on adversarial examples if you train on adversarial examples. You can see that it does actually reduce significantly it gets down to a little bit less than 1% error. And the important thing to keep in mind here is that this is a fast gradient sine method adversarial examples it's much harder to resist iterative multi step adversarial examples. We run an optimized for a long time searching for a vulnerability and another thing to keep in mind is that we're testing on the same kind of adversarial examples that we train on. It's harder to generalize from one optimization algorithm to another. By comparison if you look at the if you look at what happens on clean examples the blue curve shows what happens on the clean test set error rate. If you train only on clean examples the red curve shows what happens if you train on both clean and adversarial examples. We see that the red curve actually drops lower than the blue curve. So on this task training on adversarial examples actually helped us do the original task better. This is because on the original task we were overfitting training on adversarial examples is a good regularizer. If you're overfitting it can make you overfit less if you're underfitting it'll just make you underfit worse. Other kinds of models besides deep neural nets don't benefit as much from adversarial training. So when we started this whole topic of study we thought that deep neural nets might be uniquely vulnerable to adversarial examples. It turns out that actually there are one of the few models that has a clear path to resisting them. Linear models are just always going to be linear. They don't have much hope of resisting adversarial examples. Deep neural nets can be trained to be nonlinear. And so it seems like there's a path to a solution for them. Even with adversarial training we still find that we are unable to make models where if you optimize the input to belong to different classes you get examples of those classes. Here I start out with a C410 truck and I turn it into each of the 10 different C410 classes. To where the middle of the plot you can see the truck has started to look a little bit like a bird. But the bird class is the only one that we've come anywhere near hitting. So even with adversarial training we're still very far from solving this problem. When we do adversarial training we rely on having labels for all the examples. We have an image that's labeled as a bird. We make a perturbation that's designed to decrease the probability of the bird class. And we train the model that the image still be a bird. But what if you don't have labels? It turns out that you can actually train with out labels. You ask the model to predict the label of the first image. So if you've trained for a little while in your model isn't perfect yet, it might say, oh, maybe this is a bird. Maybe it's a plane. There's some blue sky there. I'm not sure which of these two classes it is. Then we make an adversarial perturbation that's intended to change the gas. And we just try to make it say, ah, this is a truck or something like that. It's not ever you believe it was before. You can then train it to say that the distribution of our classes should still be the same as it was before. But this should still be considered probably a bird or a plane. This technique is called virtual adversarial training. It was invented by Takay Rumiato. He was my intern at Google after he did this work. At Google, we invited him to come and apply his invention to text classification. Because this ability to learn from unlabeled examples makes it possible to do semi-supervised learning, where you learn from both unlabeled and labeled examples. And there's quite a lot of unlabeled text in the world. So we were able to bring down the error rate on several different text classification tasks by using this virtual adversarial training. Finally, there's a lot of problems where we would like to use neural nets to guide optimization procedures. If we want to make a very, very fast car, we can imagine a neural net that looks at the blueprints for a car and predicts how fast it will go. If we could then optimize with respect to the input of the neural net and find the blueprint that it predicts would go the fastest, we could build an incredibly fast car. Unfortunately, what we get right now is not a blueprint for a fast car. We get an adversarial example that the model thinks is going to be very fast. If we're able to solve the adversarial example problem, we'll be able to solve this model-based optimization problem. I like to call model-based optimization the universal engineering machine. If we're able to do model-based optimization, we'll be able to write down a function that describes a thing that doesn't exist yet, but we wish that we had. And then gradient descent and neural nets will figure out how to build it for us. We can use that to design new genes and new molecules from additional drugs. And new circuits to make GPUs run faster and things like that. So I think overall solving this problem could unlock a lot of potential technological advances. In conclusion, attacking machine learning models is extremely easy, and defending them is extremely difficult. If you use adversarial training, you can get a little bit of a defense, but there's still many caveats associated with that defense. Adversarial training and virtual adversarial training also make it possible to regularize your model and even learn from unlabeled data. So you can do better on regular test examples, even if you're not concerned about facing an adversary. And finally, if we're able to solve all of these problems, we'll be able to build a black box model-based optimization system that can solve all kinds of engineering problems that are holding us back in many different fields. I think I have a few minutes left for questions. Thank you. Oh, so there's some determinism to the choice of those 50 directions. Oh, right. Yeah. So repeating the questions. I've said that the same perturbation can fool many different models or the same perturbation can be applied to many different clean examples. I've also said that the subspace of adversarial perturbations is only about 50 dimensional, even if the input dimension is 3,000 dimensional. So how is it that these subspaces intersect? The reason is that the choice of the subspace directions is not completely random. It's generally going to be something like pointing from one class centroid to another class centroid. And if you look at that vector and visualize it as an image, it might not be meaningful to a human, just because humans aren't very good at imagining what class centroid looks like. And we're really bad at imagining differences between centroid. But there is more or less this systematic effect that causes different models to learn similar linear functions, just because they're trying to solve the same task. So the question is, is it possible to identify which layer contributes the most to this issue? It's a very abstracted that's completely robust to adversarial perturbations and can shrink them to be very, very small. And then the last layer is still linear. And then you perturb all of the different layers, all the hidden layers as well as the input. In this lecture, I only described perturbing the input because it seems like that's where most of the benefit comes from. The one thing that you can't do with adversarial training is perturb the very last layer before the softmax because that linear layer at the end has no way of learning to resist the perturbations. And doing adversarial training at that layer usually just breaks the whole process. Given that it seems very problem dependent. There's a paper by Sara Sabore and her collaborators called adversarial manipulation of deep representations, where they design adversarial examples that are intended to fool different layers of the net. And they report some things about like how large of a perturbation is needed at the end point to get different sizes of perturbation at different hidden layers. And just spec that if you trained the model to resist perturbations at one layer, then another layer would become more vulnerable. I would be like a moving target. So the question is how many adversarial examples are needed to improve the misclassification rate? Some of our plots include learning curves. Some of our papers include learning curves. So you can actually see. This one here every time we do an epoch we've generated the same number of adversarial examples as there are training examples. So every epoch here is 50,000 adversarial examples. You can see that adversarial training is a very data hungry process that you need to make new adversarial examples every time you update the weights. And they're constantly changing in reaction to whatever the model has learned most recently. Oh, the model based optimization. The question is just to elaborate further on this problem. So most of the time that we have a machine learning model, it's something like a classifier or a regression model, where we give it an input from the test set and it gives us an output. And usually that input is randomly occurring and comes from the same distribution as the training set. And we usually just run the model, get its prediction and then we're done with it. Sometimes we have feedback loops like for recommender systems. If you work at Netflix and you recommend a movie to a viewer, then they're more likely to watch that movie and then rate it. And then there's going to be more ratings of it in your training set. So you'll recommend it to more people in the future. So this feedback loop from the output of your model to the input. Most of the time when we build machine vision systems, there's no feedback loop from their output to their input. If we imagine a setting where we start using an optimization algorithm to find inputs that maximize some property of the output. Like if we have a model that looks at the blueprints of a car and outputs the expected speed of the car, then we could use gradient ascent to look for the blueprints that correspond to the fastest. Or for example, if we're designing a medicine, we could look for the molecular structure that we think is most likely to cure some form of cancer or the least likely to cause some kind of liver toxicity effect. The problem is that once we start using optimization to look for these inputs that maximize the output of the model, the input is no longer an independent sample from the same distribution as we use that the channel. The model is now guiding the process that generates the data. So we end up finding essentially adversarial examples. The model telling us how we can improve the input, what we usually find in practice is that we've got an input that fools the model into thinking that the input corresponds to something great. So we'd find molecules that are very toxic, but the model thinks they're very non-toxic. And cars that are very slow, but the model thinks are very fast. The question is, here the frog class is boosted by going in either the positive or negative adversarial direction. And in some of the other slides like these maps, you don't get that effect where subtracting epsilon off eventually boosts the adversarial class. So what I'm going on is, I think I'm using larger epsilon here, and so you might eventually see that effect if I made these maps wider. I made them maps narrower because it's like quadratic time to build a 2D map and it's linear time to build a 1D cross section. So I just didn't afford the GPU time to make them maps quite as wide. I also think that this might just be a weird effect that happened randomly on this one example. It's not something that I remember being used to seeing a lot of the time. Most things that I observed don't happen perfectly consistently, but if they happen 80% of the time, then I'll put them in my slide. A lot of what we're doing is it's trying to figure out more or less what's going on. And so if we find that something happens 80% of the time, then I consider it to be the dominant phenomenon that we're trying to explain. After we've got a better explanation for that, then I might start to try to explain some of the weirder things that happen like the frog happening with negative epsilon. I didn't fully understand the question. It's about the dimensionality of the adversary. Okay, so the question is how is the dimension of the adversary subspace related to the dimension of the input? And my answer is somewhat embarrassing, which is that we've only run this method on two data sets. So we don't actually have a good idea yet, but it's I think it's something interesting to study. If I remember correctly, my co-authors open source code, so you could probably run it on image net without too much trouble. I actually my contribution to that paper was in the week that I was unemployed between working at OpenAI and working at Google. So I had access to no GPUs, and I ran that experiment on my laptop on CPU. So it's only really small data sets. So the question is, do we end up perturbing low confidence, clean examples to low confidence adversarial examples? Yeah, in practice we usually find that we can get very high confidence on the output examples. One thing I'd mention that's a little bit unintuitive is that just getting the sign right on very many of the input pixels is enough to get a really strong response. So the angle between the weight vector matters a lot more than the exact coordinates in high dimensional systems. Does that make enough sense? Yeah. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Okay, sounds like it is.", "tokens": [50364, 1033, 11, 3263, 411, 309, 307, 13, 50864, 50864, 286, 603, 312, 3585, 291, 466, 17641, 44745, 5110, 293, 17641, 44745, 3097, 965, 13, 51114, 51114, 286, 519, 291, 13, 51264, 51264, 1018, 364, 12492, 11, 286, 603, 722, 766, 538, 3585, 291, 437, 17641, 44745, 5110, 366, 13, 51514, 51514, 1396, 286, 603, 2903, 983, 436, 1051, 11, 983, 309, 311, 1944, 337, 552, 281, 2514, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2286490864223904, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.061136290431022644}, {"id": 1, "seek": 0, "start": 10.0, "end": 15.0, "text": " I'll be telling you about adversarial examples and adversarial training today.", "tokens": [50364, 1033, 11, 3263, 411, 309, 307, 13, 50864, 50864, 286, 603, 312, 3585, 291, 466, 17641, 44745, 5110, 293, 17641, 44745, 3097, 965, 13, 51114, 51114, 286, 519, 291, 13, 51264, 51264, 1018, 364, 12492, 11, 286, 603, 722, 766, 538, 3585, 291, 437, 17641, 44745, 5110, 366, 13, 51514, 51514, 1396, 286, 603, 2903, 983, 436, 1051, 11, 983, 309, 311, 1944, 337, 552, 281, 2514, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2286490864223904, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.061136290431022644}, {"id": 2, "seek": 0, "start": 15.0, "end": 18.0, "text": " I think you.", "tokens": [50364, 1033, 11, 3263, 411, 309, 307, 13, 50864, 50864, 286, 603, 312, 3585, 291, 466, 17641, 44745, 5110, 293, 17641, 44745, 3097, 965, 13, 51114, 51114, 286, 519, 291, 13, 51264, 51264, 1018, 364, 12492, 11, 286, 603, 722, 766, 538, 3585, 291, 437, 17641, 44745, 5110, 366, 13, 51514, 51514, 1396, 286, 603, 2903, 983, 436, 1051, 11, 983, 309, 311, 1944, 337, 552, 281, 2514, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2286490864223904, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.061136290431022644}, {"id": 3, "seek": 0, "start": 18.0, "end": 23.0, "text": " As an overview, I'll start off by telling you what adversarial examples are.", "tokens": [50364, 1033, 11, 3263, 411, 309, 307, 13, 50864, 50864, 286, 603, 312, 3585, 291, 466, 17641, 44745, 5110, 293, 17641, 44745, 3097, 965, 13, 51114, 51114, 286, 519, 291, 13, 51264, 51264, 1018, 364, 12492, 11, 286, 603, 722, 766, 538, 3585, 291, 437, 17641, 44745, 5110, 366, 13, 51514, 51514, 1396, 286, 603, 2903, 983, 436, 1051, 11, 983, 309, 311, 1944, 337, 552, 281, 2514, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2286490864223904, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.061136290431022644}, {"id": 4, "seek": 0, "start": 23.0, "end": 28.0, "text": " Then I'll explain why they happen, why it's possible for them to exist.", "tokens": [50364, 1033, 11, 3263, 411, 309, 307, 13, 50864, 50864, 286, 603, 312, 3585, 291, 466, 17641, 44745, 5110, 293, 17641, 44745, 3097, 965, 13, 51114, 51114, 286, 519, 291, 13, 51264, 51264, 1018, 364, 12492, 11, 286, 603, 722, 766, 538, 3585, 291, 437, 17641, 44745, 5110, 366, 13, 51514, 51514, 1396, 286, 603, 2903, 983, 436, 1051, 11, 983, 309, 311, 1944, 337, 552, 281, 2514, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2286490864223904, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.061136290431022644}, {"id": 5, "seek": 2800, "start": 28.0, "end": 34.0, "text": " I'll talk a little bit about how adversarial examples pose real world security threats,", "tokens": [50364, 286, 603, 751, 257, 707, 857, 466, 577, 17641, 44745, 5110, 10774, 957, 1002, 3825, 14909, 11, 50664, 50664, 300, 436, 393, 767, 312, 1143, 281, 18577, 3652, 1322, 322, 3479, 2539, 13, 50914, 50914, 286, 603, 980, 291, 437, 264, 35989, 366, 370, 1400, 11, 51064, 51064, 457, 5240, 35989, 366, 364, 1269, 2132, 1154, 300, 286, 1454, 512, 295, 291, 486, 1286, 322, 281, 14896, 13, 51364, 51364, 400, 550, 2721, 11, 286, 603, 980, 291, 577, 281, 764, 17641, 44745, 5110, 281, 3470, 661, 3479, 2539, 14642, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10280262796502364, "compression_ratio": 1.7344398340248963, "no_speech_prob": 8.185490150935948e-05}, {"id": 6, "seek": 2800, "start": 34.0, "end": 39.0, "text": " that they can actually be used to compromise systems build on machine learning.", "tokens": [50364, 286, 603, 751, 257, 707, 857, 466, 577, 17641, 44745, 5110, 10774, 957, 1002, 3825, 14909, 11, 50664, 50664, 300, 436, 393, 767, 312, 1143, 281, 18577, 3652, 1322, 322, 3479, 2539, 13, 50914, 50914, 286, 603, 980, 291, 437, 264, 35989, 366, 370, 1400, 11, 51064, 51064, 457, 5240, 35989, 366, 364, 1269, 2132, 1154, 300, 286, 1454, 512, 295, 291, 486, 1286, 322, 281, 14896, 13, 51364, 51364, 400, 550, 2721, 11, 286, 603, 980, 291, 577, 281, 764, 17641, 44745, 5110, 281, 3470, 661, 3479, 2539, 14642, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10280262796502364, "compression_ratio": 1.7344398340248963, "no_speech_prob": 8.185490150935948e-05}, {"id": 7, "seek": 2800, "start": 39.0, "end": 42.0, "text": " I'll tell you what the defenses are so far,", "tokens": [50364, 286, 603, 751, 257, 707, 857, 466, 577, 17641, 44745, 5110, 10774, 957, 1002, 3825, 14909, 11, 50664, 50664, 300, 436, 393, 767, 312, 1143, 281, 18577, 3652, 1322, 322, 3479, 2539, 13, 50914, 50914, 286, 603, 980, 291, 437, 264, 35989, 366, 370, 1400, 11, 51064, 51064, 457, 5240, 35989, 366, 364, 1269, 2132, 1154, 300, 286, 1454, 512, 295, 291, 486, 1286, 322, 281, 14896, 13, 51364, 51364, 400, 550, 2721, 11, 286, 603, 980, 291, 577, 281, 764, 17641, 44745, 5110, 281, 3470, 661, 3479, 2539, 14642, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10280262796502364, "compression_ratio": 1.7344398340248963, "no_speech_prob": 8.185490150935948e-05}, {"id": 8, "seek": 2800, "start": 42.0, "end": 48.0, "text": " but mostly defenses are an open research problem that I hope some of you will move on to tackle.", "tokens": [50364, 286, 603, 751, 257, 707, 857, 466, 577, 17641, 44745, 5110, 10774, 957, 1002, 3825, 14909, 11, 50664, 50664, 300, 436, 393, 767, 312, 1143, 281, 18577, 3652, 1322, 322, 3479, 2539, 13, 50914, 50914, 286, 603, 980, 291, 437, 264, 35989, 366, 370, 1400, 11, 51064, 51064, 457, 5240, 35989, 366, 364, 1269, 2132, 1154, 300, 286, 1454, 512, 295, 291, 486, 1286, 322, 281, 14896, 13, 51364, 51364, 400, 550, 2721, 11, 286, 603, 980, 291, 577, 281, 764, 17641, 44745, 5110, 281, 3470, 661, 3479, 2539, 14642, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10280262796502364, "compression_ratio": 1.7344398340248963, "no_speech_prob": 8.185490150935948e-05}, {"id": 9, "seek": 2800, "start": 48.0, "end": 54.0, "text": " And then finally, I'll tell you how to use adversarial examples to improve other machine learning algorithms,", "tokens": [50364, 286, 603, 751, 257, 707, 857, 466, 577, 17641, 44745, 5110, 10774, 957, 1002, 3825, 14909, 11, 50664, 50664, 300, 436, 393, 767, 312, 1143, 281, 18577, 3652, 1322, 322, 3479, 2539, 13, 50914, 50914, 286, 603, 980, 291, 437, 264, 35989, 366, 370, 1400, 11, 51064, 51064, 457, 5240, 35989, 366, 364, 1269, 2132, 1154, 300, 286, 1454, 512, 295, 291, 486, 1286, 322, 281, 14896, 13, 51364, 51364, 400, 550, 2721, 11, 286, 603, 980, 291, 577, 281, 764, 17641, 44745, 5110, 281, 3470, 661, 3479, 2539, 14642, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10280262796502364, "compression_ratio": 1.7344398340248963, "no_speech_prob": 8.185490150935948e-05}, {"id": 10, "seek": 5400, "start": 54.0, "end": 61.0, "text": " even if you want to build a machine learning algorithm that won't face a real world adversary.", "tokens": [50364, 754, 498, 291, 528, 281, 1322, 257, 3479, 2539, 9284, 300, 1582, 380, 1851, 257, 957, 1002, 48222, 13, 50714, 50714, 2053, 412, 264, 955, 3036, 293, 264, 4319, 337, 341, 7991, 13, 50964, 50964, 286, 519, 881, 295, 291, 366, 1391, 510, 570, 291, 600, 2198, 577, 6252, 4005, 293, 4406, 3479, 2539, 307, 13, 51314, 51314, 440, 588, 867, 819, 9608, 300, 727, 406, 312, 13041, 365, 4722, 949, 366, 586, 1404, 17915, 721, 281, 2452, 2539, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08586233207978398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 4.510390863288194e-05}, {"id": 11, "seek": 5400, "start": 61.0, "end": 66.0, "text": " Look at the big picture and the context for this lecture.", "tokens": [50364, 754, 498, 291, 528, 281, 1322, 257, 3479, 2539, 9284, 300, 1582, 380, 1851, 257, 957, 1002, 48222, 13, 50714, 50714, 2053, 412, 264, 955, 3036, 293, 264, 4319, 337, 341, 7991, 13, 50964, 50964, 286, 519, 881, 295, 291, 366, 1391, 510, 570, 291, 600, 2198, 577, 6252, 4005, 293, 4406, 3479, 2539, 307, 13, 51314, 51314, 440, 588, 867, 819, 9608, 300, 727, 406, 312, 13041, 365, 4722, 949, 366, 586, 1404, 17915, 721, 281, 2452, 2539, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08586233207978398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 4.510390863288194e-05}, {"id": 12, "seek": 5400, "start": 66.0, "end": 73.0, "text": " I think most of you are probably here because you've heard how incredibly powerful and successful machine learning is.", "tokens": [50364, 754, 498, 291, 528, 281, 1322, 257, 3479, 2539, 9284, 300, 1582, 380, 1851, 257, 957, 1002, 48222, 13, 50714, 50714, 2053, 412, 264, 955, 3036, 293, 264, 4319, 337, 341, 7991, 13, 50964, 50964, 286, 519, 881, 295, 291, 366, 1391, 510, 570, 291, 600, 2198, 577, 6252, 4005, 293, 4406, 3479, 2539, 307, 13, 51314, 51314, 440, 588, 867, 819, 9608, 300, 727, 406, 312, 13041, 365, 4722, 949, 366, 586, 1404, 17915, 721, 281, 2452, 2539, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08586233207978398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 4.510390863288194e-05}, {"id": 13, "seek": 5400, "start": 73.0, "end": 81.0, "text": " The very many different tasks that could not be solved with software before are now solvable things to deep learning", "tokens": [50364, 754, 498, 291, 528, 281, 1322, 257, 3479, 2539, 9284, 300, 1582, 380, 1851, 257, 957, 1002, 48222, 13, 50714, 50714, 2053, 412, 264, 955, 3036, 293, 264, 4319, 337, 341, 7991, 13, 50964, 50964, 286, 519, 881, 295, 291, 366, 1391, 510, 570, 291, 600, 2198, 577, 6252, 4005, 293, 4406, 3479, 2539, 307, 13, 51314, 51314, 440, 588, 867, 819, 9608, 300, 727, 406, 312, 13041, 365, 4722, 949, 366, 586, 1404, 17915, 721, 281, 2452, 2539, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08586233207978398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 4.510390863288194e-05}, {"id": 14, "seek": 8100, "start": 81.0, "end": 84.0, "text": " and convolutional networks and gradient descent.", "tokens": [50364, 293, 45216, 304, 9590, 293, 16235, 23475, 13, 50514, 50514, 1057, 295, 613, 7943, 300, 366, 1364, 534, 731, 13, 50664, 50664, 1042, 11, 1826, 445, 257, 1326, 924, 2057, 11, 613, 7943, 994, 380, 534, 589, 13, 50914, 50914, 682, 466, 9012, 11, 321, 1409, 281, 536, 264, 2452, 2539, 11042, 1952, 1496, 3389, 412, 257, 688, 295, 819, 9608, 13, 51264, 51264, 492, 1866, 300, 45216, 304, 36170, 727, 5521, 6565, 293, 5267, 293, 6175, 466, 264, 912, 382, 561, 294, 729, 43751, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10696324242485894, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.870811865956057e-05}, {"id": 15, "seek": 8100, "start": 84.0, "end": 87.0, "text": " All of these technologies that are working really well.", "tokens": [50364, 293, 45216, 304, 9590, 293, 16235, 23475, 13, 50514, 50514, 1057, 295, 613, 7943, 300, 366, 1364, 534, 731, 13, 50664, 50664, 1042, 11, 1826, 445, 257, 1326, 924, 2057, 11, 613, 7943, 994, 380, 534, 589, 13, 50914, 50914, 682, 466, 9012, 11, 321, 1409, 281, 536, 264, 2452, 2539, 11042, 1952, 1496, 3389, 412, 257, 688, 295, 819, 9608, 13, 51264, 51264, 492, 1866, 300, 45216, 304, 36170, 727, 5521, 6565, 293, 5267, 293, 6175, 466, 264, 912, 382, 561, 294, 729, 43751, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10696324242485894, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.870811865956057e-05}, {"id": 16, "seek": 8100, "start": 87.0, "end": 92.0, "text": " Well, until just a few years ago, these technologies didn't really work.", "tokens": [50364, 293, 45216, 304, 9590, 293, 16235, 23475, 13, 50514, 50514, 1057, 295, 613, 7943, 300, 366, 1364, 534, 731, 13, 50664, 50664, 1042, 11, 1826, 445, 257, 1326, 924, 2057, 11, 613, 7943, 994, 380, 534, 589, 13, 50914, 50914, 682, 466, 9012, 11, 321, 1409, 281, 536, 264, 2452, 2539, 11042, 1952, 1496, 3389, 412, 257, 688, 295, 819, 9608, 13, 51264, 51264, 492, 1866, 300, 45216, 304, 36170, 727, 5521, 6565, 293, 5267, 293, 6175, 466, 264, 912, 382, 561, 294, 729, 43751, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10696324242485894, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.870811865956057e-05}, {"id": 17, "seek": 8100, "start": 92.0, "end": 99.0, "text": " In about 2013, we started to see the deep learning achieved human level performance at a lot of different tasks.", "tokens": [50364, 293, 45216, 304, 9590, 293, 16235, 23475, 13, 50514, 50514, 1057, 295, 613, 7943, 300, 366, 1364, 534, 731, 13, 50664, 50664, 1042, 11, 1826, 445, 257, 1326, 924, 2057, 11, 613, 7943, 994, 380, 534, 589, 13, 50914, 50914, 682, 466, 9012, 11, 321, 1409, 281, 536, 264, 2452, 2539, 11042, 1952, 1496, 3389, 412, 257, 688, 295, 819, 9608, 13, 51264, 51264, 492, 1866, 300, 45216, 304, 36170, 727, 5521, 6565, 293, 5267, 293, 6175, 466, 264, 912, 382, 561, 294, 729, 43751, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10696324242485894, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.870811865956057e-05}, {"id": 18, "seek": 8100, "start": 99.0, "end": 108.0, "text": " We saw that convolutional nets could recognize objects and images and score about the same as people in those benchmarks.", "tokens": [50364, 293, 45216, 304, 9590, 293, 16235, 23475, 13, 50514, 50514, 1057, 295, 613, 7943, 300, 366, 1364, 534, 731, 13, 50664, 50664, 1042, 11, 1826, 445, 257, 1326, 924, 2057, 11, 613, 7943, 994, 380, 534, 589, 13, 50914, 50914, 682, 466, 9012, 11, 321, 1409, 281, 536, 264, 2452, 2539, 11042, 1952, 1496, 3389, 412, 257, 688, 295, 819, 9608, 13, 51264, 51264, 492, 1866, 300, 45216, 304, 36170, 727, 5521, 6565, 293, 5267, 293, 6175, 466, 264, 912, 382, 561, 294, 729, 43751, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10696324242485894, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.870811865956057e-05}, {"id": 19, "seek": 10800, "start": 108.0, "end": 116.0, "text": " With the caveat, the part of the reason that algorithm score as well as people is that people can't tell a less than Huskies from Siberian Huskies very well.", "tokens": [50364, 2022, 264, 43012, 11, 264, 644, 295, 264, 1778, 300, 9284, 6175, 382, 731, 382, 561, 307, 300, 561, 393, 380, 980, 257, 1570, 813, 21282, 74, 530, 490, 42608, 952, 21282, 74, 530, 588, 731, 13, 50764, 50764, 583, 31111, 11, 264, 1056, 10784, 442, 295, 264, 43751, 2452, 2539, 5415, 493, 281, 466, 1952, 1496, 3389, 337, 2657, 11150, 294, 466, 9012, 13, 51264, 51264, 440, 912, 1064, 11, 321, 611, 1866, 300, 2657, 11150, 6456, 281, 1952, 8475, 5415, 493, 281, 466, 1952, 1496, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14388877412547235, "compression_ratio": 1.7456896551724137, "no_speech_prob": 2.0339599359431304e-05}, {"id": 20, "seek": 10800, "start": 116.0, "end": 126.0, "text": " But modular, the strangeness of the benchmarks deep learning caught up to about human level performance for object recognition in about 2013.", "tokens": [50364, 2022, 264, 43012, 11, 264, 644, 295, 264, 1778, 300, 9284, 6175, 382, 731, 382, 561, 307, 300, 561, 393, 380, 980, 257, 1570, 813, 21282, 74, 530, 490, 42608, 952, 21282, 74, 530, 588, 731, 13, 50764, 50764, 583, 31111, 11, 264, 1056, 10784, 442, 295, 264, 43751, 2452, 2539, 5415, 493, 281, 466, 1952, 1496, 3389, 337, 2657, 11150, 294, 466, 9012, 13, 51264, 51264, 440, 912, 1064, 11, 321, 611, 1866, 300, 2657, 11150, 6456, 281, 1952, 8475, 5415, 493, 281, 466, 1952, 1496, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14388877412547235, "compression_ratio": 1.7456896551724137, "no_speech_prob": 2.0339599359431304e-05}, {"id": 21, "seek": 10800, "start": 126.0, "end": 133.0, "text": " The same year, we also saw that object recognition applied to human faces caught up to about human level.", "tokens": [50364, 2022, 264, 43012, 11, 264, 644, 295, 264, 1778, 300, 9284, 6175, 382, 731, 382, 561, 307, 300, 561, 393, 380, 980, 257, 1570, 813, 21282, 74, 530, 490, 42608, 952, 21282, 74, 530, 588, 731, 13, 50764, 50764, 583, 31111, 11, 264, 1056, 10784, 442, 295, 264, 43751, 2452, 2539, 5415, 493, 281, 466, 1952, 1496, 3389, 337, 2657, 11150, 294, 466, 9012, 13, 51264, 51264, 440, 912, 1064, 11, 321, 611, 1866, 300, 2657, 11150, 6456, 281, 1952, 8475, 5415, 493, 281, 466, 1952, 1496, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14388877412547235, "compression_ratio": 1.7456896551724137, "no_speech_prob": 2.0339599359431304e-05}, {"id": 22, "seek": 13300, "start": 133.0, "end": 142.0, "text": " That suddenly we had computers that could recognize faces about as well as you or I could recognize faces of strangers.", "tokens": [50364, 663, 5800, 321, 632, 10807, 300, 727, 5521, 8475, 466, 382, 731, 382, 291, 420, 286, 727, 5521, 8475, 295, 22724, 13, 50814, 50814, 509, 393, 5521, 264, 8475, 295, 428, 1855, 293, 1605, 1101, 813, 257, 3820, 13, 51064, 51064, 583, 562, 291, 434, 6260, 365, 561, 300, 291, 2378, 380, 632, 257, 688, 295, 1752, 365, 264, 3820, 5415, 493, 281, 505, 294, 466, 9012, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.067335221502516, "compression_ratio": 1.6446700507614214, "no_speech_prob": 3.716344144777395e-05}, {"id": 23, "seek": 13300, "start": 142.0, "end": 147.0, "text": " You can recognize the faces of your friends and family better than a computer.", "tokens": [50364, 663, 5800, 321, 632, 10807, 300, 727, 5521, 8475, 466, 382, 731, 382, 291, 420, 286, 727, 5521, 8475, 295, 22724, 13, 50814, 50814, 509, 393, 5521, 264, 8475, 295, 428, 1855, 293, 1605, 1101, 813, 257, 3820, 13, 51064, 51064, 583, 562, 291, 434, 6260, 365, 561, 300, 291, 2378, 380, 632, 257, 688, 295, 1752, 365, 264, 3820, 5415, 493, 281, 505, 294, 466, 9012, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.067335221502516, "compression_ratio": 1.6446700507614214, "no_speech_prob": 3.716344144777395e-05}, {"id": 24, "seek": 13300, "start": 147.0, "end": 155.0, "text": " But when you're dealing with people that you haven't had a lot of experience with the computer caught up to us in about 2013.", "tokens": [50364, 663, 5800, 321, 632, 10807, 300, 727, 5521, 8475, 466, 382, 731, 382, 291, 420, 286, 727, 5521, 8475, 295, 22724, 13, 50814, 50814, 509, 393, 5521, 264, 8475, 295, 428, 1855, 293, 1605, 1101, 813, 257, 3820, 13, 51064, 51064, 583, 562, 291, 434, 6260, 365, 561, 300, 291, 2378, 380, 632, 257, 688, 295, 1752, 365, 264, 3820, 5415, 493, 281, 505, 294, 466, 9012, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.067335221502516, "compression_ratio": 1.6446700507614214, "no_speech_prob": 3.716344144777395e-05}, {"id": 25, "seek": 15500, "start": 155.0, "end": 164.0, "text": " We also saw that computers caught up to humans for reading typewritten fonts in photos in about 2013.", "tokens": [50364, 492, 611, 1866, 300, 10807, 5415, 493, 281, 6255, 337, 3760, 2010, 26859, 35316, 294, 5787, 294, 466, 9012, 13, 50814, 50814, 467, 754, 658, 281, 264, 935, 300, 321, 393, 572, 2854, 764, 27986, 281, 980, 1968, 257, 4195, 295, 257, 3670, 3028, 307, 1952, 420, 406, 11, 570, 264, 45216, 304, 3209, 307, 1101, 412, 3760, 493, 12, 675, 12, 4417, 770, 2487, 813, 257, 1952, 307, 13, 51464, 51464, 407, 365, 341, 4319, 965, 295, 2452, 2539, 1364, 534, 731, 11, 2318, 337, 3820, 5201, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15860406814082975, "compression_ratio": 1.608, "no_speech_prob": 1.1132468898722436e-05}, {"id": 26, "seek": 15500, "start": 164.0, "end": 177.0, "text": " It even got to the point that we can no longer use captures to tell whether a user of a web page is human or not, because the convolutional network is better at reading up-he-scated text than a human is.", "tokens": [50364, 492, 611, 1866, 300, 10807, 5415, 493, 281, 6255, 337, 3760, 2010, 26859, 35316, 294, 5787, 294, 466, 9012, 13, 50814, 50814, 467, 754, 658, 281, 264, 935, 300, 321, 393, 572, 2854, 764, 27986, 281, 980, 1968, 257, 4195, 295, 257, 3670, 3028, 307, 1952, 420, 406, 11, 570, 264, 45216, 304, 3209, 307, 1101, 412, 3760, 493, 12, 675, 12, 4417, 770, 2487, 813, 257, 1952, 307, 13, 51464, 51464, 407, 365, 341, 4319, 965, 295, 2452, 2539, 1364, 534, 731, 11, 2318, 337, 3820, 5201, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15860406814082975, "compression_ratio": 1.608, "no_speech_prob": 1.1132468898722436e-05}, {"id": 27, "seek": 15500, "start": 177.0, "end": 183.0, "text": " So with this context today of deep learning working really well, especially for computer vision,", "tokens": [50364, 492, 611, 1866, 300, 10807, 5415, 493, 281, 6255, 337, 3760, 2010, 26859, 35316, 294, 5787, 294, 466, 9012, 13, 50814, 50814, 467, 754, 658, 281, 264, 935, 300, 321, 393, 572, 2854, 764, 27986, 281, 980, 1968, 257, 4195, 295, 257, 3670, 3028, 307, 1952, 420, 406, 11, 570, 264, 45216, 304, 3209, 307, 1101, 412, 3760, 493, 12, 675, 12, 4417, 770, 2487, 813, 257, 1952, 307, 13, 51464, 51464, 407, 365, 341, 4319, 965, 295, 2452, 2539, 1364, 534, 731, 11, 2318, 337, 3820, 5201, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15860406814082975, "compression_ratio": 1.608, "no_speech_prob": 1.1132468898722436e-05}, {"id": 28, "seek": 18300, "start": 183.0, "end": 188.0, "text": " it's a little bit unusual to think about the computer making a mistake.", "tokens": [50364, 309, 311, 257, 707, 857, 10901, 281, 519, 466, 264, 3820, 1455, 257, 6146, 13, 50614, 50614, 4546, 466, 9012, 11, 5079, 390, 1562, 6100, 498, 264, 3820, 1027, 257, 6146, 13, 50864, 50864, 663, 390, 264, 4978, 11, 406, 264, 11183, 13, 50964, 50964, 400, 370, 965, 311, 4829, 11, 597, 307, 439, 466, 10901, 8038, 300, 2452, 2539, 14642, 652, 11, 51264, 51264, 341, 4829, 2067, 380, 534, 257, 3156, 39230, 295, 2979, 1826, 264, 9284, 1409, 281, 589, 731, 881, 295, 264, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06646251678466797, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.3602546207257546e-05}, {"id": 29, "seek": 18300, "start": 188.0, "end": 193.0, "text": " Before about 2013, nobody was ever surprised if the computer made a mistake.", "tokens": [50364, 309, 311, 257, 707, 857, 10901, 281, 519, 466, 264, 3820, 1455, 257, 6146, 13, 50614, 50614, 4546, 466, 9012, 11, 5079, 390, 1562, 6100, 498, 264, 3820, 1027, 257, 6146, 13, 50864, 50864, 663, 390, 264, 4978, 11, 406, 264, 11183, 13, 50964, 50964, 400, 370, 965, 311, 4829, 11, 597, 307, 439, 466, 10901, 8038, 300, 2452, 2539, 14642, 652, 11, 51264, 51264, 341, 4829, 2067, 380, 534, 257, 3156, 39230, 295, 2979, 1826, 264, 9284, 1409, 281, 589, 731, 881, 295, 264, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06646251678466797, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.3602546207257546e-05}, {"id": 30, "seek": 18300, "start": 193.0, "end": 195.0, "text": " That was the rule, not the exception.", "tokens": [50364, 309, 311, 257, 707, 857, 10901, 281, 519, 466, 264, 3820, 1455, 257, 6146, 13, 50614, 50614, 4546, 466, 9012, 11, 5079, 390, 1562, 6100, 498, 264, 3820, 1027, 257, 6146, 13, 50864, 50864, 663, 390, 264, 4978, 11, 406, 264, 11183, 13, 50964, 50964, 400, 370, 965, 311, 4829, 11, 597, 307, 439, 466, 10901, 8038, 300, 2452, 2539, 14642, 652, 11, 51264, 51264, 341, 4829, 2067, 380, 534, 257, 3156, 39230, 295, 2979, 1826, 264, 9284, 1409, 281, 589, 731, 881, 295, 264, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06646251678466797, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.3602546207257546e-05}, {"id": 31, "seek": 18300, "start": 195.0, "end": 201.0, "text": " And so today's topic, which is all about unusual mistakes that deep learning algorithms make,", "tokens": [50364, 309, 311, 257, 707, 857, 10901, 281, 519, 466, 264, 3820, 1455, 257, 6146, 13, 50614, 50614, 4546, 466, 9012, 11, 5079, 390, 1562, 6100, 498, 264, 3820, 1027, 257, 6146, 13, 50864, 50864, 663, 390, 264, 4978, 11, 406, 264, 11183, 13, 50964, 50964, 400, 370, 965, 311, 4829, 11, 597, 307, 439, 466, 10901, 8038, 300, 2452, 2539, 14642, 652, 11, 51264, 51264, 341, 4829, 2067, 380, 534, 257, 3156, 39230, 295, 2979, 1826, 264, 9284, 1409, 281, 589, 731, 881, 295, 264, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06646251678466797, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.3602546207257546e-05}, {"id": 32, "seek": 18300, "start": 201.0, "end": 208.0, "text": " this topic wasn't really a serious avenue of study until the algorithm started to work well most of the time.", "tokens": [50364, 309, 311, 257, 707, 857, 10901, 281, 519, 466, 264, 3820, 1455, 257, 6146, 13, 50614, 50614, 4546, 466, 9012, 11, 5079, 390, 1562, 6100, 498, 264, 3820, 1027, 257, 6146, 13, 50864, 50864, 663, 390, 264, 4978, 11, 406, 264, 11183, 13, 50964, 50964, 400, 370, 965, 311, 4829, 11, 597, 307, 439, 466, 10901, 8038, 300, 2452, 2539, 14642, 652, 11, 51264, 51264, 341, 4829, 2067, 380, 534, 257, 3156, 39230, 295, 2979, 1826, 264, 9284, 1409, 281, 589, 731, 881, 295, 264, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06646251678466797, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.3602546207257546e-05}, {"id": 33, "seek": 20800, "start": 208.0, "end": 216.0, "text": " And now we will study the way that they break. Now that that's actually the exception rather than the rule.", "tokens": [50364, 400, 586, 321, 486, 2979, 264, 636, 300, 436, 1821, 13, 823, 300, 300, 311, 767, 264, 11183, 2831, 813, 264, 4978, 13, 50764, 50764, 1107, 17641, 44745, 1365, 307, 364, 1365, 300, 575, 668, 7500, 40610, 281, 312, 3346, 11665, 2587, 13, 51164, 51164, 682, 257, 688, 295, 3331, 321, 434, 1075, 281, 652, 264, 777, 3256, 1016, 468, 7050, 742, 712, 281, 257, 1952, 27878, 490, 264, 3380, 3256, 13, 51464, 51464, 1692, 286, 855, 291, 472, 689, 321, 722, 365, 257, 46685, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09473011228773329, "compression_ratio": 1.5764192139737991, "no_speech_prob": 1.1735124644474126e-05}, {"id": 34, "seek": 20800, "start": 216.0, "end": 224.0, "text": " An adversarial example is an example that has been carefully computed to be misclassified.", "tokens": [50364, 400, 586, 321, 486, 2979, 264, 636, 300, 436, 1821, 13, 823, 300, 300, 311, 767, 264, 11183, 2831, 813, 264, 4978, 13, 50764, 50764, 1107, 17641, 44745, 1365, 307, 364, 1365, 300, 575, 668, 7500, 40610, 281, 312, 3346, 11665, 2587, 13, 51164, 51164, 682, 257, 688, 295, 3331, 321, 434, 1075, 281, 652, 264, 777, 3256, 1016, 468, 7050, 742, 712, 281, 257, 1952, 27878, 490, 264, 3380, 3256, 13, 51464, 51464, 1692, 286, 855, 291, 472, 689, 321, 722, 365, 257, 46685, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09473011228773329, "compression_ratio": 1.5764192139737991, "no_speech_prob": 1.1735124644474126e-05}, {"id": 35, "seek": 20800, "start": 224.0, "end": 230.0, "text": " In a lot of cases we're able to make the new image indistinguishable to a human observer from the original image.", "tokens": [50364, 400, 586, 321, 486, 2979, 264, 636, 300, 436, 1821, 13, 823, 300, 300, 311, 767, 264, 11183, 2831, 813, 264, 4978, 13, 50764, 50764, 1107, 17641, 44745, 1365, 307, 364, 1365, 300, 575, 668, 7500, 40610, 281, 312, 3346, 11665, 2587, 13, 51164, 51164, 682, 257, 688, 295, 3331, 321, 434, 1075, 281, 652, 264, 777, 3256, 1016, 468, 7050, 742, 712, 281, 257, 1952, 27878, 490, 264, 3380, 3256, 13, 51464, 51464, 1692, 286, 855, 291, 472, 689, 321, 722, 365, 257, 46685, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09473011228773329, "compression_ratio": 1.5764192139737991, "no_speech_prob": 1.1735124644474126e-05}, {"id": 36, "seek": 20800, "start": 230.0, "end": 233.0, "text": " Here I show you one where we start with a panda.", "tokens": [50364, 400, 586, 321, 486, 2979, 264, 636, 300, 436, 1821, 13, 823, 300, 300, 311, 767, 264, 11183, 2831, 813, 264, 4978, 13, 50764, 50764, 1107, 17641, 44745, 1365, 307, 364, 1365, 300, 575, 668, 7500, 40610, 281, 312, 3346, 11665, 2587, 13, 51164, 51164, 682, 257, 688, 295, 3331, 321, 434, 1075, 281, 652, 264, 777, 3256, 1016, 468, 7050, 742, 712, 281, 257, 1952, 27878, 490, 264, 3380, 3256, 13, 51464, 51464, 1692, 286, 855, 291, 472, 689, 321, 722, 365, 257, 46685, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09473011228773329, "compression_ratio": 1.5764192139737991, "no_speech_prob": 1.1735124644474126e-05}, {"id": 37, "seek": 23300, "start": 233.0, "end": 238.0, "text": " On the left, this is a panda that has not been modified in any way.", "tokens": [50364, 1282, 264, 1411, 11, 341, 307, 257, 46685, 300, 575, 406, 668, 15873, 294, 604, 636, 13, 50614, 50614, 400, 257, 45216, 304, 3209, 8895, 322, 264, 3256, 2533, 1412, 992, 307, 1075, 281, 5521, 309, 382, 885, 257, 46685, 13, 50914, 50914, 1485, 1880, 551, 307, 300, 264, 2316, 1177, 380, 362, 257, 1379, 688, 295, 6687, 294, 300, 3537, 13, 51164, 51164, 467, 6269, 82, 466, 4060, 4, 8482, 281, 341, 3256, 885, 257, 46685, 13, 51414, 51414, 759, 321, 550, 14722, 2293, 264, 636, 300, 321, 727, 16927, 264, 3256, 281, 3082, 264, 45216, 304, 3209, 281, 652, 257, 6146, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055252141422695585, "compression_ratio": 1.7110266159695817, "no_speech_prob": 1.851785600592848e-05}, {"id": 38, "seek": 23300, "start": 238.0, "end": 244.0, "text": " And a convolutional network trained on the image net data set is able to recognize it as being a panda.", "tokens": [50364, 1282, 264, 1411, 11, 341, 307, 257, 46685, 300, 575, 406, 668, 15873, 294, 604, 636, 13, 50614, 50614, 400, 257, 45216, 304, 3209, 8895, 322, 264, 3256, 2533, 1412, 992, 307, 1075, 281, 5521, 309, 382, 885, 257, 46685, 13, 50914, 50914, 1485, 1880, 551, 307, 300, 264, 2316, 1177, 380, 362, 257, 1379, 688, 295, 6687, 294, 300, 3537, 13, 51164, 51164, 467, 6269, 82, 466, 4060, 4, 8482, 281, 341, 3256, 885, 257, 46685, 13, 51414, 51414, 759, 321, 550, 14722, 2293, 264, 636, 300, 321, 727, 16927, 264, 3256, 281, 3082, 264, 45216, 304, 3209, 281, 652, 257, 6146, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055252141422695585, "compression_ratio": 1.7110266159695817, "no_speech_prob": 1.851785600592848e-05}, {"id": 39, "seek": 23300, "start": 244.0, "end": 249.0, "text": " One interesting thing is that the model doesn't have a whole lot of confidence in that decision.", "tokens": [50364, 1282, 264, 1411, 11, 341, 307, 257, 46685, 300, 575, 406, 668, 15873, 294, 604, 636, 13, 50614, 50614, 400, 257, 45216, 304, 3209, 8895, 322, 264, 3256, 2533, 1412, 992, 307, 1075, 281, 5521, 309, 382, 885, 257, 46685, 13, 50914, 50914, 1485, 1880, 551, 307, 300, 264, 2316, 1177, 380, 362, 257, 1379, 688, 295, 6687, 294, 300, 3537, 13, 51164, 51164, 467, 6269, 82, 466, 4060, 4, 8482, 281, 341, 3256, 885, 257, 46685, 13, 51414, 51414, 759, 321, 550, 14722, 2293, 264, 636, 300, 321, 727, 16927, 264, 3256, 281, 3082, 264, 45216, 304, 3209, 281, 652, 257, 6146, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055252141422695585, "compression_ratio": 1.7110266159695817, "no_speech_prob": 1.851785600592848e-05}, {"id": 40, "seek": 23300, "start": 249.0, "end": 254.0, "text": " It assigns about 60% probability to this image being a panda.", "tokens": [50364, 1282, 264, 1411, 11, 341, 307, 257, 46685, 300, 575, 406, 668, 15873, 294, 604, 636, 13, 50614, 50614, 400, 257, 45216, 304, 3209, 8895, 322, 264, 3256, 2533, 1412, 992, 307, 1075, 281, 5521, 309, 382, 885, 257, 46685, 13, 50914, 50914, 1485, 1880, 551, 307, 300, 264, 2316, 1177, 380, 362, 257, 1379, 688, 295, 6687, 294, 300, 3537, 13, 51164, 51164, 467, 6269, 82, 466, 4060, 4, 8482, 281, 341, 3256, 885, 257, 46685, 13, 51414, 51414, 759, 321, 550, 14722, 2293, 264, 636, 300, 321, 727, 16927, 264, 3256, 281, 3082, 264, 45216, 304, 3209, 281, 652, 257, 6146, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055252141422695585, "compression_ratio": 1.7110266159695817, "no_speech_prob": 1.851785600592848e-05}, {"id": 41, "seek": 23300, "start": 254.0, "end": 261.0, "text": " If we then compute exactly the way that we could modify the image to cause the convolutional network to make a mistake,", "tokens": [50364, 1282, 264, 1411, 11, 341, 307, 257, 46685, 300, 575, 406, 668, 15873, 294, 604, 636, 13, 50614, 50614, 400, 257, 45216, 304, 3209, 8895, 322, 264, 3256, 2533, 1412, 992, 307, 1075, 281, 5521, 309, 382, 885, 257, 46685, 13, 50914, 50914, 1485, 1880, 551, 307, 300, 264, 2316, 1177, 380, 362, 257, 1379, 688, 295, 6687, 294, 300, 3537, 13, 51164, 51164, 467, 6269, 82, 466, 4060, 4, 8482, 281, 341, 3256, 885, 257, 46685, 13, 51414, 51414, 759, 321, 550, 14722, 2293, 264, 636, 300, 321, 727, 16927, 264, 3256, 281, 3082, 264, 45216, 304, 3209, 281, 652, 257, 6146, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055252141422695585, "compression_ratio": 1.7110266159695817, "no_speech_prob": 1.851785600592848e-05}, {"id": 42, "seek": 26100, "start": 261.0, "end": 267.0, "text": " we find that the optimal direction to move all the pixels is given by this image in the middle.", "tokens": [50364, 321, 915, 300, 264, 16252, 3513, 281, 1286, 439, 264, 18668, 307, 2212, 538, 341, 3256, 294, 264, 2808, 13, 50664, 50664, 1407, 257, 1952, 309, 1542, 257, 688, 411, 5658, 13, 50814, 50814, 467, 311, 406, 767, 5658, 13, 467, 311, 7500, 40610, 382, 257, 2445, 295, 264, 9834, 295, 264, 3209, 13, 51064, 51064, 821, 311, 767, 257, 688, 295, 3877, 456, 13, 51164, 51164, 759, 321, 12972, 300, 3256, 295, 264, 18519, 2690, 538, 257, 588, 1359, 17619, 293, 909, 309, 281, 264, 3380, 46685, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0576692755504321, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.4300179575220682e-05}, {"id": 43, "seek": 26100, "start": 267.0, "end": 270.0, "text": " To a human it looks a lot like noise.", "tokens": [50364, 321, 915, 300, 264, 16252, 3513, 281, 1286, 439, 264, 18668, 307, 2212, 538, 341, 3256, 294, 264, 2808, 13, 50664, 50664, 1407, 257, 1952, 309, 1542, 257, 688, 411, 5658, 13, 50814, 50814, 467, 311, 406, 767, 5658, 13, 467, 311, 7500, 40610, 382, 257, 2445, 295, 264, 9834, 295, 264, 3209, 13, 51064, 51064, 821, 311, 767, 257, 688, 295, 3877, 456, 13, 51164, 51164, 759, 321, 12972, 300, 3256, 295, 264, 18519, 2690, 538, 257, 588, 1359, 17619, 293, 909, 309, 281, 264, 3380, 46685, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0576692755504321, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.4300179575220682e-05}, {"id": 44, "seek": 26100, "start": 270.0, "end": 275.0, "text": " It's not actually noise. It's carefully computed as a function of the parameters of the network.", "tokens": [50364, 321, 915, 300, 264, 16252, 3513, 281, 1286, 439, 264, 18668, 307, 2212, 538, 341, 3256, 294, 264, 2808, 13, 50664, 50664, 1407, 257, 1952, 309, 1542, 257, 688, 411, 5658, 13, 50814, 50814, 467, 311, 406, 767, 5658, 13, 467, 311, 7500, 40610, 382, 257, 2445, 295, 264, 9834, 295, 264, 3209, 13, 51064, 51064, 821, 311, 767, 257, 688, 295, 3877, 456, 13, 51164, 51164, 759, 321, 12972, 300, 3256, 295, 264, 18519, 2690, 538, 257, 588, 1359, 17619, 293, 909, 309, 281, 264, 3380, 46685, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0576692755504321, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.4300179575220682e-05}, {"id": 45, "seek": 26100, "start": 275.0, "end": 277.0, "text": " There's actually a lot of structure there.", "tokens": [50364, 321, 915, 300, 264, 16252, 3513, 281, 1286, 439, 264, 18668, 307, 2212, 538, 341, 3256, 294, 264, 2808, 13, 50664, 50664, 1407, 257, 1952, 309, 1542, 257, 688, 411, 5658, 13, 50814, 50814, 467, 311, 406, 767, 5658, 13, 467, 311, 7500, 40610, 382, 257, 2445, 295, 264, 9834, 295, 264, 3209, 13, 51064, 51064, 821, 311, 767, 257, 688, 295, 3877, 456, 13, 51164, 51164, 759, 321, 12972, 300, 3256, 295, 264, 18519, 2690, 538, 257, 588, 1359, 17619, 293, 909, 309, 281, 264, 3380, 46685, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0576692755504321, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.4300179575220682e-05}, {"id": 46, "seek": 26100, "start": 277.0, "end": 286.0, "text": " If we multiply that image of the structured attack by a very small coefficient and add it to the original panda,", "tokens": [50364, 321, 915, 300, 264, 16252, 3513, 281, 1286, 439, 264, 18668, 307, 2212, 538, 341, 3256, 294, 264, 2808, 13, 50664, 50664, 1407, 257, 1952, 309, 1542, 257, 688, 411, 5658, 13, 50814, 50814, 467, 311, 406, 767, 5658, 13, 467, 311, 7500, 40610, 382, 257, 2445, 295, 264, 9834, 295, 264, 3209, 13, 51064, 51064, 821, 311, 767, 257, 688, 295, 3877, 456, 13, 51164, 51164, 759, 321, 12972, 300, 3256, 295, 264, 18519, 2690, 538, 257, 588, 1359, 17619, 293, 909, 309, 281, 264, 3380, 46685, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0576692755504321, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.4300179575220682e-05}, {"id": 47, "seek": 28600, "start": 286.0, "end": 291.0, "text": " we get an image that a human can't tell from the original panda.", "tokens": [50364, 321, 483, 364, 3256, 300, 257, 1952, 393, 380, 980, 490, 264, 3380, 46685, 13, 50614, 50614, 682, 1186, 11, 322, 341, 4137, 456, 307, 572, 2649, 1296, 264, 46685, 322, 264, 1411, 293, 264, 46685, 322, 264, 558, 13, 50914, 50914, 1133, 321, 1974, 264, 3256, 281, 264, 45216, 304, 3209, 11, 321, 764, 8858, 857, 12607, 935, 4190, 13, 51214, 51214, 440, 6002, 510, 393, 787, 4674, 1649, 9239, 295, 2017, 8669, 13, 51414, 51414, 400, 321, 600, 1027, 257, 1319, 300, 311, 445, 10268, 886, 1359, 281, 3345, 264, 16998, 295, 729, 1649, 9239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06938541169260062, "compression_ratio": 1.6507936507936507, "no_speech_prob": 7.354048648267053e-06}, {"id": 48, "seek": 28600, "start": 291.0, "end": 297.0, "text": " In fact, on this slide there is no difference between the panda on the left and the panda on the right.", "tokens": [50364, 321, 483, 364, 3256, 300, 257, 1952, 393, 380, 980, 490, 264, 3380, 46685, 13, 50614, 50614, 682, 1186, 11, 322, 341, 4137, 456, 307, 572, 2649, 1296, 264, 46685, 322, 264, 1411, 293, 264, 46685, 322, 264, 558, 13, 50914, 50914, 1133, 321, 1974, 264, 3256, 281, 264, 45216, 304, 3209, 11, 321, 764, 8858, 857, 12607, 935, 4190, 13, 51214, 51214, 440, 6002, 510, 393, 787, 4674, 1649, 9239, 295, 2017, 8669, 13, 51414, 51414, 400, 321, 600, 1027, 257, 1319, 300, 311, 445, 10268, 886, 1359, 281, 3345, 264, 16998, 295, 729, 1649, 9239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06938541169260062, "compression_ratio": 1.6507936507936507, "no_speech_prob": 7.354048648267053e-06}, {"id": 49, "seek": 28600, "start": 297.0, "end": 303.0, "text": " When we present the image to the convolutional network, we use 32 bit floating point values.", "tokens": [50364, 321, 483, 364, 3256, 300, 257, 1952, 393, 380, 980, 490, 264, 3380, 46685, 13, 50614, 50614, 682, 1186, 11, 322, 341, 4137, 456, 307, 572, 2649, 1296, 264, 46685, 322, 264, 1411, 293, 264, 46685, 322, 264, 558, 13, 50914, 50914, 1133, 321, 1974, 264, 3256, 281, 264, 45216, 304, 3209, 11, 321, 764, 8858, 857, 12607, 935, 4190, 13, 51214, 51214, 440, 6002, 510, 393, 787, 4674, 1649, 9239, 295, 2017, 8669, 13, 51414, 51414, 400, 321, 600, 1027, 257, 1319, 300, 311, 445, 10268, 886, 1359, 281, 3345, 264, 16998, 295, 729, 1649, 9239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06938541169260062, "compression_ratio": 1.6507936507936507, "no_speech_prob": 7.354048648267053e-06}, {"id": 50, "seek": 28600, "start": 303.0, "end": 307.0, "text": " The monitor here can only display 8 bits of color resolution.", "tokens": [50364, 321, 483, 364, 3256, 300, 257, 1952, 393, 380, 980, 490, 264, 3380, 46685, 13, 50614, 50614, 682, 1186, 11, 322, 341, 4137, 456, 307, 572, 2649, 1296, 264, 46685, 322, 264, 1411, 293, 264, 46685, 322, 264, 558, 13, 50914, 50914, 1133, 321, 1974, 264, 3256, 281, 264, 45216, 304, 3209, 11, 321, 764, 8858, 857, 12607, 935, 4190, 13, 51214, 51214, 440, 6002, 510, 393, 787, 4674, 1649, 9239, 295, 2017, 8669, 13, 51414, 51414, 400, 321, 600, 1027, 257, 1319, 300, 311, 445, 10268, 886, 1359, 281, 3345, 264, 16998, 295, 729, 1649, 9239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06938541169260062, "compression_ratio": 1.6507936507936507, "no_speech_prob": 7.354048648267053e-06}, {"id": 51, "seek": 28600, "start": 307.0, "end": 313.0, "text": " And we've made a change that's just barely too small to affect the smallest of those 8 bits.", "tokens": [50364, 321, 483, 364, 3256, 300, 257, 1952, 393, 380, 980, 490, 264, 3380, 46685, 13, 50614, 50614, 682, 1186, 11, 322, 341, 4137, 456, 307, 572, 2649, 1296, 264, 46685, 322, 264, 1411, 293, 264, 46685, 322, 264, 558, 13, 50914, 50914, 1133, 321, 1974, 264, 3256, 281, 264, 45216, 304, 3209, 11, 321, 764, 8858, 857, 12607, 935, 4190, 13, 51214, 51214, 440, 6002, 510, 393, 787, 4674, 1649, 9239, 295, 2017, 8669, 13, 51414, 51414, 400, 321, 600, 1027, 257, 1319, 300, 311, 445, 10268, 886, 1359, 281, 3345, 264, 16998, 295, 729, 1649, 9239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06938541169260062, "compression_ratio": 1.6507936507936507, "no_speech_prob": 7.354048648267053e-06}, {"id": 52, "seek": 31300, "start": 313.0, "end": 318.0, "text": " But it affects the other 24 of the 32 bit floating point representation.", "tokens": [50364, 583, 309, 11807, 264, 661, 4022, 295, 264, 8858, 857, 12607, 935, 10290, 13, 50614, 50614, 663, 707, 5870, 1319, 307, 1547, 281, 7979, 264, 45216, 304, 3209, 666, 18538, 341, 3256, 295, 257, 46685, 382, 885, 257, 2212, 13, 51064, 51064, 3996, 1880, 551, 307, 300, 309, 1177, 380, 445, 1319, 264, 1508, 13, 51264, 51264, 467, 311, 406, 300, 321, 445, 10268, 1352, 264, 3537, 12866, 293, 445, 10268, 15251, 2108, 309, 13, 51464, 51464, 440, 45216, 304, 3209, 767, 575, 709, 544, 6687, 294, 1080, 18424, 17630, 300, 264, 3256, 322, 264, 558, 307, 257, 2212, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05313772880114042, "compression_ratio": 1.7383512544802868, "no_speech_prob": 5.32327794644516e-06}, {"id": 53, "seek": 31300, "start": 318.0, "end": 327.0, "text": " That little tiny change is enough to fool the convolutional network into recognizing this image of a panda as being a given.", "tokens": [50364, 583, 309, 11807, 264, 661, 4022, 295, 264, 8858, 857, 12607, 935, 10290, 13, 50614, 50614, 663, 707, 5870, 1319, 307, 1547, 281, 7979, 264, 45216, 304, 3209, 666, 18538, 341, 3256, 295, 257, 46685, 382, 885, 257, 2212, 13, 51064, 51064, 3996, 1880, 551, 307, 300, 309, 1177, 380, 445, 1319, 264, 1508, 13, 51264, 51264, 467, 311, 406, 300, 321, 445, 10268, 1352, 264, 3537, 12866, 293, 445, 10268, 15251, 2108, 309, 13, 51464, 51464, 440, 45216, 304, 3209, 767, 575, 709, 544, 6687, 294, 1080, 18424, 17630, 300, 264, 3256, 322, 264, 558, 307, 257, 2212, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05313772880114042, "compression_ratio": 1.7383512544802868, "no_speech_prob": 5.32327794644516e-06}, {"id": 54, "seek": 31300, "start": 327.0, "end": 331.0, "text": " Another interesting thing is that it doesn't just change the class.", "tokens": [50364, 583, 309, 11807, 264, 661, 4022, 295, 264, 8858, 857, 12607, 935, 10290, 13, 50614, 50614, 663, 707, 5870, 1319, 307, 1547, 281, 7979, 264, 45216, 304, 3209, 666, 18538, 341, 3256, 295, 257, 46685, 382, 885, 257, 2212, 13, 51064, 51064, 3996, 1880, 551, 307, 300, 309, 1177, 380, 445, 1319, 264, 1508, 13, 51264, 51264, 467, 311, 406, 300, 321, 445, 10268, 1352, 264, 3537, 12866, 293, 445, 10268, 15251, 2108, 309, 13, 51464, 51464, 440, 45216, 304, 3209, 767, 575, 709, 544, 6687, 294, 1080, 18424, 17630, 300, 264, 3256, 322, 264, 558, 307, 257, 2212, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05313772880114042, "compression_ratio": 1.7383512544802868, "no_speech_prob": 5.32327794644516e-06}, {"id": 55, "seek": 31300, "start": 331.0, "end": 335.0, "text": " It's not that we just barely found the decision boundary and just barely stepped across it.", "tokens": [50364, 583, 309, 11807, 264, 661, 4022, 295, 264, 8858, 857, 12607, 935, 10290, 13, 50614, 50614, 663, 707, 5870, 1319, 307, 1547, 281, 7979, 264, 45216, 304, 3209, 666, 18538, 341, 3256, 295, 257, 46685, 382, 885, 257, 2212, 13, 51064, 51064, 3996, 1880, 551, 307, 300, 309, 1177, 380, 445, 1319, 264, 1508, 13, 51264, 51264, 467, 311, 406, 300, 321, 445, 10268, 1352, 264, 3537, 12866, 293, 445, 10268, 15251, 2108, 309, 13, 51464, 51464, 440, 45216, 304, 3209, 767, 575, 709, 544, 6687, 294, 1080, 18424, 17630, 300, 264, 3256, 322, 264, 558, 307, 257, 2212, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05313772880114042, "compression_ratio": 1.7383512544802868, "no_speech_prob": 5.32327794644516e-06}, {"id": 56, "seek": 31300, "start": 335.0, "end": 342.0, "text": " The convolutional network actually has much more confidence in its incorrect prediction that the image on the right is a given.", "tokens": [50364, 583, 309, 11807, 264, 661, 4022, 295, 264, 8858, 857, 12607, 935, 10290, 13, 50614, 50614, 663, 707, 5870, 1319, 307, 1547, 281, 7979, 264, 45216, 304, 3209, 666, 18538, 341, 3256, 295, 257, 46685, 382, 885, 257, 2212, 13, 51064, 51064, 3996, 1880, 551, 307, 300, 309, 1177, 380, 445, 1319, 264, 1508, 13, 51264, 51264, 467, 311, 406, 300, 321, 445, 10268, 1352, 264, 3537, 12866, 293, 445, 10268, 15251, 2108, 309, 13, 51464, 51464, 440, 45216, 304, 3209, 767, 575, 709, 544, 6687, 294, 1080, 18424, 17630, 300, 264, 3256, 322, 264, 558, 307, 257, 2212, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05313772880114042, "compression_ratio": 1.7383512544802868, "no_speech_prob": 5.32327794644516e-06}, {"id": 57, "seek": 34200, "start": 342.0, "end": 346.0, "text": " It's a given than it had for the original being a panda.", "tokens": [50364, 467, 311, 257, 2212, 813, 309, 632, 337, 264, 3380, 885, 257, 46685, 13, 50564, 50564, 1282, 264, 558, 309, 12307, 300, 264, 3256, 307, 257, 2212, 365, 11803, 13, 24, 4, 8482, 13, 50864, 50864, 407, 949, 309, 1194, 300, 456, 390, 466, 257, 2636, 2931, 300, 309, 390, 746, 661, 813, 257, 46685, 13, 51164, 51164, 400, 586, 309, 311, 466, 382, 1629, 382, 309, 393, 6264, 312, 300, 309, 311, 257, 2212, 13, 51414, 51414, 1018, 257, 707, 857, 295, 2503, 11, 561, 362, 9454, 2098, 295, 15866, 8122, 281, 7979, 819, 3479, 2539, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09584529654493609, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.5977718400536105e-05}, {"id": 58, "seek": 34200, "start": 346.0, "end": 352.0, "text": " On the right it believes that the image is a given with 99.9% probability.", "tokens": [50364, 467, 311, 257, 2212, 813, 309, 632, 337, 264, 3380, 885, 257, 46685, 13, 50564, 50564, 1282, 264, 558, 309, 12307, 300, 264, 3256, 307, 257, 2212, 365, 11803, 13, 24, 4, 8482, 13, 50864, 50864, 407, 949, 309, 1194, 300, 456, 390, 466, 257, 2636, 2931, 300, 309, 390, 746, 661, 813, 257, 46685, 13, 51164, 51164, 400, 586, 309, 311, 466, 382, 1629, 382, 309, 393, 6264, 312, 300, 309, 311, 257, 2212, 13, 51414, 51414, 1018, 257, 707, 857, 295, 2503, 11, 561, 362, 9454, 2098, 295, 15866, 8122, 281, 7979, 819, 3479, 2539, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09584529654493609, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.5977718400536105e-05}, {"id": 59, "seek": 34200, "start": 352.0, "end": 358.0, "text": " So before it thought that there was about a third chance that it was something other than a panda.", "tokens": [50364, 467, 311, 257, 2212, 813, 309, 632, 337, 264, 3380, 885, 257, 46685, 13, 50564, 50564, 1282, 264, 558, 309, 12307, 300, 264, 3256, 307, 257, 2212, 365, 11803, 13, 24, 4, 8482, 13, 50864, 50864, 407, 949, 309, 1194, 300, 456, 390, 466, 257, 2636, 2931, 300, 309, 390, 746, 661, 813, 257, 46685, 13, 51164, 51164, 400, 586, 309, 311, 466, 382, 1629, 382, 309, 393, 6264, 312, 300, 309, 311, 257, 2212, 13, 51414, 51414, 1018, 257, 707, 857, 295, 2503, 11, 561, 362, 9454, 2098, 295, 15866, 8122, 281, 7979, 819, 3479, 2539, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09584529654493609, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.5977718400536105e-05}, {"id": 60, "seek": 34200, "start": 358.0, "end": 363.0, "text": " And now it's about as certain as it can possibly be that it's a given.", "tokens": [50364, 467, 311, 257, 2212, 813, 309, 632, 337, 264, 3380, 885, 257, 46685, 13, 50564, 50564, 1282, 264, 558, 309, 12307, 300, 264, 3256, 307, 257, 2212, 365, 11803, 13, 24, 4, 8482, 13, 50864, 50864, 407, 949, 309, 1194, 300, 456, 390, 466, 257, 2636, 2931, 300, 309, 390, 746, 661, 813, 257, 46685, 13, 51164, 51164, 400, 586, 309, 311, 466, 382, 1629, 382, 309, 393, 6264, 312, 300, 309, 311, 257, 2212, 13, 51414, 51414, 1018, 257, 707, 857, 295, 2503, 11, 561, 362, 9454, 2098, 295, 15866, 8122, 281, 7979, 819, 3479, 2539, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09584529654493609, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.5977718400536105e-05}, {"id": 61, "seek": 34200, "start": 363.0, "end": 371.0, "text": " As a little bit of history, people have studied ways of computing attacks to fool different machine learning models.", "tokens": [50364, 467, 311, 257, 2212, 813, 309, 632, 337, 264, 3380, 885, 257, 46685, 13, 50564, 50564, 1282, 264, 558, 309, 12307, 300, 264, 3256, 307, 257, 2212, 365, 11803, 13, 24, 4, 8482, 13, 50864, 50864, 407, 949, 309, 1194, 300, 456, 390, 466, 257, 2636, 2931, 300, 309, 390, 746, 661, 813, 257, 46685, 13, 51164, 51164, 400, 586, 309, 311, 466, 382, 1629, 382, 309, 393, 6264, 312, 300, 309, 311, 257, 2212, 13, 51414, 51414, 1018, 257, 707, 857, 295, 2503, 11, 561, 362, 9454, 2098, 295, 15866, 8122, 281, 7979, 819, 3479, 2539, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09584529654493609, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.5977718400536105e-05}, {"id": 62, "seek": 37100, "start": 371.0, "end": 374.0, "text": " And at least about 2004 and maybe earlier.", "tokens": [50364, 400, 412, 1935, 466, 15817, 293, 1310, 3071, 13, 50514, 50514, 1171, 257, 938, 565, 341, 390, 1096, 294, 264, 4319, 295, 7979, 278, 24028, 46866, 13, 50764, 50764, 682, 466, 9012, 11, 457, 1143, 257, 955, 1004, 1352, 300, 291, 727, 7979, 18161, 9590, 294, 341, 636, 13, 51064, 51064, 400, 926, 264, 912, 565, 452, 13532, 5778, 1176, 1146, 507, 1352, 300, 291, 727, 652, 341, 733, 295, 2690, 1970, 2452, 18161, 9590, 13, 51414, 51414, 1449, 538, 1228, 364, 19618, 9284, 281, 3164, 322, 264, 4846, 295, 264, 3256, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1914651811737375, "compression_ratio": 1.623015873015873, "no_speech_prob": 6.844025483587757e-05}, {"id": 63, "seek": 37100, "start": 374.0, "end": 379.0, "text": " For a long time this was done in the context of fooling spam detectors.", "tokens": [50364, 400, 412, 1935, 466, 15817, 293, 1310, 3071, 13, 50514, 50514, 1171, 257, 938, 565, 341, 390, 1096, 294, 264, 4319, 295, 7979, 278, 24028, 46866, 13, 50764, 50764, 682, 466, 9012, 11, 457, 1143, 257, 955, 1004, 1352, 300, 291, 727, 7979, 18161, 9590, 294, 341, 636, 13, 51064, 51064, 400, 926, 264, 912, 565, 452, 13532, 5778, 1176, 1146, 507, 1352, 300, 291, 727, 652, 341, 733, 295, 2690, 1970, 2452, 18161, 9590, 13, 51414, 51414, 1449, 538, 1228, 364, 19618, 9284, 281, 3164, 322, 264, 4846, 295, 264, 3256, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1914651811737375, "compression_ratio": 1.623015873015873, "no_speech_prob": 6.844025483587757e-05}, {"id": 64, "seek": 37100, "start": 379.0, "end": 385.0, "text": " In about 2013, but used a bigio found that you could fool neural networks in this way.", "tokens": [50364, 400, 412, 1935, 466, 15817, 293, 1310, 3071, 13, 50514, 50514, 1171, 257, 938, 565, 341, 390, 1096, 294, 264, 4319, 295, 7979, 278, 24028, 46866, 13, 50764, 50764, 682, 466, 9012, 11, 457, 1143, 257, 955, 1004, 1352, 300, 291, 727, 7979, 18161, 9590, 294, 341, 636, 13, 51064, 51064, 400, 926, 264, 912, 565, 452, 13532, 5778, 1176, 1146, 507, 1352, 300, 291, 727, 652, 341, 733, 295, 2690, 1970, 2452, 18161, 9590, 13, 51414, 51414, 1449, 538, 1228, 364, 19618, 9284, 281, 3164, 322, 264, 4846, 295, 264, 3256, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1914651811737375, "compression_ratio": 1.623015873015873, "no_speech_prob": 6.844025483587757e-05}, {"id": 65, "seek": 37100, "start": 385.0, "end": 392.0, "text": " And around the same time my colleague Christian Zegity found that you could make this kind of attack against deep neural networks.", "tokens": [50364, 400, 412, 1935, 466, 15817, 293, 1310, 3071, 13, 50514, 50514, 1171, 257, 938, 565, 341, 390, 1096, 294, 264, 4319, 295, 7979, 278, 24028, 46866, 13, 50764, 50764, 682, 466, 9012, 11, 457, 1143, 257, 955, 1004, 1352, 300, 291, 727, 7979, 18161, 9590, 294, 341, 636, 13, 51064, 51064, 400, 926, 264, 912, 565, 452, 13532, 5778, 1176, 1146, 507, 1352, 300, 291, 727, 652, 341, 733, 295, 2690, 1970, 2452, 18161, 9590, 13, 51414, 51414, 1449, 538, 1228, 364, 19618, 9284, 281, 3164, 322, 264, 4846, 295, 264, 3256, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1914651811737375, "compression_ratio": 1.623015873015873, "no_speech_prob": 6.844025483587757e-05}, {"id": 66, "seek": 37100, "start": 392.0, "end": 397.0, "text": " Just by using an optimization algorithm to search on the input of the image.", "tokens": [50364, 400, 412, 1935, 466, 15817, 293, 1310, 3071, 13, 50514, 50514, 1171, 257, 938, 565, 341, 390, 1096, 294, 264, 4319, 295, 7979, 278, 24028, 46866, 13, 50764, 50764, 682, 466, 9012, 11, 457, 1143, 257, 955, 1004, 1352, 300, 291, 727, 7979, 18161, 9590, 294, 341, 636, 13, 51064, 51064, 400, 926, 264, 912, 565, 452, 13532, 5778, 1176, 1146, 507, 1352, 300, 291, 727, 652, 341, 733, 295, 2690, 1970, 2452, 18161, 9590, 13, 51414, 51414, 1449, 538, 1228, 364, 19618, 9284, 281, 3164, 322, 264, 4846, 295, 264, 3256, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1914651811737375, "compression_ratio": 1.623015873015873, "no_speech_prob": 6.844025483587757e-05}, {"id": 67, "seek": 39700, "start": 397.0, "end": 401.0, "text": " A lot of what I'll be telling you about today is my own follow up work on this topic.", "tokens": [50364, 316, 688, 295, 437, 286, 603, 312, 3585, 291, 466, 965, 307, 452, 1065, 1524, 493, 589, 322, 341, 4829, 13, 50564, 50564, 583, 286, 600, 4418, 257, 688, 295, 452, 3988, 670, 264, 1791, 1326, 924, 3701, 983, 613, 8122, 366, 1944, 293, 983, 309, 311, 370, 1858, 281, 7979, 613, 45216, 304, 9590, 13, 51164, 51164, 1133, 452, 13532, 5778, 700, 6941, 341, 14029, 21761, 490, 363, 1024, 413, 455, 328, 1004, 11, 457, 926, 264, 912, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16344837861902572, "compression_ratio": 1.5269709543568464, "no_speech_prob": 4.24813297286164e-05}, {"id": 68, "seek": 39700, "start": 401.0, "end": 413.0, "text": " But I've spent a lot of my career over the past few years understanding why these attacks are possible and why it's so easy to fool these convolutional networks.", "tokens": [50364, 316, 688, 295, 437, 286, 603, 312, 3585, 291, 466, 965, 307, 452, 1065, 1524, 493, 589, 322, 341, 4829, 13, 50564, 50564, 583, 286, 600, 4418, 257, 688, 295, 452, 3988, 670, 264, 1791, 1326, 924, 3701, 983, 613, 8122, 366, 1944, 293, 983, 309, 311, 370, 1858, 281, 7979, 613, 45216, 304, 9590, 13, 51164, 51164, 1133, 452, 13532, 5778, 700, 6941, 341, 14029, 21761, 490, 363, 1024, 413, 455, 328, 1004, 11, 457, 926, 264, 912, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16344837861902572, "compression_ratio": 1.5269709543568464, "no_speech_prob": 4.24813297286164e-05}, {"id": 69, "seek": 39700, "start": 413.0, "end": 422.0, "text": " When my colleague Christian first discovered this phenomenon independently from Bates Dabigio, but around the same time.", "tokens": [50364, 316, 688, 295, 437, 286, 603, 312, 3585, 291, 466, 965, 307, 452, 1065, 1524, 493, 589, 322, 341, 4829, 13, 50564, 50564, 583, 286, 600, 4418, 257, 688, 295, 452, 3988, 670, 264, 1791, 1326, 924, 3701, 983, 613, 8122, 366, 1944, 293, 983, 309, 311, 370, 1858, 281, 7979, 613, 45216, 304, 9590, 13, 51164, 51164, 1133, 452, 13532, 5778, 700, 6941, 341, 14029, 21761, 490, 363, 1024, 413, 455, 328, 1004, 11, 457, 926, 264, 912, 565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16344837861902572, "compression_ratio": 1.5269709543568464, "no_speech_prob": 4.24813297286164e-05}, {"id": 70, "seek": 42200, "start": 422.0, "end": 429.0, "text": " He found that it was actually a result of a visualization he was trying to make.", "tokens": [50364, 634, 1352, 300, 309, 390, 767, 257, 1874, 295, 257, 25801, 415, 390, 1382, 281, 652, 13, 50714, 50714, 634, 2067, 380, 7601, 3825, 11, 415, 2067, 380, 7601, 577, 281, 7979, 257, 18161, 3209, 13, 50964, 50964, 7156, 415, 632, 257, 45216, 304, 3209, 300, 727, 5521, 6565, 588, 731, 293, 415, 1415, 281, 1223, 577, 309, 2732, 13, 51264, 51264, 407, 415, 1194, 300, 1310, 415, 727, 747, 364, 3256, 295, 257, 4145, 411, 337, 1365, 257, 3036, 295, 257, 5374, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06629788875579834, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.741782009252347e-05}, {"id": 71, "seek": 42200, "start": 429.0, "end": 434.0, "text": " He wasn't studying security, he wasn't studying how to fool a neural network.", "tokens": [50364, 634, 1352, 300, 309, 390, 767, 257, 1874, 295, 257, 25801, 415, 390, 1382, 281, 652, 13, 50714, 50714, 634, 2067, 380, 7601, 3825, 11, 415, 2067, 380, 7601, 577, 281, 7979, 257, 18161, 3209, 13, 50964, 50964, 7156, 415, 632, 257, 45216, 304, 3209, 300, 727, 5521, 6565, 588, 731, 293, 415, 1415, 281, 1223, 577, 309, 2732, 13, 51264, 51264, 407, 415, 1194, 300, 1310, 415, 727, 747, 364, 3256, 295, 257, 4145, 411, 337, 1365, 257, 3036, 295, 257, 5374, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06629788875579834, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.741782009252347e-05}, {"id": 72, "seek": 42200, "start": 434.0, "end": 440.0, "text": " Instead he had a convolutional network that could recognize objects very well and he wanted to understand how it worked.", "tokens": [50364, 634, 1352, 300, 309, 390, 767, 257, 1874, 295, 257, 25801, 415, 390, 1382, 281, 652, 13, 50714, 50714, 634, 2067, 380, 7601, 3825, 11, 415, 2067, 380, 7601, 577, 281, 7979, 257, 18161, 3209, 13, 50964, 50964, 7156, 415, 632, 257, 45216, 304, 3209, 300, 727, 5521, 6565, 588, 731, 293, 415, 1415, 281, 1223, 577, 309, 2732, 13, 51264, 51264, 407, 415, 1194, 300, 1310, 415, 727, 747, 364, 3256, 295, 257, 4145, 411, 337, 1365, 257, 3036, 295, 257, 5374, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06629788875579834, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.741782009252347e-05}, {"id": 73, "seek": 42200, "start": 440.0, "end": 447.0, "text": " So he thought that maybe he could take an image of a scene like for example a picture of a ship.", "tokens": [50364, 634, 1352, 300, 309, 390, 767, 257, 1874, 295, 257, 25801, 415, 390, 1382, 281, 652, 13, 50714, 50714, 634, 2067, 380, 7601, 3825, 11, 415, 2067, 380, 7601, 577, 281, 7979, 257, 18161, 3209, 13, 50964, 50964, 7156, 415, 632, 257, 45216, 304, 3209, 300, 727, 5521, 6565, 588, 731, 293, 415, 1415, 281, 1223, 577, 309, 2732, 13, 51264, 51264, 407, 415, 1194, 300, 1310, 415, 727, 747, 364, 3256, 295, 257, 4145, 411, 337, 1365, 257, 3036, 295, 257, 5374, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06629788875579834, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.741782009252347e-05}, {"id": 74, "seek": 44700, "start": 447.0, "end": 454.0, "text": " And he could gradually transform that image into something that the network would recognize as being an airplane.", "tokens": [50364, 400, 415, 727, 13145, 4088, 300, 3256, 666, 746, 300, 264, 3209, 576, 5521, 382, 885, 364, 17130, 13, 50714, 50714, 400, 670, 264, 1164, 295, 300, 9887, 415, 727, 536, 577, 264, 4122, 295, 264, 4846, 1319, 13, 50964, 50964, 509, 1062, 2066, 300, 1310, 264, 3678, 576, 1261, 3344, 281, 574, 411, 264, 5443, 2261, 364, 17130, 13, 51264, 51264, 1610, 291, 1062, 2066, 300, 1310, 264, 5374, 576, 1852, 11405, 281, 574, 544, 411, 364, 17130, 13, 51464, 51464, 400, 550, 291, 727, 16886, 490, 300, 300, 264, 45216, 304, 3209, 4960, 264, 3344, 5443, 420, 4960, 264, 11405, 281, 5521, 32947, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05965871810913086, "compression_ratio": 1.9848484848484849, "no_speech_prob": 1.2563037671498023e-05}, {"id": 75, "seek": 44700, "start": 454.0, "end": 459.0, "text": " And over the course of that transformation he could see how the features of the input change.", "tokens": [50364, 400, 415, 727, 13145, 4088, 300, 3256, 666, 746, 300, 264, 3209, 576, 5521, 382, 885, 364, 17130, 13, 50714, 50714, 400, 670, 264, 1164, 295, 300, 9887, 415, 727, 536, 577, 264, 4122, 295, 264, 4846, 1319, 13, 50964, 50964, 509, 1062, 2066, 300, 1310, 264, 3678, 576, 1261, 3344, 281, 574, 411, 264, 5443, 2261, 364, 17130, 13, 51264, 51264, 1610, 291, 1062, 2066, 300, 1310, 264, 5374, 576, 1852, 11405, 281, 574, 544, 411, 364, 17130, 13, 51464, 51464, 400, 550, 291, 727, 16886, 490, 300, 300, 264, 45216, 304, 3209, 4960, 264, 3344, 5443, 420, 4960, 264, 11405, 281, 5521, 32947, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05965871810913086, "compression_ratio": 1.9848484848484849, "no_speech_prob": 1.2563037671498023e-05}, {"id": 76, "seek": 44700, "start": 459.0, "end": 465.0, "text": " You might expect that maybe the background would turn blue to look like the sky behind an airplane.", "tokens": [50364, 400, 415, 727, 13145, 4088, 300, 3256, 666, 746, 300, 264, 3209, 576, 5521, 382, 885, 364, 17130, 13, 50714, 50714, 400, 670, 264, 1164, 295, 300, 9887, 415, 727, 536, 577, 264, 4122, 295, 264, 4846, 1319, 13, 50964, 50964, 509, 1062, 2066, 300, 1310, 264, 3678, 576, 1261, 3344, 281, 574, 411, 264, 5443, 2261, 364, 17130, 13, 51264, 51264, 1610, 291, 1062, 2066, 300, 1310, 264, 5374, 576, 1852, 11405, 281, 574, 544, 411, 364, 17130, 13, 51464, 51464, 400, 550, 291, 727, 16886, 490, 300, 300, 264, 45216, 304, 3209, 4960, 264, 3344, 5443, 420, 4960, 264, 11405, 281, 5521, 32947, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05965871810913086, "compression_ratio": 1.9848484848484849, "no_speech_prob": 1.2563037671498023e-05}, {"id": 77, "seek": 44700, "start": 465.0, "end": 469.0, "text": " Or you might expect that maybe the ship would grow wings to look more like an airplane.", "tokens": [50364, 400, 415, 727, 13145, 4088, 300, 3256, 666, 746, 300, 264, 3209, 576, 5521, 382, 885, 364, 17130, 13, 50714, 50714, 400, 670, 264, 1164, 295, 300, 9887, 415, 727, 536, 577, 264, 4122, 295, 264, 4846, 1319, 13, 50964, 50964, 509, 1062, 2066, 300, 1310, 264, 3678, 576, 1261, 3344, 281, 574, 411, 264, 5443, 2261, 364, 17130, 13, 51264, 51264, 1610, 291, 1062, 2066, 300, 1310, 264, 5374, 576, 1852, 11405, 281, 574, 544, 411, 364, 17130, 13, 51464, 51464, 400, 550, 291, 727, 16886, 490, 300, 300, 264, 45216, 304, 3209, 4960, 264, 3344, 5443, 420, 4960, 264, 11405, 281, 5521, 32947, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05965871810913086, "compression_ratio": 1.9848484848484849, "no_speech_prob": 1.2563037671498023e-05}, {"id": 78, "seek": 44700, "start": 469.0, "end": 476.0, "text": " And then you could conclude from that that the convolutional network uses the blue sky or uses the wings to recognize airplanes.", "tokens": [50364, 400, 415, 727, 13145, 4088, 300, 3256, 666, 746, 300, 264, 3209, 576, 5521, 382, 885, 364, 17130, 13, 50714, 50714, 400, 670, 264, 1164, 295, 300, 9887, 415, 727, 536, 577, 264, 4122, 295, 264, 4846, 1319, 13, 50964, 50964, 509, 1062, 2066, 300, 1310, 264, 3678, 576, 1261, 3344, 281, 574, 411, 264, 5443, 2261, 364, 17130, 13, 51264, 51264, 1610, 291, 1062, 2066, 300, 1310, 264, 5374, 576, 1852, 11405, 281, 574, 544, 411, 364, 17130, 13, 51464, 51464, 400, 550, 291, 727, 16886, 490, 300, 300, 264, 45216, 304, 3209, 4960, 264, 3344, 5443, 420, 4960, 264, 11405, 281, 5521, 32947, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05965871810913086, "compression_ratio": 1.9848484848484849, "no_speech_prob": 1.2563037671498023e-05}, {"id": 79, "seek": 47600, "start": 476.0, "end": 479.0, "text": " That's actually not really what happened at all.", "tokens": [50364, 663, 311, 767, 406, 534, 437, 2011, 412, 439, 13, 50514, 50514, 407, 1184, 295, 613, 13419, 510, 3110, 364, 9603, 300, 291, 1401, 1411, 281, 558, 11, 1192, 281, 2767, 13, 50764, 50764, 6947, 4831, 307, 1071, 1823, 295, 16235, 257, 1489, 322, 264, 3565, 8482, 300, 264, 4846, 307, 364, 17130, 4650, 281, 257, 45216, 304, 3209, 2316, 13, 51314, 51314, 400, 550, 264, 3480, 264, 16235, 322, 264, 4846, 281, 264, 3256, 11, 291, 434, 1391, 1143, 281, 3480, 264, 16235, 322, 264, 9834, 295, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1245188462106805, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0002029723400482908}, {"id": 80, "seek": 47600, "start": 479.0, "end": 484.0, "text": " So each of these panels here shows an animation that you read left to right, top to bottom.", "tokens": [50364, 663, 311, 767, 406, 534, 437, 2011, 412, 439, 13, 50514, 50514, 407, 1184, 295, 613, 13419, 510, 3110, 364, 9603, 300, 291, 1401, 1411, 281, 558, 11, 1192, 281, 2767, 13, 50764, 50764, 6947, 4831, 307, 1071, 1823, 295, 16235, 257, 1489, 322, 264, 3565, 8482, 300, 264, 4846, 307, 364, 17130, 4650, 281, 257, 45216, 304, 3209, 2316, 13, 51314, 51314, 400, 550, 264, 3480, 264, 16235, 322, 264, 4846, 281, 264, 3256, 11, 291, 434, 1391, 1143, 281, 3480, 264, 16235, 322, 264, 9834, 295, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1245188462106805, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0002029723400482908}, {"id": 81, "seek": 47600, "start": 484.0, "end": 495.0, "text": " Each panel is another step of gradient a cent on the log probability that the input is an airplane according to a convolutional network model.", "tokens": [50364, 663, 311, 767, 406, 534, 437, 2011, 412, 439, 13, 50514, 50514, 407, 1184, 295, 613, 13419, 510, 3110, 364, 9603, 300, 291, 1401, 1411, 281, 558, 11, 1192, 281, 2767, 13, 50764, 50764, 6947, 4831, 307, 1071, 1823, 295, 16235, 257, 1489, 322, 264, 3565, 8482, 300, 264, 4846, 307, 364, 17130, 4650, 281, 257, 45216, 304, 3209, 2316, 13, 51314, 51314, 400, 550, 264, 3480, 264, 16235, 322, 264, 4846, 281, 264, 3256, 11, 291, 434, 1391, 1143, 281, 3480, 264, 16235, 322, 264, 9834, 295, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1245188462106805, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0002029723400482908}, {"id": 82, "seek": 47600, "start": 495.0, "end": 502.0, "text": " And then the following the gradient on the input to the image, you're probably used to following the gradient on the parameters of a model.", "tokens": [50364, 663, 311, 767, 406, 534, 437, 2011, 412, 439, 13, 50514, 50514, 407, 1184, 295, 613, 13419, 510, 3110, 364, 9603, 300, 291, 1401, 1411, 281, 558, 11, 1192, 281, 2767, 13, 50764, 50764, 6947, 4831, 307, 1071, 1823, 295, 16235, 257, 1489, 322, 264, 3565, 8482, 300, 264, 4846, 307, 364, 17130, 4650, 281, 257, 45216, 304, 3209, 2316, 13, 51314, 51314, 400, 550, 264, 3480, 264, 16235, 322, 264, 4846, 281, 264, 3256, 11, 291, 434, 1391, 1143, 281, 3480, 264, 16235, 322, 264, 9834, 295, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1245188462106805, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0002029723400482908}, {"id": 83, "seek": 50200, "start": 502.0, "end": 512.0, "text": " You can use the back propagation algorithm to compute the gradient on the input image using exactly the same procedure that you would use to compute the gradient on the parameters.", "tokens": [50364, 509, 393, 764, 264, 646, 38377, 9284, 281, 14722, 264, 16235, 322, 264, 4846, 3256, 1228, 2293, 264, 912, 10747, 300, 291, 576, 764, 281, 14722, 264, 16235, 322, 264, 9834, 13, 50864, 50864, 682, 341, 9603, 295, 264, 5374, 294, 264, 6597, 1411, 11, 321, 536, 1732, 13419, 300, 439, 574, 1936, 264, 912, 13, 51164, 51164, 16710, 1196, 23475, 1177, 380, 1643, 281, 362, 4259, 264, 3256, 412, 439, 13, 51314, 51314, 583, 538, 264, 1036, 4831, 264, 3209, 307, 2584, 6679, 300, 341, 307, 364, 17130, 13, 51564, 51564, 1133, 291, 700, 3089, 493, 341, 733, 295, 5120, 11, 2318, 498, 291, 500, 380, 458, 437, 311, 516, 281, 1051, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918668746948242, "compression_ratio": 1.7993197278911566, "no_speech_prob": 1.7151038264273666e-05}, {"id": 84, "seek": 50200, "start": 512.0, "end": 518.0, "text": " In this animation of the ship in the upper left, we see five panels that all look basically the same.", "tokens": [50364, 509, 393, 764, 264, 646, 38377, 9284, 281, 14722, 264, 16235, 322, 264, 4846, 3256, 1228, 2293, 264, 912, 10747, 300, 291, 576, 764, 281, 14722, 264, 16235, 322, 264, 9834, 13, 50864, 50864, 682, 341, 9603, 295, 264, 5374, 294, 264, 6597, 1411, 11, 321, 536, 1732, 13419, 300, 439, 574, 1936, 264, 912, 13, 51164, 51164, 16710, 1196, 23475, 1177, 380, 1643, 281, 362, 4259, 264, 3256, 412, 439, 13, 51314, 51314, 583, 538, 264, 1036, 4831, 264, 3209, 307, 2584, 6679, 300, 341, 307, 364, 17130, 13, 51564, 51564, 1133, 291, 700, 3089, 493, 341, 733, 295, 5120, 11, 2318, 498, 291, 500, 380, 458, 437, 311, 516, 281, 1051, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918668746948242, "compression_ratio": 1.7993197278911566, "no_speech_prob": 1.7151038264273666e-05}, {"id": 85, "seek": 50200, "start": 518.0, "end": 521.0, "text": " Gradient descent doesn't seem to have moved the image at all.", "tokens": [50364, 509, 393, 764, 264, 646, 38377, 9284, 281, 14722, 264, 16235, 322, 264, 4846, 3256, 1228, 2293, 264, 912, 10747, 300, 291, 576, 764, 281, 14722, 264, 16235, 322, 264, 9834, 13, 50864, 50864, 682, 341, 9603, 295, 264, 5374, 294, 264, 6597, 1411, 11, 321, 536, 1732, 13419, 300, 439, 574, 1936, 264, 912, 13, 51164, 51164, 16710, 1196, 23475, 1177, 380, 1643, 281, 362, 4259, 264, 3256, 412, 439, 13, 51314, 51314, 583, 538, 264, 1036, 4831, 264, 3209, 307, 2584, 6679, 300, 341, 307, 364, 17130, 13, 51564, 51564, 1133, 291, 700, 3089, 493, 341, 733, 295, 5120, 11, 2318, 498, 291, 500, 380, 458, 437, 311, 516, 281, 1051, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918668746948242, "compression_ratio": 1.7993197278911566, "no_speech_prob": 1.7151038264273666e-05}, {"id": 86, "seek": 50200, "start": 521.0, "end": 526.0, "text": " But by the last panel the network is completely confident that this is an airplane.", "tokens": [50364, 509, 393, 764, 264, 646, 38377, 9284, 281, 14722, 264, 16235, 322, 264, 4846, 3256, 1228, 2293, 264, 912, 10747, 300, 291, 576, 764, 281, 14722, 264, 16235, 322, 264, 9834, 13, 50864, 50864, 682, 341, 9603, 295, 264, 5374, 294, 264, 6597, 1411, 11, 321, 536, 1732, 13419, 300, 439, 574, 1936, 264, 912, 13, 51164, 51164, 16710, 1196, 23475, 1177, 380, 1643, 281, 362, 4259, 264, 3256, 412, 439, 13, 51314, 51314, 583, 538, 264, 1036, 4831, 264, 3209, 307, 2584, 6679, 300, 341, 307, 364, 17130, 13, 51564, 51564, 1133, 291, 700, 3089, 493, 341, 733, 295, 5120, 11, 2318, 498, 291, 500, 380, 458, 437, 311, 516, 281, 1051, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918668746948242, "compression_ratio": 1.7993197278911566, "no_speech_prob": 1.7151038264273666e-05}, {"id": 87, "seek": 50200, "start": 526.0, "end": 530.0, "text": " When you first code up this kind of experiment, especially if you don't know what's going to happen,", "tokens": [50364, 509, 393, 764, 264, 646, 38377, 9284, 281, 14722, 264, 16235, 322, 264, 4846, 3256, 1228, 2293, 264, 912, 10747, 300, 291, 576, 764, 281, 14722, 264, 16235, 322, 264, 9834, 13, 50864, 50864, 682, 341, 9603, 295, 264, 5374, 294, 264, 6597, 1411, 11, 321, 536, 1732, 13419, 300, 439, 574, 1936, 264, 912, 13, 51164, 51164, 16710, 1196, 23475, 1177, 380, 1643, 281, 362, 4259, 264, 3256, 412, 439, 13, 51314, 51314, 583, 538, 264, 1036, 4831, 264, 3209, 307, 2584, 6679, 300, 341, 307, 364, 17130, 13, 51564, 51564, 1133, 291, 700, 3089, 493, 341, 733, 295, 5120, 11, 2318, 498, 291, 500, 380, 458, 437, 311, 516, 281, 1051, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918668746948242, "compression_ratio": 1.7993197278911566, "no_speech_prob": 1.7151038264273666e-05}, {"id": 88, "seek": 53000, "start": 530.0, "end": 535.0, "text": " it feels a little bit like you have a bug in your script and you're just displaying the same image over and over again.", "tokens": [50364, 309, 3417, 257, 707, 857, 411, 291, 362, 257, 7426, 294, 428, 5755, 293, 291, 434, 445, 36834, 264, 912, 3256, 670, 293, 670, 797, 13, 50614, 50614, 440, 700, 565, 286, 994, 380, 11, 286, 2809, 380, 1697, 309, 390, 2737, 293, 286, 632, 281, 1269, 493, 264, 5267, 293, 1031, 8200, 293, 747, 264, 2649, 295, 552, 293, 652, 988, 300, 456, 390, 767, 257, 2107, 4018, 2649, 456, 13, 51114, 51114, 583, 456, 307, 13, 51264, 51264, 286, 4712, 2940, 819, 22868, 510, 295, 257, 5374, 11, 257, 1032, 11, 257, 3857, 293, 257, 5898, 13, 51514, 51514, 440, 787, 472, 689, 286, 767, 536, 604, 1319, 412, 439, 307, 264, 3256, 295, 264, 3857, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09547018810985534, "compression_ratio": 1.7697841726618706, "no_speech_prob": 1.438731123926118e-05}, {"id": 89, "seek": 53000, "start": 535.0, "end": 545.0, "text": " The first time I didn't, I couldn't believe it was happening and I had to open up the images and numpy and take the difference of them and make sure that there was actually a non zero difference there.", "tokens": [50364, 309, 3417, 257, 707, 857, 411, 291, 362, 257, 7426, 294, 428, 5755, 293, 291, 434, 445, 36834, 264, 912, 3256, 670, 293, 670, 797, 13, 50614, 50614, 440, 700, 565, 286, 994, 380, 11, 286, 2809, 380, 1697, 309, 390, 2737, 293, 286, 632, 281, 1269, 493, 264, 5267, 293, 1031, 8200, 293, 747, 264, 2649, 295, 552, 293, 652, 988, 300, 456, 390, 767, 257, 2107, 4018, 2649, 456, 13, 51114, 51114, 583, 456, 307, 13, 51264, 51264, 286, 4712, 2940, 819, 22868, 510, 295, 257, 5374, 11, 257, 1032, 11, 257, 3857, 293, 257, 5898, 13, 51514, 51514, 440, 787, 472, 689, 286, 767, 536, 604, 1319, 412, 439, 307, 264, 3256, 295, 264, 3857, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09547018810985534, "compression_ratio": 1.7697841726618706, "no_speech_prob": 1.438731123926118e-05}, {"id": 90, "seek": 53000, "start": 545.0, "end": 548.0, "text": " But there is.", "tokens": [50364, 309, 3417, 257, 707, 857, 411, 291, 362, 257, 7426, 294, 428, 5755, 293, 291, 434, 445, 36834, 264, 912, 3256, 670, 293, 670, 797, 13, 50614, 50614, 440, 700, 565, 286, 994, 380, 11, 286, 2809, 380, 1697, 309, 390, 2737, 293, 286, 632, 281, 1269, 493, 264, 5267, 293, 1031, 8200, 293, 747, 264, 2649, 295, 552, 293, 652, 988, 300, 456, 390, 767, 257, 2107, 4018, 2649, 456, 13, 51114, 51114, 583, 456, 307, 13, 51264, 51264, 286, 4712, 2940, 819, 22868, 510, 295, 257, 5374, 11, 257, 1032, 11, 257, 3857, 293, 257, 5898, 13, 51514, 51514, 440, 787, 472, 689, 286, 767, 536, 604, 1319, 412, 439, 307, 264, 3256, 295, 264, 3857, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09547018810985534, "compression_ratio": 1.7697841726618706, "no_speech_prob": 1.438731123926118e-05}, {"id": 91, "seek": 53000, "start": 548.0, "end": 553.0, "text": " I showed several different animations here of a ship, a car, a cat and a truck.", "tokens": [50364, 309, 3417, 257, 707, 857, 411, 291, 362, 257, 7426, 294, 428, 5755, 293, 291, 434, 445, 36834, 264, 912, 3256, 670, 293, 670, 797, 13, 50614, 50614, 440, 700, 565, 286, 994, 380, 11, 286, 2809, 380, 1697, 309, 390, 2737, 293, 286, 632, 281, 1269, 493, 264, 5267, 293, 1031, 8200, 293, 747, 264, 2649, 295, 552, 293, 652, 988, 300, 456, 390, 767, 257, 2107, 4018, 2649, 456, 13, 51114, 51114, 583, 456, 307, 13, 51264, 51264, 286, 4712, 2940, 819, 22868, 510, 295, 257, 5374, 11, 257, 1032, 11, 257, 3857, 293, 257, 5898, 13, 51514, 51514, 440, 787, 472, 689, 286, 767, 536, 604, 1319, 412, 439, 307, 264, 3256, 295, 264, 3857, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09547018810985534, "compression_ratio": 1.7697841726618706, "no_speech_prob": 1.438731123926118e-05}, {"id": 92, "seek": 53000, "start": 553.0, "end": 558.0, "text": " The only one where I actually see any change at all is the image of the cat.", "tokens": [50364, 309, 3417, 257, 707, 857, 411, 291, 362, 257, 7426, 294, 428, 5755, 293, 291, 434, 445, 36834, 264, 912, 3256, 670, 293, 670, 797, 13, 50614, 50614, 440, 700, 565, 286, 994, 380, 11, 286, 2809, 380, 1697, 309, 390, 2737, 293, 286, 632, 281, 1269, 493, 264, 5267, 293, 1031, 8200, 293, 747, 264, 2649, 295, 552, 293, 652, 988, 300, 456, 390, 767, 257, 2107, 4018, 2649, 456, 13, 51114, 51114, 583, 456, 307, 13, 51264, 51264, 286, 4712, 2940, 819, 22868, 510, 295, 257, 5374, 11, 257, 1032, 11, 257, 3857, 293, 257, 5898, 13, 51514, 51514, 440, 787, 472, 689, 286, 767, 536, 604, 1319, 412, 439, 307, 264, 3256, 295, 264, 3857, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09547018810985534, "compression_ratio": 1.7697841726618706, "no_speech_prob": 1.438731123926118e-05}, {"id": 93, "seek": 55800, "start": 558.0, "end": 566.0, "text": " The color of the cat's face changes a little bit and maybe it becomes a little bit more like the color of a metal airplane.", "tokens": [50364, 440, 2017, 295, 264, 3857, 311, 1851, 2962, 257, 707, 857, 293, 1310, 309, 3643, 257, 707, 857, 544, 411, 264, 2017, 295, 257, 5760, 17130, 13, 50764, 50764, 5358, 813, 300, 11, 286, 500, 380, 536, 604, 2962, 294, 604, 295, 613, 22868, 293, 286, 500, 380, 536, 1340, 588, 3402, 488, 295, 364, 17130, 13, 51164, 51164, 407, 16235, 23475, 11, 2831, 813, 6246, 264, 4846, 666, 364, 1365, 295, 364, 17130, 11, 575, 1352, 364, 3256, 300, 38625, 264, 3209, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07274577518304189, "compression_ratio": 1.882882882882883, "no_speech_prob": 1.33865287352819e-05}, {"id": 94, "seek": 55800, "start": 566.0, "end": 574.0, "text": " Other than that, I don't see any changes in any of these animations and I don't see anything very suggestive of an airplane.", "tokens": [50364, 440, 2017, 295, 264, 3857, 311, 1851, 2962, 257, 707, 857, 293, 1310, 309, 3643, 257, 707, 857, 544, 411, 264, 2017, 295, 257, 5760, 17130, 13, 50764, 50764, 5358, 813, 300, 11, 286, 500, 380, 536, 604, 2962, 294, 604, 295, 613, 22868, 293, 286, 500, 380, 536, 1340, 588, 3402, 488, 295, 364, 17130, 13, 51164, 51164, 407, 16235, 23475, 11, 2831, 813, 6246, 264, 4846, 666, 364, 1365, 295, 364, 17130, 11, 575, 1352, 364, 3256, 300, 38625, 264, 3209, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07274577518304189, "compression_ratio": 1.882882882882883, "no_speech_prob": 1.33865287352819e-05}, {"id": 95, "seek": 55800, "start": 574.0, "end": 586.0, "text": " So gradient descent, rather than turning the input into an example of an airplane, has found an image that fools the network into thinking that the input is an airplane.", "tokens": [50364, 440, 2017, 295, 264, 3857, 311, 1851, 2962, 257, 707, 857, 293, 1310, 309, 3643, 257, 707, 857, 544, 411, 264, 2017, 295, 257, 5760, 17130, 13, 50764, 50764, 5358, 813, 300, 11, 286, 500, 380, 536, 604, 2962, 294, 604, 295, 613, 22868, 293, 286, 500, 380, 536, 1340, 588, 3402, 488, 295, 364, 17130, 13, 51164, 51164, 407, 16235, 23475, 11, 2831, 813, 6246, 264, 4846, 666, 364, 1365, 295, 364, 17130, 11, 575, 1352, 364, 3256, 300, 38625, 264, 3209, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07274577518304189, "compression_ratio": 1.882882882882883, "no_speech_prob": 1.33865287352819e-05}, {"id": 96, "seek": 58600, "start": 586.0, "end": 591.0, "text": " And if we were malicious attackers, we didn't even have to work very hard to figure out how to fool the network.", "tokens": [50364, 400, 498, 321, 645, 33496, 45129, 11, 321, 994, 380, 754, 362, 281, 589, 588, 1152, 281, 2573, 484, 577, 281, 7979, 264, 3209, 13, 50614, 50614, 492, 445, 2351, 264, 3209, 281, 976, 505, 364, 3256, 295, 364, 17130, 293, 309, 2729, 505, 746, 300, 38625, 309, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51114, 51114, 1133, 5778, 700, 6572, 341, 589, 11, 257, 688, 295, 11290, 1361, 484, 365, 12992, 411, 264, 13717, 35583, 5092, 294, 633, 2452, 18161, 3209, 420, 2452, 2539, 575, 2452, 27108, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06032124067607679, "compression_ratio": 1.7295081967213115, "no_speech_prob": 7.768773684802e-06}, {"id": 97, "seek": 58600, "start": 591.0, "end": 601.0, "text": " We just asked the network to give us an image of an airplane and it gave us something that fools it into thinking that the input is an airplane.", "tokens": [50364, 400, 498, 321, 645, 33496, 45129, 11, 321, 994, 380, 754, 362, 281, 589, 588, 1152, 281, 2573, 484, 577, 281, 7979, 264, 3209, 13, 50614, 50614, 492, 445, 2351, 264, 3209, 281, 976, 505, 364, 3256, 295, 364, 17130, 293, 309, 2729, 505, 746, 300, 38625, 309, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51114, 51114, 1133, 5778, 700, 6572, 341, 589, 11, 257, 688, 295, 11290, 1361, 484, 365, 12992, 411, 264, 13717, 35583, 5092, 294, 633, 2452, 18161, 3209, 420, 2452, 2539, 575, 2452, 27108, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06032124067607679, "compression_ratio": 1.7295081967213115, "no_speech_prob": 7.768773684802e-06}, {"id": 98, "seek": 58600, "start": 601.0, "end": 611.0, "text": " When Christian first published this work, a lot of articles came out with titles like the flaw lurking in every deep neural network or deep learning has deep flaws.", "tokens": [50364, 400, 498, 321, 645, 33496, 45129, 11, 321, 994, 380, 754, 362, 281, 589, 588, 1152, 281, 2573, 484, 577, 281, 7979, 264, 3209, 13, 50614, 50614, 492, 445, 2351, 264, 3209, 281, 976, 505, 364, 3256, 295, 364, 17130, 293, 309, 2729, 505, 746, 300, 38625, 309, 666, 1953, 300, 264, 4846, 307, 364, 17130, 13, 51114, 51114, 1133, 5778, 700, 6572, 341, 589, 11, 257, 688, 295, 11290, 1361, 484, 365, 12992, 411, 264, 13717, 35583, 5092, 294, 633, 2452, 18161, 3209, 420, 2452, 2539, 575, 2452, 27108, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06032124067607679, "compression_ratio": 1.7295081967213115, "no_speech_prob": 7.768773684802e-06}, {"id": 99, "seek": 61100, "start": 611.0, "end": 619.0, "text": " It's important to remember that these vulnerabilities apply to essentially every machine learning algorithm that we've studied so far.", "tokens": [50364, 467, 311, 1021, 281, 1604, 300, 613, 37633, 3079, 281, 4476, 633, 3479, 2539, 9284, 300, 321, 600, 9454, 370, 1400, 13, 50764, 50764, 2188, 295, 552, 411, 40302, 37, 9590, 293, 21156, 278, 10305, 8017, 3391, 366, 1075, 281, 4597, 341, 1802, 8344, 11, 457, 754, 588, 2199, 3479, 2539, 14642, 366, 5405, 10955, 281, 17641, 44745, 5110, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10049211978912354, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.0031784515595064e-05}, {"id": 100, "seek": 61100, "start": 619.0, "end": 632.0, "text": " Some of them like RBF networks and parsing density estimators are able to resist this effect somewhat, but even very simple machine learning algorithms are highly vulnerable to adversarial examples.", "tokens": [50364, 467, 311, 1021, 281, 1604, 300, 613, 37633, 3079, 281, 4476, 633, 3479, 2539, 9284, 300, 321, 600, 9454, 370, 1400, 13, 50764, 50764, 2188, 295, 552, 411, 40302, 37, 9590, 293, 21156, 278, 10305, 8017, 3391, 366, 1075, 281, 4597, 341, 1802, 8344, 11, 457, 754, 588, 2199, 3479, 2539, 14642, 366, 5405, 10955, 281, 17641, 44745, 5110, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10049211978912354, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.0031784515595064e-05}, {"id": 101, "seek": 63200, "start": 632.0, "end": 642.0, "text": " In this image, I show an animation of what happens when we attack a linear model. So it's not a deep algorithm and all, it's just a shallow softmax model.", "tokens": [50364, 682, 341, 3256, 11, 286, 855, 364, 9603, 295, 437, 2314, 562, 321, 2690, 257, 8213, 2316, 13, 407, 309, 311, 406, 257, 2452, 9284, 293, 439, 11, 309, 311, 445, 257, 20488, 2787, 41167, 2316, 13, 50864, 50864, 509, 12972, 538, 257, 8141, 11, 291, 909, 257, 8062, 295, 12577, 2115, 11, 291, 3079, 264, 2787, 41167, 2445, 293, 291, 600, 658, 428, 8482, 7316, 295, 264, 1266, 376, 297, 468, 5359, 13, 51364, 51364, 1711, 264, 6597, 1411, 11, 286, 722, 365, 364, 3256, 295, 257, 1722, 11, 293, 550, 382, 321, 1286, 1411, 281, 558, 11, 1192, 281, 2767, 11, 286, 13145, 4088, 309, 281, 312, 257, 1958, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1368900496384193, "compression_ratio": 1.5921985815602837, "no_speech_prob": 6.19895217823796e-05}, {"id": 102, "seek": 63200, "start": 642.0, "end": 652.0, "text": " You multiply by a matrix, you add a vector of bias terms, you apply the softmax function and you've got your probability distribution of the 10 M nist classes.", "tokens": [50364, 682, 341, 3256, 11, 286, 855, 364, 9603, 295, 437, 2314, 562, 321, 2690, 257, 8213, 2316, 13, 407, 309, 311, 406, 257, 2452, 9284, 293, 439, 11, 309, 311, 445, 257, 20488, 2787, 41167, 2316, 13, 50864, 50864, 509, 12972, 538, 257, 8141, 11, 291, 909, 257, 8062, 295, 12577, 2115, 11, 291, 3079, 264, 2787, 41167, 2445, 293, 291, 600, 658, 428, 8482, 7316, 295, 264, 1266, 376, 297, 468, 5359, 13, 51364, 51364, 1711, 264, 6597, 1411, 11, 286, 722, 365, 364, 3256, 295, 257, 1722, 11, 293, 550, 382, 321, 1286, 1411, 281, 558, 11, 1192, 281, 2767, 11, 286, 13145, 4088, 309, 281, 312, 257, 1958, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1368900496384193, "compression_ratio": 1.5921985815602837, "no_speech_prob": 6.19895217823796e-05}, {"id": 103, "seek": 63200, "start": 652.0, "end": 661.0, "text": " At the upper left, I start with an image of a 9, and then as we move left to right, top to bottom, I gradually transform it to be a 0.", "tokens": [50364, 682, 341, 3256, 11, 286, 855, 364, 9603, 295, 437, 2314, 562, 321, 2690, 257, 8213, 2316, 13, 407, 309, 311, 406, 257, 2452, 9284, 293, 439, 11, 309, 311, 445, 257, 20488, 2787, 41167, 2316, 13, 50864, 50864, 509, 12972, 538, 257, 8141, 11, 291, 909, 257, 8062, 295, 12577, 2115, 11, 291, 3079, 264, 2787, 41167, 2445, 293, 291, 600, 658, 428, 8482, 7316, 295, 264, 1266, 376, 297, 468, 5359, 13, 51364, 51364, 1711, 264, 6597, 1411, 11, 286, 722, 365, 364, 3256, 295, 257, 1722, 11, 293, 550, 382, 321, 1286, 1411, 281, 558, 11, 1192, 281, 2767, 11, 286, 13145, 4088, 309, 281, 312, 257, 1958, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1368900496384193, "compression_ratio": 1.5921985815602837, "no_speech_prob": 6.19895217823796e-05}, {"id": 104, "seek": 66100, "start": 661.0, "end": 672.0, "text": " Where I've drawn the yellow box, the model assigns high probability to it being a 0. I forget exactly what my threshold was for high probability, but I think it was around 0.9 or so.", "tokens": [50364, 2305, 286, 600, 10117, 264, 5566, 2424, 11, 264, 2316, 6269, 82, 1090, 8482, 281, 309, 885, 257, 1958, 13, 286, 2870, 2293, 437, 452, 14678, 390, 337, 1090, 8482, 11, 457, 286, 519, 309, 390, 926, 1958, 13, 24, 420, 370, 13, 50914, 50914, 1396, 382, 321, 1286, 281, 264, 1150, 5386, 11, 286, 4088, 309, 666, 257, 502, 11, 293, 264, 1150, 5566, 2424, 16203, 689, 321, 600, 10727, 33372, 264, 2316, 666, 1953, 309, 311, 257, 502, 365, 1090, 8482, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.06393798914822665, "compression_ratio": 1.6712328767123288, "no_speech_prob": 4.66991841676645e-05}, {"id": 105, "seek": 66100, "start": 672.0, "end": 682.0, "text": " Then as we move to the second row, I transform it into a 1, and the second yellow box indicates where we've successfully fooled the model into thinking it's a 1 with high probability.", "tokens": [50364, 2305, 286, 600, 10117, 264, 5566, 2424, 11, 264, 2316, 6269, 82, 1090, 8482, 281, 309, 885, 257, 1958, 13, 286, 2870, 2293, 437, 452, 14678, 390, 337, 1090, 8482, 11, 457, 286, 519, 309, 390, 926, 1958, 13, 24, 420, 370, 13, 50914, 50914, 1396, 382, 321, 1286, 281, 264, 1150, 5386, 11, 286, 4088, 309, 666, 257, 502, 11, 293, 264, 1150, 5566, 2424, 16203, 689, 321, 600, 10727, 33372, 264, 2316, 666, 1953, 309, 311, 257, 502, 365, 1090, 8482, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.06393798914822665, "compression_ratio": 1.6712328767123288, "no_speech_prob": 4.66991841676645e-05}, {"id": 106, "seek": 68200, "start": 682.0, "end": 694.0, "text": " And then as you read the rest of the yellow box is left to right, top to bottom, we go through the 2s, 3s, 4s and so on. Until finally at the lower right, we have a 9 that has a yellow box around it, and it actually looks like a 9.", "tokens": [50364, 400, 550, 382, 291, 1401, 264, 1472, 295, 264, 5566, 2424, 307, 1411, 281, 558, 11, 1192, 281, 2767, 11, 321, 352, 807, 264, 568, 82, 11, 805, 82, 11, 1017, 82, 293, 370, 322, 13, 9088, 2721, 412, 264, 3126, 558, 11, 321, 362, 257, 1722, 300, 575, 257, 5566, 2424, 926, 309, 11, 293, 309, 767, 1542, 411, 257, 1722, 13, 50964, 50964, 583, 294, 341, 1389, 11, 264, 787, 1778, 309, 767, 1542, 411, 257, 1722, 307, 300, 321, 1409, 264, 1379, 1399, 365, 257, 1722, 13, 51264, 51264, 492, 10727, 31791, 807, 439, 1266, 5359, 295, 376, 297, 468, 1553, 30797, 4473, 264, 3256, 295, 264, 14293, 294, 604, 636, 300, 576, 23946, 365, 1952, 11150, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08026187896728515, "compression_ratio": 1.75, "no_speech_prob": 3.054854096262716e-05}, {"id": 107, "seek": 68200, "start": 694.0, "end": 700.0, "text": " But in this case, the only reason it actually looks like a 9 is that we started the whole process with a 9.", "tokens": [50364, 400, 550, 382, 291, 1401, 264, 1472, 295, 264, 5566, 2424, 307, 1411, 281, 558, 11, 1192, 281, 2767, 11, 321, 352, 807, 264, 568, 82, 11, 805, 82, 11, 1017, 82, 293, 370, 322, 13, 9088, 2721, 412, 264, 3126, 558, 11, 321, 362, 257, 1722, 300, 575, 257, 5566, 2424, 926, 309, 11, 293, 309, 767, 1542, 411, 257, 1722, 13, 50964, 50964, 583, 294, 341, 1389, 11, 264, 787, 1778, 309, 767, 1542, 411, 257, 1722, 307, 300, 321, 1409, 264, 1379, 1399, 365, 257, 1722, 13, 51264, 51264, 492, 10727, 31791, 807, 439, 1266, 5359, 295, 376, 297, 468, 1553, 30797, 4473, 264, 3256, 295, 264, 14293, 294, 604, 636, 300, 576, 23946, 365, 1952, 11150, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08026187896728515, "compression_ratio": 1.75, "no_speech_prob": 3.054854096262716e-05}, {"id": 108, "seek": 68200, "start": 700.0, "end": 711.0, "text": " We successfully swept through all 10 classes of M nist without substantially changing the image of the digit in any way that would interfere with human recognition.", "tokens": [50364, 400, 550, 382, 291, 1401, 264, 1472, 295, 264, 5566, 2424, 307, 1411, 281, 558, 11, 1192, 281, 2767, 11, 321, 352, 807, 264, 568, 82, 11, 805, 82, 11, 1017, 82, 293, 370, 322, 13, 9088, 2721, 412, 264, 3126, 558, 11, 321, 362, 257, 1722, 300, 575, 257, 5566, 2424, 926, 309, 11, 293, 309, 767, 1542, 411, 257, 1722, 13, 50964, 50964, 583, 294, 341, 1389, 11, 264, 787, 1778, 309, 767, 1542, 411, 257, 1722, 307, 300, 321, 1409, 264, 1379, 1399, 365, 257, 1722, 13, 51264, 51264, 492, 10727, 31791, 807, 439, 1266, 5359, 295, 376, 297, 468, 1553, 30797, 4473, 264, 3256, 295, 264, 14293, 294, 604, 636, 300, 576, 23946, 365, 1952, 11150, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08026187896728515, "compression_ratio": 1.75, "no_speech_prob": 3.054854096262716e-05}, {"id": 109, "seek": 71100, "start": 711.0, "end": 716.0, "text": " So this linear model was actually extremely easy to fool.", "tokens": [50364, 407, 341, 8213, 2316, 390, 767, 4664, 1858, 281, 7979, 13, 50614, 50614, 13212, 8213, 5245, 11, 321, 600, 611, 1612, 300, 321, 393, 7979, 867, 819, 3685, 295, 8213, 5245, 3009, 3565, 3142, 24590, 293, 31910, 44, 311, 13, 51064, 51064, 492, 600, 611, 1352, 300, 321, 393, 7979, 3537, 5852, 293, 281, 257, 22043, 8396, 23831, 5987, 311, 1508, 23463, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09517280379338051, "compression_ratio": 1.6174863387978142, "no_speech_prob": 2.027836671913974e-05}, {"id": 110, "seek": 71100, "start": 716.0, "end": 725.0, "text": " Besides linear models, we've also seen that we can fool many different kinds of linear models including logistic regression and SVM's.", "tokens": [50364, 407, 341, 8213, 2316, 390, 767, 4664, 1858, 281, 7979, 13, 50614, 50614, 13212, 8213, 5245, 11, 321, 600, 611, 1612, 300, 321, 393, 7979, 867, 819, 3685, 295, 8213, 5245, 3009, 3565, 3142, 24590, 293, 31910, 44, 311, 13, 51064, 51064, 492, 600, 611, 1352, 300, 321, 393, 7979, 3537, 5852, 293, 281, 257, 22043, 8396, 23831, 5987, 311, 1508, 23463, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09517280379338051, "compression_ratio": 1.6174863387978142, "no_speech_prob": 2.027836671913974e-05}, {"id": 111, "seek": 71100, "start": 725.0, "end": 734.0, "text": " We've also found that we can fool decision trees and to a lesser extent nearest neighbor's classifiers.", "tokens": [50364, 407, 341, 8213, 2316, 390, 767, 4664, 1858, 281, 7979, 13, 50614, 50614, 13212, 8213, 5245, 11, 321, 600, 611, 1612, 300, 321, 393, 7979, 867, 819, 3685, 295, 8213, 5245, 3009, 3565, 3142, 24590, 293, 31910, 44, 311, 13, 51064, 51064, 492, 600, 611, 1352, 300, 321, 393, 7979, 3537, 5852, 293, 281, 257, 22043, 8396, 23831, 5987, 311, 1508, 23463, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09517280379338051, "compression_ratio": 1.6174863387978142, "no_speech_prob": 2.027836671913974e-05}, {"id": 112, "seek": 73400, "start": 734.0, "end": 746.0, "text": " We wanted to explain exactly why this happens. Back in about 2014 after we'd published the original paper where we said that these problems exist, we were trying to figure out why they happened.", "tokens": [50364, 492, 1415, 281, 2903, 2293, 983, 341, 2314, 13, 5833, 294, 466, 8227, 934, 321, 1116, 6572, 264, 3380, 3035, 689, 321, 848, 300, 613, 2740, 2514, 11, 321, 645, 1382, 281, 2573, 484, 983, 436, 2011, 13, 50964, 50964, 1133, 321, 4114, 527, 700, 3035, 11, 321, 1194, 300, 1936, 341, 307, 257, 1254, 295, 670, 69, 2414, 300, 291, 362, 257, 588, 6179, 2452, 18161, 3209, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10600654393026274, "compression_ratio": 1.536697247706422, "no_speech_prob": 1.4281639778346289e-05}, {"id": 113, "seek": 73400, "start": 746.0, "end": 755.0, "text": " When we wrote our first paper, we thought that basically this is a form of overfitting that you have a very complicated deep neural network.", "tokens": [50364, 492, 1415, 281, 2903, 2293, 983, 341, 2314, 13, 5833, 294, 466, 8227, 934, 321, 1116, 6572, 264, 3380, 3035, 689, 321, 848, 300, 613, 2740, 2514, 11, 321, 645, 1382, 281, 2573, 484, 983, 436, 2011, 13, 50964, 50964, 1133, 321, 4114, 527, 700, 3035, 11, 321, 1194, 300, 1936, 341, 307, 257, 1254, 295, 670, 69, 2414, 300, 291, 362, 257, 588, 6179, 2452, 18161, 3209, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10600654393026274, "compression_ratio": 1.536697247706422, "no_speech_prob": 1.4281639778346289e-05}, {"id": 114, "seek": 75500, "start": 755.0, "end": 764.0, "text": " It learns to fit the training set. It's behavior on the test set is somewhat undefined, and then it makes random mistakes that an attacker can exploit.", "tokens": [50364, 467, 27152, 281, 3318, 264, 3097, 992, 13, 467, 311, 5223, 322, 264, 1500, 992, 307, 8344, 674, 5666, 2001, 11, 293, 550, 309, 1669, 4974, 8038, 300, 364, 35871, 393, 25924, 13, 50814, 50814, 407, 718, 311, 1792, 807, 437, 300, 1657, 1542, 411, 8344, 39481, 736, 13, 51014, 51014, 286, 362, 510, 257, 3097, 992, 295, 1045, 3344, 35387, 293, 1045, 3092, 277, 311, 11, 293, 321, 528, 281, 652, 257, 1508, 9902, 300, 393, 5521, 1783, 311, 293, 5521, 422, 311, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1319831462388628, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.848699220223352e-05}, {"id": 115, "seek": 75500, "start": 764.0, "end": 768.0, "text": " So let's walk through what that story looks like somewhat concretely.", "tokens": [50364, 467, 27152, 281, 3318, 264, 3097, 992, 13, 467, 311, 5223, 322, 264, 1500, 992, 307, 8344, 674, 5666, 2001, 11, 293, 550, 309, 1669, 4974, 8038, 300, 364, 35871, 393, 25924, 13, 50814, 50814, 407, 718, 311, 1792, 807, 437, 300, 1657, 1542, 411, 8344, 39481, 736, 13, 51014, 51014, 286, 362, 510, 257, 3097, 992, 295, 1045, 3344, 35387, 293, 1045, 3092, 277, 311, 11, 293, 321, 528, 281, 652, 257, 1508, 9902, 300, 393, 5521, 1783, 311, 293, 5521, 422, 311, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1319831462388628, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.848699220223352e-05}, {"id": 116, "seek": 75500, "start": 768.0, "end": 778.0, "text": " I have here a training set of three blue axes and three green o's, and we want to make a classifier that can recognize X's and recognize O's.", "tokens": [50364, 467, 27152, 281, 3318, 264, 3097, 992, 13, 467, 311, 5223, 322, 264, 1500, 992, 307, 8344, 674, 5666, 2001, 11, 293, 550, 309, 1669, 4974, 8038, 300, 364, 35871, 393, 25924, 13, 50814, 50814, 407, 718, 311, 1792, 807, 437, 300, 1657, 1542, 411, 8344, 39481, 736, 13, 51014, 51014, 286, 362, 510, 257, 3097, 992, 295, 1045, 3344, 35387, 293, 1045, 3092, 277, 311, 11, 293, 321, 528, 281, 652, 257, 1508, 9902, 300, 393, 5521, 1783, 311, 293, 5521, 422, 311, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1319831462388628, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.848699220223352e-05}, {"id": 117, "seek": 77800, "start": 778.0, "end": 791.0, "text": " We have a very complicated classifier that can easily fit the training set. So we represent everywhere it believes X's should be with blobs of blue color, and I've drawn a blob of blue around all of the training set X's.", "tokens": [50364, 492, 362, 257, 588, 6179, 1508, 9902, 300, 393, 3612, 3318, 264, 3097, 992, 13, 407, 321, 2906, 5315, 309, 12307, 1783, 311, 820, 312, 365, 1749, 929, 295, 3344, 2017, 11, 293, 286, 600, 10117, 257, 46115, 295, 3344, 926, 439, 295, 264, 3097, 992, 1783, 311, 13, 51014, 51014, 407, 309, 8944, 1508, 11221, 264, 3097, 992, 13, 51164, 51164, 467, 611, 575, 257, 46115, 295, 3092, 2758, 4099, 689, 264, 422, 311, 366, 11, 293, 309, 10727, 9001, 439, 295, 264, 3092, 3097, 992, 422, 311, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0741630107798475, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.68947881826898e-06}, {"id": 118, "seek": 77800, "start": 791.0, "end": 794.0, "text": " So it correctly classifies the training set.", "tokens": [50364, 492, 362, 257, 588, 6179, 1508, 9902, 300, 393, 3612, 3318, 264, 3097, 992, 13, 407, 321, 2906, 5315, 309, 12307, 1783, 311, 820, 312, 365, 1749, 929, 295, 3344, 2017, 11, 293, 286, 600, 10117, 257, 46115, 295, 3344, 926, 439, 295, 264, 3097, 992, 1783, 311, 13, 51014, 51014, 407, 309, 8944, 1508, 11221, 264, 3097, 992, 13, 51164, 51164, 467, 611, 575, 257, 46115, 295, 3092, 2758, 4099, 689, 264, 422, 311, 366, 11, 293, 309, 10727, 9001, 439, 295, 264, 3092, 3097, 992, 422, 311, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0741630107798475, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.68947881826898e-06}, {"id": 119, "seek": 77800, "start": 794.0, "end": 802.0, "text": " It also has a blob of green mass showing where the O's are, and it successfully fits all of the green training set O's.", "tokens": [50364, 492, 362, 257, 588, 6179, 1508, 9902, 300, 393, 3612, 3318, 264, 3097, 992, 13, 407, 321, 2906, 5315, 309, 12307, 1783, 311, 820, 312, 365, 1749, 929, 295, 3344, 2017, 11, 293, 286, 600, 10117, 257, 46115, 295, 3344, 926, 439, 295, 264, 3097, 992, 1783, 311, 13, 51014, 51014, 407, 309, 8944, 1508, 11221, 264, 3097, 992, 13, 51164, 51164, 467, 611, 575, 257, 46115, 295, 3092, 2758, 4099, 689, 264, 422, 311, 366, 11, 293, 309, 10727, 9001, 439, 295, 264, 3092, 3097, 992, 422, 311, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0741630107798475, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.68947881826898e-06}, {"id": 120, "seek": 80200, "start": 802.0, "end": 811.0, "text": " But then because this is a very complicated function and it has just way more parameters than it actually needs to represent the training task.", "tokens": [50364, 583, 550, 570, 341, 307, 257, 588, 6179, 2445, 293, 309, 575, 445, 636, 544, 9834, 813, 309, 767, 2203, 281, 2906, 264, 3097, 5633, 13, 50814, 50814, 467, 19251, 707, 1749, 929, 295, 8482, 2758, 926, 264, 1472, 295, 1901, 16979, 13, 51064, 51064, 1282, 264, 1411, 456, 311, 257, 46115, 295, 3092, 1901, 300, 311, 733, 295, 2651, 264, 3097, 992, 1783, 311, 11, 293, 286, 600, 10117, 257, 2182, 1783, 456, 281, 855, 300, 1310, 341, 576, 312, 364, 17641, 44745, 1365, 689, 321, 2066, 257, 21538, 281, 312, 1783, 457, 264, 2316, 6269, 82, 422, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0920472786976741, "compression_ratio": 1.6227758007117439, "no_speech_prob": 1.2428220543370117e-05}, {"id": 121, "seek": 80200, "start": 811.0, "end": 816.0, "text": " It throws little blobs of probability mass around the rest of space randomly.", "tokens": [50364, 583, 550, 570, 341, 307, 257, 588, 6179, 2445, 293, 309, 575, 445, 636, 544, 9834, 813, 309, 767, 2203, 281, 2906, 264, 3097, 5633, 13, 50814, 50814, 467, 19251, 707, 1749, 929, 295, 8482, 2758, 926, 264, 1472, 295, 1901, 16979, 13, 51064, 51064, 1282, 264, 1411, 456, 311, 257, 46115, 295, 3092, 1901, 300, 311, 733, 295, 2651, 264, 3097, 992, 1783, 311, 11, 293, 286, 600, 10117, 257, 2182, 1783, 456, 281, 855, 300, 1310, 341, 576, 312, 364, 17641, 44745, 1365, 689, 321, 2066, 257, 21538, 281, 312, 1783, 457, 264, 2316, 6269, 82, 422, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0920472786976741, "compression_ratio": 1.6227758007117439, "no_speech_prob": 1.2428220543370117e-05}, {"id": 122, "seek": 80200, "start": 816.0, "end": 829.0, "text": " On the left there's a blob of green space that's kind of near the training set X's, and I've drawn a red X there to show that maybe this would be an adversarial example where we expect a classification to be X but the model assigns O.", "tokens": [50364, 583, 550, 570, 341, 307, 257, 588, 6179, 2445, 293, 309, 575, 445, 636, 544, 9834, 813, 309, 767, 2203, 281, 2906, 264, 3097, 5633, 13, 50814, 50814, 467, 19251, 707, 1749, 929, 295, 8482, 2758, 926, 264, 1472, 295, 1901, 16979, 13, 51064, 51064, 1282, 264, 1411, 456, 311, 257, 46115, 295, 3092, 1901, 300, 311, 733, 295, 2651, 264, 3097, 992, 1783, 311, 11, 293, 286, 600, 10117, 257, 2182, 1783, 456, 281, 855, 300, 1310, 341, 576, 312, 364, 17641, 44745, 1365, 689, 321, 2066, 257, 21538, 281, 312, 1783, 457, 264, 2316, 6269, 82, 422, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0920472786976741, "compression_ratio": 1.6227758007117439, "no_speech_prob": 1.2428220543370117e-05}, {"id": 123, "seek": 82900, "start": 829.0, "end": 836.0, "text": " And on the right I've shown that there's a red O where we have another adversarial example. We're very near the other O's.", "tokens": [50364, 400, 322, 264, 558, 286, 600, 4898, 300, 456, 311, 257, 2182, 422, 689, 321, 362, 1071, 17641, 44745, 1365, 13, 492, 434, 588, 2651, 264, 661, 422, 311, 13, 50714, 50714, 492, 1062, 2066, 264, 2316, 281, 6269, 341, 1508, 281, 312, 364, 422, 11, 293, 1939, 570, 309, 311, 10117, 3344, 2758, 456, 11, 309, 311, 767, 49602, 309, 281, 312, 364, 1783, 13, 51114, 51114, 407, 498, 670, 69, 2414, 307, 534, 264, 1657, 11, 550, 1184, 17641, 44745, 1365, 307, 544, 420, 1570, 264, 1874, 295, 1578, 3668, 11, 293, 611, 544, 420, 1570, 3845, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08603848860814022, "compression_ratio": 1.7004219409282701, "no_speech_prob": 6.127052074589301e-06}, {"id": 124, "seek": 82900, "start": 836.0, "end": 844.0, "text": " We might expect the model to assign this class to be an O, and yet because it's drawn blue mass there, it's actually assigning it to be an X.", "tokens": [50364, 400, 322, 264, 558, 286, 600, 4898, 300, 456, 311, 257, 2182, 422, 689, 321, 362, 1071, 17641, 44745, 1365, 13, 492, 434, 588, 2651, 264, 661, 422, 311, 13, 50714, 50714, 492, 1062, 2066, 264, 2316, 281, 6269, 341, 1508, 281, 312, 364, 422, 11, 293, 1939, 570, 309, 311, 10117, 3344, 2758, 456, 11, 309, 311, 767, 49602, 309, 281, 312, 364, 1783, 13, 51114, 51114, 407, 498, 670, 69, 2414, 307, 534, 264, 1657, 11, 550, 1184, 17641, 44745, 1365, 307, 544, 420, 1570, 264, 1874, 295, 1578, 3668, 11, 293, 611, 544, 420, 1570, 3845, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08603848860814022, "compression_ratio": 1.7004219409282701, "no_speech_prob": 6.127052074589301e-06}, {"id": 125, "seek": 82900, "start": 844.0, "end": 853.0, "text": " So if overfitting is really the story, then each adversarial example is more or less the result of bad luck, and also more or less unique.", "tokens": [50364, 400, 322, 264, 558, 286, 600, 4898, 300, 456, 311, 257, 2182, 422, 689, 321, 362, 1071, 17641, 44745, 1365, 13, 492, 434, 588, 2651, 264, 661, 422, 311, 13, 50714, 50714, 492, 1062, 2066, 264, 2316, 281, 6269, 341, 1508, 281, 312, 364, 422, 11, 293, 1939, 570, 309, 311, 10117, 3344, 2758, 456, 11, 309, 311, 767, 49602, 309, 281, 312, 364, 1783, 13, 51114, 51114, 407, 498, 670, 69, 2414, 307, 534, 264, 1657, 11, 550, 1184, 17641, 44745, 1365, 307, 544, 420, 1570, 264, 1874, 295, 1578, 3668, 11, 293, 611, 544, 420, 1570, 3845, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08603848860814022, "compression_ratio": 1.7004219409282701, "no_speech_prob": 6.127052074589301e-06}, {"id": 126, "seek": 85300, "start": 853.0, "end": 862.0, "text": " If we fit the model again or we fit a slightly different model, we would expect to make different random mistakes on these points that are off the training set.", "tokens": [50364, 759, 321, 3318, 264, 2316, 797, 420, 321, 3318, 257, 4748, 819, 2316, 11, 321, 576, 2066, 281, 652, 819, 4974, 8038, 322, 613, 2793, 300, 366, 766, 264, 3097, 992, 13, 50814, 50814, 583, 300, 390, 767, 406, 437, 321, 1352, 412, 439, 13, 50964, 50964, 492, 1352, 300, 867, 819, 5245, 576, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 11, 293, 436, 576, 6269, 264, 912, 1508, 281, 552, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07381841114589147, "compression_ratio": 1.7208121827411167, "no_speech_prob": 1.4999351151345763e-05}, {"id": 127, "seek": 85300, "start": 862.0, "end": 865.0, "text": " But that was actually not what we found at all.", "tokens": [50364, 759, 321, 3318, 264, 2316, 797, 420, 321, 3318, 257, 4748, 819, 2316, 11, 321, 576, 2066, 281, 652, 819, 4974, 8038, 322, 613, 2793, 300, 366, 766, 264, 3097, 992, 13, 50814, 50814, 583, 300, 390, 767, 406, 437, 321, 1352, 412, 439, 13, 50964, 50964, 492, 1352, 300, 867, 819, 5245, 576, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 11, 293, 436, 576, 6269, 264, 912, 1508, 281, 552, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07381841114589147, "compression_ratio": 1.7208121827411167, "no_speech_prob": 1.4999351151345763e-05}, {"id": 128, "seek": 85300, "start": 865.0, "end": 874.0, "text": " We found that many different models would misclassify the same adversarial examples, and they would assign the same class to them.", "tokens": [50364, 759, 321, 3318, 264, 2316, 797, 420, 321, 3318, 257, 4748, 819, 2316, 11, 321, 576, 2066, 281, 652, 819, 4974, 8038, 322, 613, 2793, 300, 366, 766, 264, 3097, 992, 13, 50814, 50814, 583, 300, 390, 767, 406, 437, 321, 1352, 412, 439, 13, 50964, 50964, 492, 1352, 300, 867, 819, 5245, 576, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 11, 293, 436, 576, 6269, 264, 912, 1508, 281, 552, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07381841114589147, "compression_ratio": 1.7208121827411167, "no_speech_prob": 1.4999351151345763e-05}, {"id": 129, "seek": 87400, "start": 874.0, "end": 889.0, "text": " We also found that if we took the difference between an original example and an adversarial example, then we had a direction in input space, and we could add that same offset vector to any clean example,", "tokens": [50364, 492, 611, 1352, 300, 498, 321, 1890, 264, 2649, 1296, 364, 3380, 1365, 293, 364, 17641, 44745, 1365, 11, 550, 321, 632, 257, 3513, 294, 4846, 1901, 11, 293, 321, 727, 909, 300, 912, 18687, 8062, 281, 604, 2541, 1365, 11, 51114, 51114, 293, 321, 576, 1920, 1009, 483, 364, 17641, 44745, 1365, 382, 257, 1874, 13, 51264, 51264, 407, 321, 1409, 281, 4325, 300, 456, 307, 257, 27249, 1802, 516, 322, 510, 11, 406, 445, 257, 4974, 1802, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0672213009425572, "compression_ratio": 1.6972477064220184, "no_speech_prob": 9.107259757001884e-06}, {"id": 130, "seek": 87400, "start": 889.0, "end": 892.0, "text": " and we would almost always get an adversarial example as a result.", "tokens": [50364, 492, 611, 1352, 300, 498, 321, 1890, 264, 2649, 1296, 364, 3380, 1365, 293, 364, 17641, 44745, 1365, 11, 550, 321, 632, 257, 3513, 294, 4846, 1901, 11, 293, 321, 727, 909, 300, 912, 18687, 8062, 281, 604, 2541, 1365, 11, 51114, 51114, 293, 321, 576, 1920, 1009, 483, 364, 17641, 44745, 1365, 382, 257, 1874, 13, 51264, 51264, 407, 321, 1409, 281, 4325, 300, 456, 307, 257, 27249, 1802, 516, 322, 510, 11, 406, 445, 257, 4974, 1802, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0672213009425572, "compression_ratio": 1.6972477064220184, "no_speech_prob": 9.107259757001884e-06}, {"id": 131, "seek": 87400, "start": 892.0, "end": 898.0, "text": " So we started to realize that there is a systematic effect going on here, not just a random effect.", "tokens": [50364, 492, 611, 1352, 300, 498, 321, 1890, 264, 2649, 1296, 364, 3380, 1365, 293, 364, 17641, 44745, 1365, 11, 550, 321, 632, 257, 3513, 294, 4846, 1901, 11, 293, 321, 727, 909, 300, 912, 18687, 8062, 281, 604, 2541, 1365, 11, 51114, 51114, 293, 321, 576, 1920, 1009, 483, 364, 17641, 44745, 1365, 382, 257, 1874, 13, 51264, 51264, 407, 321, 1409, 281, 4325, 300, 456, 307, 257, 27249, 1802, 516, 322, 510, 11, 406, 445, 257, 4974, 1802, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0672213009425572, "compression_ratio": 1.6972477064220184, "no_speech_prob": 9.107259757001884e-06}, {"id": 132, "seek": 89800, "start": 898.0, "end": 906.0, "text": " That led us to another idea, which is that adversarial examples might actually be more like underfitting rather than overfitting.", "tokens": [50364, 663, 4684, 505, 281, 1071, 1558, 11, 597, 307, 300, 17641, 44745, 5110, 1062, 767, 312, 544, 411, 833, 69, 2414, 2831, 813, 670, 69, 2414, 13, 50764, 50764, 814, 1062, 767, 808, 490, 264, 2316, 885, 886, 8213, 13, 50964, 50964, 1692, 286, 2642, 264, 912, 5633, 797, 11, 689, 321, 362, 264, 912, 47138, 295, 422, 311, 293, 264, 912, 1622, 295, 1783, 311, 13, 51314, 51314, 400, 341, 565, 286, 3318, 257, 8213, 2316, 281, 264, 1412, 992, 11, 2831, 813, 15669, 257, 1090, 6042, 2107, 28263, 2316, 281, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06496669808212592, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.7780435882741585e-05}, {"id": 133, "seek": 89800, "start": 906.0, "end": 910.0, "text": " They might actually come from the model being too linear.", "tokens": [50364, 663, 4684, 505, 281, 1071, 1558, 11, 597, 307, 300, 17641, 44745, 5110, 1062, 767, 312, 544, 411, 833, 69, 2414, 2831, 813, 670, 69, 2414, 13, 50764, 50764, 814, 1062, 767, 808, 490, 264, 2316, 885, 886, 8213, 13, 50964, 50964, 1692, 286, 2642, 264, 912, 5633, 797, 11, 689, 321, 362, 264, 912, 47138, 295, 422, 311, 293, 264, 912, 1622, 295, 1783, 311, 13, 51314, 51314, 400, 341, 565, 286, 3318, 257, 8213, 2316, 281, 264, 1412, 992, 11, 2831, 813, 15669, 257, 1090, 6042, 2107, 28263, 2316, 281, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06496669808212592, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.7780435882741585e-05}, {"id": 134, "seek": 89800, "start": 910.0, "end": 917.0, "text": " Here I draw the same task again, where we have the same manifold of O's and the same line of X's.", "tokens": [50364, 663, 4684, 505, 281, 1071, 1558, 11, 597, 307, 300, 17641, 44745, 5110, 1062, 767, 312, 544, 411, 833, 69, 2414, 2831, 813, 670, 69, 2414, 13, 50764, 50764, 814, 1062, 767, 808, 490, 264, 2316, 885, 886, 8213, 13, 50964, 50964, 1692, 286, 2642, 264, 912, 5633, 797, 11, 689, 321, 362, 264, 912, 47138, 295, 422, 311, 293, 264, 912, 1622, 295, 1783, 311, 13, 51314, 51314, 400, 341, 565, 286, 3318, 257, 8213, 2316, 281, 264, 1412, 992, 11, 2831, 813, 15669, 257, 1090, 6042, 2107, 28263, 2316, 281, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06496669808212592, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.7780435882741585e-05}, {"id": 135, "seek": 89800, "start": 917.0, "end": 924.0, "text": " And this time I fit a linear model to the data set, rather than fitting a high capacity nonlinear model to it.", "tokens": [50364, 663, 4684, 505, 281, 1071, 1558, 11, 597, 307, 300, 17641, 44745, 5110, 1062, 767, 312, 544, 411, 833, 69, 2414, 2831, 813, 670, 69, 2414, 13, 50764, 50764, 814, 1062, 767, 808, 490, 264, 2316, 885, 886, 8213, 13, 50964, 50964, 1692, 286, 2642, 264, 912, 5633, 797, 11, 689, 321, 362, 264, 912, 47138, 295, 422, 311, 293, 264, 912, 1622, 295, 1783, 311, 13, 51314, 51314, 400, 341, 565, 286, 3318, 257, 8213, 2316, 281, 264, 1412, 992, 11, 2831, 813, 15669, 257, 1090, 6042, 2107, 28263, 2316, 281, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06496669808212592, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.7780435882741585e-05}, {"id": 136, "seek": 92400, "start": 924.0, "end": 929.0, "text": " We see that we get a dividing hyperplane running in between the two classes.", "tokens": [50364, 492, 536, 300, 321, 483, 257, 26764, 9848, 36390, 2614, 294, 1296, 264, 732, 5359, 13, 50614, 50614, 583, 341, 9848, 36390, 1177, 380, 534, 7983, 264, 2074, 3877, 295, 264, 5359, 13, 50864, 50864, 440, 422, 311, 366, 4448, 18721, 294, 257, 22535, 71, 18653, 47138, 13, 51064, 51064, 759, 321, 1066, 4494, 1791, 264, 917, 295, 264, 422, 311, 11, 321, 3278, 264, 3537, 12866, 11, 293, 321, 600, 10117, 257, 2182, 422, 11, 689, 754, 1673, 321, 434, 588, 2651, 264, 3537, 12866, 293, 2651, 661, 422, 311, 11, 321, 1697, 300, 309, 307, 586, 364, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07473211288452149, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.0687115465989336e-05}, {"id": 137, "seek": 92400, "start": 929.0, "end": 934.0, "text": " But this hyperplane doesn't really capture the true structure of the classes.", "tokens": [50364, 492, 536, 300, 321, 483, 257, 26764, 9848, 36390, 2614, 294, 1296, 264, 732, 5359, 13, 50614, 50614, 583, 341, 9848, 36390, 1177, 380, 534, 7983, 264, 2074, 3877, 295, 264, 5359, 13, 50864, 50864, 440, 422, 311, 366, 4448, 18721, 294, 257, 22535, 71, 18653, 47138, 13, 51064, 51064, 759, 321, 1066, 4494, 1791, 264, 917, 295, 264, 422, 311, 11, 321, 3278, 264, 3537, 12866, 11, 293, 321, 600, 10117, 257, 2182, 422, 11, 689, 754, 1673, 321, 434, 588, 2651, 264, 3537, 12866, 293, 2651, 661, 422, 311, 11, 321, 1697, 300, 309, 307, 586, 364, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07473211288452149, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.0687115465989336e-05}, {"id": 138, "seek": 92400, "start": 934.0, "end": 938.0, "text": " The O's are clearly arranged in a seashaped manifold.", "tokens": [50364, 492, 536, 300, 321, 483, 257, 26764, 9848, 36390, 2614, 294, 1296, 264, 732, 5359, 13, 50614, 50614, 583, 341, 9848, 36390, 1177, 380, 534, 7983, 264, 2074, 3877, 295, 264, 5359, 13, 50864, 50864, 440, 422, 311, 366, 4448, 18721, 294, 257, 22535, 71, 18653, 47138, 13, 51064, 51064, 759, 321, 1066, 4494, 1791, 264, 917, 295, 264, 422, 311, 11, 321, 3278, 264, 3537, 12866, 11, 293, 321, 600, 10117, 257, 2182, 422, 11, 689, 754, 1673, 321, 434, 588, 2651, 264, 3537, 12866, 293, 2651, 661, 422, 311, 11, 321, 1697, 300, 309, 307, 586, 364, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07473211288452149, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.0687115465989336e-05}, {"id": 139, "seek": 92400, "start": 938.0, "end": 950.0, "text": " If we keep walking past the end of the O's, we cross the decision boundary, and we've drawn a red O, where even though we're very near the decision boundary and near other O's, we believe that it is now an X.", "tokens": [50364, 492, 536, 300, 321, 483, 257, 26764, 9848, 36390, 2614, 294, 1296, 264, 732, 5359, 13, 50614, 50614, 583, 341, 9848, 36390, 1177, 380, 534, 7983, 264, 2074, 3877, 295, 264, 5359, 13, 50864, 50864, 440, 422, 311, 366, 4448, 18721, 294, 257, 22535, 71, 18653, 47138, 13, 51064, 51064, 759, 321, 1066, 4494, 1791, 264, 917, 295, 264, 422, 311, 11, 321, 3278, 264, 3537, 12866, 11, 293, 321, 600, 10117, 257, 2182, 422, 11, 689, 754, 1673, 321, 434, 588, 2651, 264, 3537, 12866, 293, 2651, 661, 422, 311, 11, 321, 1697, 300, 309, 307, 586, 364, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07473211288452149, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.0687115465989336e-05}, {"id": 140, "seek": 95000, "start": 950.0, "end": 958.0, "text": " Similarly, we can take steps that go from near X's to just over the line that our classified as O's.", "tokens": [50364, 13157, 11, 321, 393, 747, 4439, 300, 352, 490, 2651, 1783, 311, 281, 445, 670, 264, 1622, 300, 527, 20627, 382, 422, 311, 13, 50764, 50764, 3996, 551, 300, 311, 8344, 10901, 466, 341, 7542, 307, 300, 498, 321, 574, 412, 264, 3126, 1411, 420, 6597, 558, 12413, 11, 613, 12413, 366, 588, 41956, 20627, 382, 885, 1783, 311, 322, 264, 3126, 1411, 420, 422, 311, 322, 264, 6597, 558, 13, 51364, 51364, 2754, 1673, 321, 600, 1128, 1612, 604, 1412, 670, 456, 412, 439, 11, 264, 8213, 2316, 1605, 5874, 264, 2316, 281, 362, 588, 1090, 6687, 294, 613, 10682, 300, 366, 588, 1400, 490, 264, 3537, 12866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06905624322723924, "compression_ratio": 1.8178571428571428, "no_speech_prob": 1.0503342309675645e-05}, {"id": 141, "seek": 95000, "start": 958.0, "end": 970.0, "text": " Another thing that's somewhat unusual about this plot is that if we look at the lower left or upper right corners, these corners are very confidently classified as being X's on the lower left or O's on the upper right.", "tokens": [50364, 13157, 11, 321, 393, 747, 4439, 300, 352, 490, 2651, 1783, 311, 281, 445, 670, 264, 1622, 300, 527, 20627, 382, 422, 311, 13, 50764, 50764, 3996, 551, 300, 311, 8344, 10901, 466, 341, 7542, 307, 300, 498, 321, 574, 412, 264, 3126, 1411, 420, 6597, 558, 12413, 11, 613, 12413, 366, 588, 41956, 20627, 382, 885, 1783, 311, 322, 264, 3126, 1411, 420, 422, 311, 322, 264, 6597, 558, 13, 51364, 51364, 2754, 1673, 321, 600, 1128, 1612, 604, 1412, 670, 456, 412, 439, 11, 264, 8213, 2316, 1605, 5874, 264, 2316, 281, 362, 588, 1090, 6687, 294, 613, 10682, 300, 366, 588, 1400, 490, 264, 3537, 12866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06905624322723924, "compression_ratio": 1.8178571428571428, "no_speech_prob": 1.0503342309675645e-05}, {"id": 142, "seek": 95000, "start": 970.0, "end": 977.0, "text": " Even though we've never seen any data over there at all, the linear model family forces the model to have very high confidence in these regions that are very far from the decision boundary.", "tokens": [50364, 13157, 11, 321, 393, 747, 4439, 300, 352, 490, 2651, 1783, 311, 281, 445, 670, 264, 1622, 300, 527, 20627, 382, 422, 311, 13, 50764, 50764, 3996, 551, 300, 311, 8344, 10901, 466, 341, 7542, 307, 300, 498, 321, 574, 412, 264, 3126, 1411, 420, 6597, 558, 12413, 11, 613, 12413, 366, 588, 41956, 20627, 382, 885, 1783, 311, 322, 264, 3126, 1411, 420, 422, 311, 322, 264, 6597, 558, 13, 51364, 51364, 2754, 1673, 321, 600, 1128, 1612, 604, 1412, 670, 456, 412, 439, 11, 264, 8213, 2316, 1605, 5874, 264, 2316, 281, 362, 588, 1090, 6687, 294, 613, 10682, 300, 366, 588, 1400, 490, 264, 3537, 12866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06905624322723924, "compression_ratio": 1.8178571428571428, "no_speech_prob": 1.0503342309675645e-05}, {"id": 143, "seek": 97700, "start": 977.0, "end": 993.0, "text": " We've seen that linear models can actually assign really unusual confidence as you move very far from the decision boundary, even if there isn't any data there.", "tokens": [50364, 492, 600, 1612, 300, 8213, 5245, 393, 767, 6269, 534, 10901, 6687, 382, 291, 1286, 588, 1400, 490, 264, 3537, 12866, 11, 754, 498, 456, 1943, 380, 604, 1412, 456, 13, 51164, 51164, 583, 527, 2452, 18161, 9590, 767, 1340, 411, 8213, 5245, 727, 8213, 5245, 767, 2903, 1340, 466, 577, 309, 307, 300, 2452, 18161, 36170, 3061, 13, 51614, 51614, 467, 4523, 484, 300, 4363, 2452, 18161, 36170, 366, 767, 588, 2522, 3711, 8213, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06398510932922363, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.6871775844483636e-06}, {"id": 144, "seek": 97700, "start": 993.0, "end": 1002.0, "text": " But our deep neural networks actually anything like linear models could linear models actually explain anything about how it is that deep neural nets fail.", "tokens": [50364, 492, 600, 1612, 300, 8213, 5245, 393, 767, 6269, 534, 10901, 6687, 382, 291, 1286, 588, 1400, 490, 264, 3537, 12866, 11, 754, 498, 456, 1943, 380, 604, 1412, 456, 13, 51164, 51164, 583, 527, 2452, 18161, 9590, 767, 1340, 411, 8213, 5245, 727, 8213, 5245, 767, 2903, 1340, 466, 577, 309, 307, 300, 2452, 18161, 36170, 3061, 13, 51614, 51614, 467, 4523, 484, 300, 4363, 2452, 18161, 36170, 366, 767, 588, 2522, 3711, 8213, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06398510932922363, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.6871775844483636e-06}, {"id": 145, "seek": 97700, "start": 1002.0, "end": 1006.0, "text": " It turns out that modern deep neural nets are actually very piecewise linear.", "tokens": [50364, 492, 600, 1612, 300, 8213, 5245, 393, 767, 6269, 534, 10901, 6687, 382, 291, 1286, 588, 1400, 490, 264, 3537, 12866, 11, 754, 498, 456, 1943, 380, 604, 1412, 456, 13, 51164, 51164, 583, 527, 2452, 18161, 9590, 767, 1340, 411, 8213, 5245, 727, 8213, 5245, 767, 2903, 1340, 466, 577, 309, 307, 300, 2452, 18161, 36170, 3061, 13, 51614, 51614, 467, 4523, 484, 300, 4363, 2452, 18161, 36170, 366, 767, 588, 2522, 3711, 8213, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06398510932922363, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.6871775844483636e-06}, {"id": 146, "seek": 100600, "start": 1006.0, "end": 1014.0, "text": " Rather than being a single linear function, there are piecewise linear with maybe not that many linear pieces.", "tokens": [50364, 16571, 813, 885, 257, 2167, 8213, 2445, 11, 456, 366, 2522, 3711, 8213, 365, 1310, 406, 300, 867, 8213, 3755, 13, 50764, 50764, 759, 321, 764, 11048, 2587, 8213, 6815, 11, 550, 264, 18350, 490, 264, 4846, 3256, 281, 264, 5598, 3565, 1208, 307, 3736, 257, 2522, 3711, 8213, 2445, 13, 51264, 51264, 3146, 264, 3565, 1208, 11, 286, 914, 264, 517, 23157, 1602, 3565, 33783, 949, 321, 3079, 264, 2787, 19513, 404, 412, 264, 5598, 295, 264, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09944015457516625, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.8163303543115035e-05}, {"id": 147, "seek": 100600, "start": 1014.0, "end": 1024.0, "text": " If we use rectified linear units, then the mapping from the input image to the output logits is literally a piecewise linear function.", "tokens": [50364, 16571, 813, 885, 257, 2167, 8213, 2445, 11, 456, 366, 2522, 3711, 8213, 365, 1310, 406, 300, 867, 8213, 3755, 13, 50764, 50764, 759, 321, 764, 11048, 2587, 8213, 6815, 11, 550, 264, 18350, 490, 264, 4846, 3256, 281, 264, 5598, 3565, 1208, 307, 3736, 257, 2522, 3711, 8213, 2445, 13, 51264, 51264, 3146, 264, 3565, 1208, 11, 286, 914, 264, 517, 23157, 1602, 3565, 33783, 949, 321, 3079, 264, 2787, 19513, 404, 412, 264, 5598, 295, 264, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09944015457516625, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.8163303543115035e-05}, {"id": 148, "seek": 100600, "start": 1024.0, "end": 1032.0, "text": " By the logits, I mean the unnormalized log probabilities before we apply the soft backsop at the output of the model.", "tokens": [50364, 16571, 813, 885, 257, 2167, 8213, 2445, 11, 456, 366, 2522, 3711, 8213, 365, 1310, 406, 300, 867, 8213, 3755, 13, 50764, 50764, 759, 321, 764, 11048, 2587, 8213, 6815, 11, 550, 264, 18350, 490, 264, 4846, 3256, 281, 264, 5598, 3565, 1208, 307, 3736, 257, 2522, 3711, 8213, 2445, 13, 51264, 51264, 3146, 264, 3565, 1208, 11, 286, 914, 264, 517, 23157, 1602, 3565, 33783, 949, 321, 3079, 264, 2787, 19513, 404, 412, 264, 5598, 295, 264, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09944015457516625, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.8163303543115035e-05}, {"id": 149, "seek": 103200, "start": 1032.0, "end": 1037.0, "text": " There are other neural networks like max out networks that are also literally piecewise linear.", "tokens": [50364, 821, 366, 661, 18161, 9590, 411, 11469, 484, 9590, 300, 366, 611, 3736, 2522, 3711, 8213, 13, 50614, 50614, 400, 550, 456, 366, 2940, 300, 808, 588, 1998, 281, 309, 13, 50764, 50764, 4546, 11048, 2587, 8213, 6815, 3062, 3743, 11, 881, 561, 1143, 281, 764, 4556, 3280, 327, 6815, 412, 472, 1254, 420, 1071, 2139, 3565, 3142, 4556, 3280, 327, 420, 9848, 65, 7940, 27747, 6815, 13, 51314, 51314, 1981, 4556, 3280, 327, 6815, 362, 281, 312, 7500, 10870, 11, 2318, 412, 364, 5883, 2144, 11, 370, 300, 291, 3496, 881, 295, 428, 565, 2651, 264, 3056, 295, 264, 4556, 3280, 327, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10775521066453722, "compression_ratio": 1.751908396946565, "no_speech_prob": 6.905858299433021e-06}, {"id": 150, "seek": 103200, "start": 1037.0, "end": 1040.0, "text": " And then there are several that come very close to it.", "tokens": [50364, 821, 366, 661, 18161, 9590, 411, 11469, 484, 9590, 300, 366, 611, 3736, 2522, 3711, 8213, 13, 50614, 50614, 400, 550, 456, 366, 2940, 300, 808, 588, 1998, 281, 309, 13, 50764, 50764, 4546, 11048, 2587, 8213, 6815, 3062, 3743, 11, 881, 561, 1143, 281, 764, 4556, 3280, 327, 6815, 412, 472, 1254, 420, 1071, 2139, 3565, 3142, 4556, 3280, 327, 420, 9848, 65, 7940, 27747, 6815, 13, 51314, 51314, 1981, 4556, 3280, 327, 6815, 362, 281, 312, 7500, 10870, 11, 2318, 412, 364, 5883, 2144, 11, 370, 300, 291, 3496, 881, 295, 428, 565, 2651, 264, 3056, 295, 264, 4556, 3280, 327, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10775521066453722, "compression_ratio": 1.751908396946565, "no_speech_prob": 6.905858299433021e-06}, {"id": 151, "seek": 103200, "start": 1040.0, "end": 1051.0, "text": " Before rectified linear units became popular, most people used to use sigmoid units at one form or another either logistic sigmoid or hyperbolic tangent units.", "tokens": [50364, 821, 366, 661, 18161, 9590, 411, 11469, 484, 9590, 300, 366, 611, 3736, 2522, 3711, 8213, 13, 50614, 50614, 400, 550, 456, 366, 2940, 300, 808, 588, 1998, 281, 309, 13, 50764, 50764, 4546, 11048, 2587, 8213, 6815, 3062, 3743, 11, 881, 561, 1143, 281, 764, 4556, 3280, 327, 6815, 412, 472, 1254, 420, 1071, 2139, 3565, 3142, 4556, 3280, 327, 420, 9848, 65, 7940, 27747, 6815, 13, 51314, 51314, 1981, 4556, 3280, 327, 6815, 362, 281, 312, 7500, 10870, 11, 2318, 412, 364, 5883, 2144, 11, 370, 300, 291, 3496, 881, 295, 428, 565, 2651, 264, 3056, 295, 264, 4556, 3280, 327, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10775521066453722, "compression_ratio": 1.751908396946565, "no_speech_prob": 6.905858299433021e-06}, {"id": 152, "seek": 103200, "start": 1051.0, "end": 1061.0, "text": " These sigmoid units have to be carefully tuned, especially at an initialization, so that you spend most of your time near the center of the sigmoid,", "tokens": [50364, 821, 366, 661, 18161, 9590, 411, 11469, 484, 9590, 300, 366, 611, 3736, 2522, 3711, 8213, 13, 50614, 50614, 400, 550, 456, 366, 2940, 300, 808, 588, 1998, 281, 309, 13, 50764, 50764, 4546, 11048, 2587, 8213, 6815, 3062, 3743, 11, 881, 561, 1143, 281, 764, 4556, 3280, 327, 6815, 412, 472, 1254, 420, 1071, 2139, 3565, 3142, 4556, 3280, 327, 420, 9848, 65, 7940, 27747, 6815, 13, 51314, 51314, 1981, 4556, 3280, 327, 6815, 362, 281, 312, 7500, 10870, 11, 2318, 412, 364, 5883, 2144, 11, 370, 300, 291, 3496, 881, 295, 428, 565, 2651, 264, 3056, 295, 264, 4556, 3280, 327, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10775521066453722, "compression_ratio": 1.751908396946565, "no_speech_prob": 6.905858299433021e-06}, {"id": 153, "seek": 106100, "start": 1061.0, "end": 1064.0, "text": " or the sigmoid is approximately linear.", "tokens": [50364, 420, 264, 4556, 3280, 327, 307, 10447, 8213, 13, 50514, 50514, 400, 550, 2721, 11, 264, 441, 6840, 44, 11, 257, 733, 295, 18680, 1753, 3209, 300, 307, 472, 295, 264, 881, 3743, 18680, 1753, 9590, 965, 11, 50814, 50814, 4960, 4500, 490, 472, 565, 1823, 281, 264, 958, 294, 1668, 281, 33384, 293, 1604, 1589, 670, 565, 13, 51164, 51164, 5349, 849, 307, 257, 4098, 2199, 1254, 295, 8213, 507, 11, 370, 321, 393, 536, 300, 264, 9285, 490, 257, 588, 17275, 565, 1823, 294, 264, 1791, 293, 264, 1974, 307, 5405, 8213, 1951, 364, 441, 6840, 44, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08662284337557279, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.0414203390828334e-05}, {"id": 154, "seek": 106100, "start": 1064.0, "end": 1070.0, "text": " And then finally, the LSTM, a kind of recurrent network that is one of the most popular recurrent networks today,", "tokens": [50364, 420, 264, 4556, 3280, 327, 307, 10447, 8213, 13, 50514, 50514, 400, 550, 2721, 11, 264, 441, 6840, 44, 11, 257, 733, 295, 18680, 1753, 3209, 300, 307, 472, 295, 264, 881, 3743, 18680, 1753, 9590, 965, 11, 50814, 50814, 4960, 4500, 490, 472, 565, 1823, 281, 264, 958, 294, 1668, 281, 33384, 293, 1604, 1589, 670, 565, 13, 51164, 51164, 5349, 849, 307, 257, 4098, 2199, 1254, 295, 8213, 507, 11, 370, 321, 393, 536, 300, 264, 9285, 490, 257, 588, 17275, 565, 1823, 294, 264, 1791, 293, 264, 1974, 307, 5405, 8213, 1951, 364, 441, 6840, 44, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08662284337557279, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.0414203390828334e-05}, {"id": 155, "seek": 106100, "start": 1070.0, "end": 1077.0, "text": " uses addition from one time step to the next in order to accumulate and remember information over time.", "tokens": [50364, 420, 264, 4556, 3280, 327, 307, 10447, 8213, 13, 50514, 50514, 400, 550, 2721, 11, 264, 441, 6840, 44, 11, 257, 733, 295, 18680, 1753, 3209, 300, 307, 472, 295, 264, 881, 3743, 18680, 1753, 9590, 965, 11, 50814, 50814, 4960, 4500, 490, 472, 565, 1823, 281, 264, 958, 294, 1668, 281, 33384, 293, 1604, 1589, 670, 565, 13, 51164, 51164, 5349, 849, 307, 257, 4098, 2199, 1254, 295, 8213, 507, 11, 370, 321, 393, 536, 300, 264, 9285, 490, 257, 588, 17275, 565, 1823, 294, 264, 1791, 293, 264, 1974, 307, 5405, 8213, 1951, 364, 441, 6840, 44, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08662284337557279, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.0414203390828334e-05}, {"id": 156, "seek": 106100, "start": 1077.0, "end": 1089.0, "text": " Addition is a particularly simple form of linearity, so we can see that the interaction from a very distant time step in the past and the present is highly linear within an LSTM.", "tokens": [50364, 420, 264, 4556, 3280, 327, 307, 10447, 8213, 13, 50514, 50514, 400, 550, 2721, 11, 264, 441, 6840, 44, 11, 257, 733, 295, 18680, 1753, 3209, 300, 307, 472, 295, 264, 881, 3743, 18680, 1753, 9590, 965, 11, 50814, 50814, 4960, 4500, 490, 472, 565, 1823, 281, 264, 958, 294, 1668, 281, 33384, 293, 1604, 1589, 670, 565, 13, 51164, 51164, 5349, 849, 307, 257, 4098, 2199, 1254, 295, 8213, 507, 11, 370, 321, 393, 536, 300, 264, 9285, 490, 257, 588, 17275, 565, 1823, 294, 264, 1791, 293, 264, 1974, 307, 5405, 8213, 1951, 364, 441, 6840, 44, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08662284337557279, "compression_ratio": 1.7233201581027668, "no_speech_prob": 2.0414203390828334e-05}, {"id": 157, "seek": 108900, "start": 1089.0, "end": 1095.0, "text": " And how to be clear, I'm speaking about the mapping from the input of the model to the output of the model.", "tokens": [50364, 400, 577, 281, 312, 1850, 11, 286, 478, 4124, 466, 264, 18350, 490, 264, 4846, 295, 264, 2316, 281, 264, 5598, 295, 264, 2316, 13, 50664, 50664, 663, 311, 437, 286, 478, 1566, 307, 1998, 281, 885, 8213, 11, 420, 307, 2522, 3711, 8213, 365, 7226, 1326, 3755, 13, 50964, 50964, 440, 18350, 490, 264, 9834, 295, 264, 3209, 281, 264, 5598, 295, 264, 3209, 307, 2107, 28263, 11, 570, 264, 3364, 32284, 412, 1184, 4583, 295, 264, 3209, 366, 17207, 1214, 13, 51514, 51514, 407, 321, 767, 483, 4664, 2107, 28263, 13280, 1296, 9834, 293, 264, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06588527068351079, "compression_ratio": 1.9098360655737705, "no_speech_prob": 3.9460323023376986e-05}, {"id": 158, "seek": 108900, "start": 1095.0, "end": 1101.0, "text": " That's what I'm saying is close to being linear, or is piecewise linear with relatively few pieces.", "tokens": [50364, 400, 577, 281, 312, 1850, 11, 286, 478, 4124, 466, 264, 18350, 490, 264, 4846, 295, 264, 2316, 281, 264, 5598, 295, 264, 2316, 13, 50664, 50664, 663, 311, 437, 286, 478, 1566, 307, 1998, 281, 885, 8213, 11, 420, 307, 2522, 3711, 8213, 365, 7226, 1326, 3755, 13, 50964, 50964, 440, 18350, 490, 264, 9834, 295, 264, 3209, 281, 264, 5598, 295, 264, 3209, 307, 2107, 28263, 11, 570, 264, 3364, 32284, 412, 1184, 4583, 295, 264, 3209, 366, 17207, 1214, 13, 51514, 51514, 407, 321, 767, 483, 4664, 2107, 28263, 13280, 1296, 9834, 293, 264, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06588527068351079, "compression_ratio": 1.9098360655737705, "no_speech_prob": 3.9460323023376986e-05}, {"id": 159, "seek": 108900, "start": 1101.0, "end": 1112.0, "text": " The mapping from the parameters of the network to the output of the network is nonlinear, because the weight matrices at each layer of the network are multiplied together.", "tokens": [50364, 400, 577, 281, 312, 1850, 11, 286, 478, 4124, 466, 264, 18350, 490, 264, 4846, 295, 264, 2316, 281, 264, 5598, 295, 264, 2316, 13, 50664, 50664, 663, 311, 437, 286, 478, 1566, 307, 1998, 281, 885, 8213, 11, 420, 307, 2522, 3711, 8213, 365, 7226, 1326, 3755, 13, 50964, 50964, 440, 18350, 490, 264, 9834, 295, 264, 3209, 281, 264, 5598, 295, 264, 3209, 307, 2107, 28263, 11, 570, 264, 3364, 32284, 412, 1184, 4583, 295, 264, 3209, 366, 17207, 1214, 13, 51514, 51514, 407, 321, 767, 483, 4664, 2107, 28263, 13280, 1296, 9834, 293, 264, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06588527068351079, "compression_ratio": 1.9098360655737705, "no_speech_prob": 3.9460323023376986e-05}, {"id": 160, "seek": 108900, "start": 1112.0, "end": 1117.0, "text": " So we actually get extremely nonlinear interactions between parameters and the output.", "tokens": [50364, 400, 577, 281, 312, 1850, 11, 286, 478, 4124, 466, 264, 18350, 490, 264, 4846, 295, 264, 2316, 281, 264, 5598, 295, 264, 2316, 13, 50664, 50664, 663, 311, 437, 286, 478, 1566, 307, 1998, 281, 885, 8213, 11, 420, 307, 2522, 3711, 8213, 365, 7226, 1326, 3755, 13, 50964, 50964, 440, 18350, 490, 264, 9834, 295, 264, 3209, 281, 264, 5598, 295, 264, 3209, 307, 2107, 28263, 11, 570, 264, 3364, 32284, 412, 1184, 4583, 295, 264, 3209, 366, 17207, 1214, 13, 51514, 51514, 407, 321, 767, 483, 4664, 2107, 28263, 13280, 1296, 9834, 293, 264, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06588527068351079, "compression_ratio": 1.9098360655737705, "no_speech_prob": 3.9460323023376986e-05}, {"id": 161, "seek": 111700, "start": 1117.0, "end": 1120.0, "text": " That's what makes training an neural network so difficult.", "tokens": [50364, 663, 311, 437, 1669, 3097, 364, 18161, 3209, 370, 2252, 13, 50514, 50514, 583, 264, 18350, 490, 264, 4846, 281, 264, 5598, 307, 709, 544, 8213, 293, 27737, 11, 50814, 50814, 293, 309, 1355, 300, 19618, 2740, 300, 5939, 281, 19719, 264, 4846, 281, 264, 2316, 366, 709, 3571, 813, 19618, 2740, 300, 5939, 281, 19719, 264, 9834, 13, 51364, 51364, 759, 321, 352, 293, 574, 337, 341, 2737, 294, 3124, 11, 321, 393, 747, 257, 45216, 304, 3209, 293, 13508, 484, 257, 472, 18795, 3100, 807, 1080, 4846, 1901, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06342964674297132, "compression_ratio": 1.8677685950413223, "no_speech_prob": 1.2787081686838064e-05}, {"id": 162, "seek": 111700, "start": 1120.0, "end": 1126.0, "text": " But the mapping from the input to the output is much more linear and predictable,", "tokens": [50364, 663, 311, 437, 1669, 3097, 364, 18161, 3209, 370, 2252, 13, 50514, 50514, 583, 264, 18350, 490, 264, 4846, 281, 264, 5598, 307, 709, 544, 8213, 293, 27737, 11, 50814, 50814, 293, 309, 1355, 300, 19618, 2740, 300, 5939, 281, 19719, 264, 4846, 281, 264, 2316, 366, 709, 3571, 813, 19618, 2740, 300, 5939, 281, 19719, 264, 9834, 13, 51364, 51364, 759, 321, 352, 293, 574, 337, 341, 2737, 294, 3124, 11, 321, 393, 747, 257, 45216, 304, 3209, 293, 13508, 484, 257, 472, 18795, 3100, 807, 1080, 4846, 1901, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06342964674297132, "compression_ratio": 1.8677685950413223, "no_speech_prob": 1.2787081686838064e-05}, {"id": 163, "seek": 111700, "start": 1126.0, "end": 1137.0, "text": " and it means that optimization problems that aim to optimize the input to the model are much easier than optimization problems that aim to optimize the parameters.", "tokens": [50364, 663, 311, 437, 1669, 3097, 364, 18161, 3209, 370, 2252, 13, 50514, 50514, 583, 264, 18350, 490, 264, 4846, 281, 264, 5598, 307, 709, 544, 8213, 293, 27737, 11, 50814, 50814, 293, 309, 1355, 300, 19618, 2740, 300, 5939, 281, 19719, 264, 4846, 281, 264, 2316, 366, 709, 3571, 813, 19618, 2740, 300, 5939, 281, 19719, 264, 9834, 13, 51364, 51364, 759, 321, 352, 293, 574, 337, 341, 2737, 294, 3124, 11, 321, 393, 747, 257, 45216, 304, 3209, 293, 13508, 484, 257, 472, 18795, 3100, 807, 1080, 4846, 1901, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06342964674297132, "compression_ratio": 1.8677685950413223, "no_speech_prob": 1.2787081686838064e-05}, {"id": 164, "seek": 111700, "start": 1137.0, "end": 1146.0, "text": " If we go and look for this happening in practice, we can take a convolutional network and trace out a one dimensional path through its input space.", "tokens": [50364, 663, 311, 437, 1669, 3097, 364, 18161, 3209, 370, 2252, 13, 50514, 50514, 583, 264, 18350, 490, 264, 4846, 281, 264, 5598, 307, 709, 544, 8213, 293, 27737, 11, 50814, 50814, 293, 309, 1355, 300, 19618, 2740, 300, 5939, 281, 19719, 264, 4846, 281, 264, 2316, 366, 709, 3571, 813, 19618, 2740, 300, 5939, 281, 19719, 264, 9834, 13, 51364, 51364, 759, 321, 352, 293, 574, 337, 341, 2737, 294, 3124, 11, 321, 393, 747, 257, 45216, 304, 3209, 293, 13508, 484, 257, 472, 18795, 3100, 807, 1080, 4846, 1901, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06342964674297132, "compression_ratio": 1.8677685950413223, "no_speech_prob": 1.2787081686838064e-05}, {"id": 165, "seek": 114600, "start": 1146.0, "end": 1150.0, "text": " So what we're doing here is we're choosing a clean example.", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 10875, 257, 2541, 1365, 13, 50564, 50564, 467, 311, 364, 3256, 295, 257, 2418, 1032, 322, 257, 2182, 3678, 11, 293, 321, 366, 10875, 257, 3513, 300, 486, 3147, 807, 1901, 13, 50914, 50914, 492, 434, 516, 281, 362, 257, 17619, 17889, 300, 321, 12972, 538, 341, 3513, 13, 51164, 51164, 407, 364, 17889, 307, 3671, 2217, 11, 411, 412, 264, 1411, 917, 295, 264, 7542, 11, 321, 434, 16390, 278, 766, 257, 688, 295, 341, 4985, 8062, 3513, 13, 51514, 51514, 1133, 17889, 307, 4018, 11, 411, 412, 264, 2808, 295, 264, 7542, 11, 321, 434, 11700, 264, 3380, 3256, 490, 264, 28872, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08801330146142992, "compression_ratio": 1.8113207547169812, "no_speech_prob": 3.062577161472291e-05}, {"id": 166, "seek": 114600, "start": 1150.0, "end": 1157.0, "text": " It's an image of a white car on a red background, and we are choosing a direction that will travel through space.", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 10875, 257, 2541, 1365, 13, 50564, 50564, 467, 311, 364, 3256, 295, 257, 2418, 1032, 322, 257, 2182, 3678, 11, 293, 321, 366, 10875, 257, 3513, 300, 486, 3147, 807, 1901, 13, 50914, 50914, 492, 434, 516, 281, 362, 257, 17619, 17889, 300, 321, 12972, 538, 341, 3513, 13, 51164, 51164, 407, 364, 17889, 307, 3671, 2217, 11, 411, 412, 264, 1411, 917, 295, 264, 7542, 11, 321, 434, 16390, 278, 766, 257, 688, 295, 341, 4985, 8062, 3513, 13, 51514, 51514, 1133, 17889, 307, 4018, 11, 411, 412, 264, 2808, 295, 264, 7542, 11, 321, 434, 11700, 264, 3380, 3256, 490, 264, 28872, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08801330146142992, "compression_ratio": 1.8113207547169812, "no_speech_prob": 3.062577161472291e-05}, {"id": 167, "seek": 114600, "start": 1157.0, "end": 1162.0, "text": " We're going to have a coefficient epsilon that we multiply by this direction.", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 10875, 257, 2541, 1365, 13, 50564, 50564, 467, 311, 364, 3256, 295, 257, 2418, 1032, 322, 257, 2182, 3678, 11, 293, 321, 366, 10875, 257, 3513, 300, 486, 3147, 807, 1901, 13, 50914, 50914, 492, 434, 516, 281, 362, 257, 17619, 17889, 300, 321, 12972, 538, 341, 3513, 13, 51164, 51164, 407, 364, 17889, 307, 3671, 2217, 11, 411, 412, 264, 1411, 917, 295, 264, 7542, 11, 321, 434, 16390, 278, 766, 257, 688, 295, 341, 4985, 8062, 3513, 13, 51514, 51514, 1133, 17889, 307, 4018, 11, 411, 412, 264, 2808, 295, 264, 7542, 11, 321, 434, 11700, 264, 3380, 3256, 490, 264, 28872, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08801330146142992, "compression_ratio": 1.8113207547169812, "no_speech_prob": 3.062577161472291e-05}, {"id": 168, "seek": 114600, "start": 1162.0, "end": 1169.0, "text": " So an epsilon is negative 30, like at the left end of the plot, we're subtracting off a lot of this unit vector direction.", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 10875, 257, 2541, 1365, 13, 50564, 50564, 467, 311, 364, 3256, 295, 257, 2418, 1032, 322, 257, 2182, 3678, 11, 293, 321, 366, 10875, 257, 3513, 300, 486, 3147, 807, 1901, 13, 50914, 50914, 492, 434, 516, 281, 362, 257, 17619, 17889, 300, 321, 12972, 538, 341, 3513, 13, 51164, 51164, 407, 364, 17889, 307, 3671, 2217, 11, 411, 412, 264, 1411, 917, 295, 264, 7542, 11, 321, 434, 16390, 278, 766, 257, 688, 295, 341, 4985, 8062, 3513, 13, 51514, 51514, 1133, 17889, 307, 4018, 11, 411, 412, 264, 2808, 295, 264, 7542, 11, 321, 434, 11700, 264, 3380, 3256, 490, 264, 28872, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08801330146142992, "compression_ratio": 1.8113207547169812, "no_speech_prob": 3.062577161472291e-05}, {"id": 169, "seek": 114600, "start": 1169.0, "end": 1175.0, "text": " When epsilon is zero, like at the middle of the plot, we're visiting the original image from the dataset.", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 10875, 257, 2541, 1365, 13, 50564, 50564, 467, 311, 364, 3256, 295, 257, 2418, 1032, 322, 257, 2182, 3678, 11, 293, 321, 366, 10875, 257, 3513, 300, 486, 3147, 807, 1901, 13, 50914, 50914, 492, 434, 516, 281, 362, 257, 17619, 17889, 300, 321, 12972, 538, 341, 3513, 13, 51164, 51164, 407, 364, 17889, 307, 3671, 2217, 11, 411, 412, 264, 1411, 917, 295, 264, 7542, 11, 321, 434, 16390, 278, 766, 257, 688, 295, 341, 4985, 8062, 3513, 13, 51514, 51514, 1133, 17889, 307, 4018, 11, 411, 412, 264, 2808, 295, 264, 7542, 11, 321, 434, 11700, 264, 3380, 3256, 490, 264, 28872, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08801330146142992, "compression_ratio": 1.8113207547169812, "no_speech_prob": 3.062577161472291e-05}, {"id": 170, "seek": 117500, "start": 1175.0, "end": 1183.0, "text": " And when epsilon is positive 30, like at the right end of the plot, we're adding this direction onto the input.", "tokens": [50364, 400, 562, 17889, 307, 3353, 2217, 11, 411, 412, 264, 558, 917, 295, 264, 7542, 11, 321, 434, 5127, 341, 3513, 3911, 264, 4846, 13, 50764, 50764, 682, 264, 4831, 322, 264, 1411, 11, 286, 855, 291, 364, 9603, 689, 321, 1286, 490, 17889, 6915, 3671, 2217, 11, 382, 493, 281, 17889, 6915, 3353, 2217, 13, 51214, 51214, 509, 1401, 264, 9603, 1411, 281, 558, 1192, 281, 2767, 13, 51364, 51364, 400, 5315, 300, 456, 311, 257, 5566, 2424, 11, 264, 4846, 307, 8944, 9823, 382, 885, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07280526769922134, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.165635800745804e-06}, {"id": 171, "seek": 117500, "start": 1183.0, "end": 1192.0, "text": " In the panel on the left, I show you an animation where we move from epsilon equals negative 30, as up to epsilon equals positive 30.", "tokens": [50364, 400, 562, 17889, 307, 3353, 2217, 11, 411, 412, 264, 558, 917, 295, 264, 7542, 11, 321, 434, 5127, 341, 3513, 3911, 264, 4846, 13, 50764, 50764, 682, 264, 4831, 322, 264, 1411, 11, 286, 855, 291, 364, 9603, 689, 321, 1286, 490, 17889, 6915, 3671, 2217, 11, 382, 493, 281, 17889, 6915, 3353, 2217, 13, 51214, 51214, 509, 1401, 264, 9603, 1411, 281, 558, 1192, 281, 2767, 13, 51364, 51364, 400, 5315, 300, 456, 311, 257, 5566, 2424, 11, 264, 4846, 307, 8944, 9823, 382, 885, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07280526769922134, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.165635800745804e-06}, {"id": 172, "seek": 117500, "start": 1192.0, "end": 1195.0, "text": " You read the animation left to right top to bottom.", "tokens": [50364, 400, 562, 17889, 307, 3353, 2217, 11, 411, 412, 264, 558, 917, 295, 264, 7542, 11, 321, 434, 5127, 341, 3513, 3911, 264, 4846, 13, 50764, 50764, 682, 264, 4831, 322, 264, 1411, 11, 286, 855, 291, 364, 9603, 689, 321, 1286, 490, 17889, 6915, 3671, 2217, 11, 382, 493, 281, 17889, 6915, 3353, 2217, 13, 51214, 51214, 509, 1401, 264, 9603, 1411, 281, 558, 1192, 281, 2767, 13, 51364, 51364, 400, 5315, 300, 456, 311, 257, 5566, 2424, 11, 264, 4846, 307, 8944, 9823, 382, 885, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07280526769922134, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.165635800745804e-06}, {"id": 173, "seek": 117500, "start": 1195.0, "end": 1202.0, "text": " And everywhere that there's a yellow box, the input is correctly recognized as being a car.", "tokens": [50364, 400, 562, 17889, 307, 3353, 2217, 11, 411, 412, 264, 558, 917, 295, 264, 7542, 11, 321, 434, 5127, 341, 3513, 3911, 264, 4846, 13, 50764, 50764, 682, 264, 4831, 322, 264, 1411, 11, 286, 855, 291, 364, 9603, 689, 321, 1286, 490, 17889, 6915, 3671, 2217, 11, 382, 493, 281, 17889, 6915, 3353, 2217, 13, 51214, 51214, 509, 1401, 264, 9603, 1411, 281, 558, 1192, 281, 2767, 13, 51364, 51364, 400, 5315, 300, 456, 311, 257, 5566, 2424, 11, 264, 4846, 307, 8944, 9823, 382, 885, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07280526769922134, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.165635800745804e-06}, {"id": 174, "seek": 120200, "start": 1202.0, "end": 1208.0, "text": " On the upper left, you see that it looks mostly blue on the upper, on the lower right, it's kind of hard to tell what's going on.", "tokens": [50364, 1282, 264, 6597, 1411, 11, 291, 536, 300, 309, 1542, 5240, 3344, 322, 264, 6597, 11, 322, 264, 3126, 558, 11, 309, 311, 733, 295, 1152, 281, 980, 437, 311, 516, 322, 13, 50664, 50664, 467, 311, 11, 309, 311, 733, 295, 2182, 40974, 293, 370, 322, 13, 50814, 50814, 682, 264, 2808, 5386, 11, 445, 934, 689, 264, 5566, 2424, 307, 293, 291, 393, 536, 1238, 4448, 300, 309, 311, 257, 1032, 322, 257, 2182, 3678, 300, 264, 3256, 307, 733, 295, 1359, 294, 613, 9788, 13, 51314, 51314, 708, 311, 1880, 281, 574, 412, 510, 307, 264, 450, 73, 1208, 300, 264, 2316, 23930, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12990167978647593, "compression_ratio": 1.7226890756302522, "no_speech_prob": 2.9738318971794797e-06}, {"id": 175, "seek": 120200, "start": 1208.0, "end": 1211.0, "text": " It's, it's kind of reddish and so on.", "tokens": [50364, 1282, 264, 6597, 1411, 11, 291, 536, 300, 309, 1542, 5240, 3344, 322, 264, 6597, 11, 322, 264, 3126, 558, 11, 309, 311, 733, 295, 1152, 281, 980, 437, 311, 516, 322, 13, 50664, 50664, 467, 311, 11, 309, 311, 733, 295, 2182, 40974, 293, 370, 322, 13, 50814, 50814, 682, 264, 2808, 5386, 11, 445, 934, 689, 264, 5566, 2424, 307, 293, 291, 393, 536, 1238, 4448, 300, 309, 311, 257, 1032, 322, 257, 2182, 3678, 300, 264, 3256, 307, 733, 295, 1359, 294, 613, 9788, 13, 51314, 51314, 708, 311, 1880, 281, 574, 412, 510, 307, 264, 450, 73, 1208, 300, 264, 2316, 23930, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12990167978647593, "compression_ratio": 1.7226890756302522, "no_speech_prob": 2.9738318971794797e-06}, {"id": 176, "seek": 120200, "start": 1211.0, "end": 1221.0, "text": " In the middle row, just after where the yellow box is and you can see pretty clearly that it's a car on a red background that the image is kind of small in these slides.", "tokens": [50364, 1282, 264, 6597, 1411, 11, 291, 536, 300, 309, 1542, 5240, 3344, 322, 264, 6597, 11, 322, 264, 3126, 558, 11, 309, 311, 733, 295, 1152, 281, 980, 437, 311, 516, 322, 13, 50664, 50664, 467, 311, 11, 309, 311, 733, 295, 2182, 40974, 293, 370, 322, 13, 50814, 50814, 682, 264, 2808, 5386, 11, 445, 934, 689, 264, 5566, 2424, 307, 293, 291, 393, 536, 1238, 4448, 300, 309, 311, 257, 1032, 322, 257, 2182, 3678, 300, 264, 3256, 307, 733, 295, 1359, 294, 613, 9788, 13, 51314, 51314, 708, 311, 1880, 281, 574, 412, 510, 307, 264, 450, 73, 1208, 300, 264, 2316, 23930, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12990167978647593, "compression_ratio": 1.7226890756302522, "no_speech_prob": 2.9738318971794797e-06}, {"id": 177, "seek": 120200, "start": 1221.0, "end": 1226.0, "text": " What's interesting to look at here is the lojits that the model outputs.", "tokens": [50364, 1282, 264, 6597, 1411, 11, 291, 536, 300, 309, 1542, 5240, 3344, 322, 264, 6597, 11, 322, 264, 3126, 558, 11, 309, 311, 733, 295, 1152, 281, 980, 437, 311, 516, 322, 13, 50664, 50664, 467, 311, 11, 309, 311, 733, 295, 2182, 40974, 293, 370, 322, 13, 50814, 50814, 682, 264, 2808, 5386, 11, 445, 934, 689, 264, 5566, 2424, 307, 293, 291, 393, 536, 1238, 4448, 300, 309, 311, 257, 1032, 322, 257, 2182, 3678, 300, 264, 3256, 307, 733, 295, 1359, 294, 613, 9788, 13, 51314, 51314, 708, 311, 1880, 281, 574, 412, 510, 307, 264, 450, 73, 1208, 300, 264, 2316, 23930, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12990167978647593, "compression_ratio": 1.7226890756302522, "no_speech_prob": 2.9738318971794797e-06}, {"id": 178, "seek": 122600, "start": 1226.0, "end": 1233.0, "text": " This is a deep convolutional rectified linear unit that work because it uses rectified linear units.", "tokens": [50364, 639, 307, 257, 2452, 45216, 304, 11048, 2587, 8213, 4985, 300, 589, 570, 309, 4960, 11048, 2587, 8213, 6815, 13, 50714, 50714, 492, 458, 300, 264, 5598, 307, 257, 2522, 3711, 8213, 2445, 295, 264, 4846, 281, 264, 2316, 13, 51014, 51014, 440, 2135, 1168, 321, 434, 3365, 538, 1455, 341, 7542, 307, 577, 867, 819, 3755, 775, 341, 2522, 3711, 8213, 2445, 362, 498, 321, 574, 412, 472, 1729, 3278, 3541, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08502375615107549, "compression_ratio": 1.7614213197969544, "no_speech_prob": 1.3187407603254542e-05}, {"id": 179, "seek": 122600, "start": 1233.0, "end": 1239.0, "text": " We know that the output is a piecewise linear function of the input to the model.", "tokens": [50364, 639, 307, 257, 2452, 45216, 304, 11048, 2587, 8213, 4985, 300, 589, 570, 309, 4960, 11048, 2587, 8213, 6815, 13, 50714, 50714, 492, 458, 300, 264, 5598, 307, 257, 2522, 3711, 8213, 2445, 295, 264, 4846, 281, 264, 2316, 13, 51014, 51014, 440, 2135, 1168, 321, 434, 3365, 538, 1455, 341, 7542, 307, 577, 867, 819, 3755, 775, 341, 2522, 3711, 8213, 2445, 362, 498, 321, 574, 412, 472, 1729, 3278, 3541, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08502375615107549, "compression_ratio": 1.7614213197969544, "no_speech_prob": 1.3187407603254542e-05}, {"id": 180, "seek": 122600, "start": 1239.0, "end": 1249.0, "text": " The main question we're asking by making this plot is how many different pieces does this piecewise linear function have if we look at one particular cross section.", "tokens": [50364, 639, 307, 257, 2452, 45216, 304, 11048, 2587, 8213, 4985, 300, 589, 570, 309, 4960, 11048, 2587, 8213, 6815, 13, 50714, 50714, 492, 458, 300, 264, 5598, 307, 257, 2522, 3711, 8213, 2445, 295, 264, 4846, 281, 264, 2316, 13, 51014, 51014, 440, 2135, 1168, 321, 434, 3365, 538, 1455, 341, 7542, 307, 577, 867, 819, 3755, 775, 341, 2522, 3711, 8213, 2445, 362, 498, 321, 574, 412, 472, 1729, 3278, 3541, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08502375615107549, "compression_ratio": 1.7614213197969544, "no_speech_prob": 1.3187407603254542e-05}, {"id": 181, "seek": 124900, "start": 1249.0, "end": 1259.0, "text": " You might think that maybe a deep net is going to represent some extremely wiggly, complicated function with lots and lots of linear pieces no matter which cross section you look in.", "tokens": [50364, 509, 1062, 519, 300, 1310, 257, 2452, 2533, 307, 516, 281, 2906, 512, 4664, 261, 46737, 11, 6179, 2445, 365, 3195, 293, 3195, 295, 8213, 3755, 572, 1871, 597, 3278, 3541, 291, 574, 294, 13, 50864, 50864, 1610, 321, 1062, 915, 300, 309, 575, 544, 420, 1570, 732, 3755, 337, 1184, 2445, 321, 574, 412, 13, 51164, 51164, 6947, 295, 264, 819, 19490, 322, 341, 7542, 307, 264, 450, 73, 1208, 337, 257, 819, 1508, 13, 51464, 51464, 492, 536, 300, 484, 412, 264, 28537, 295, 264, 7542, 300, 264, 17259, 1508, 307, 264, 881, 3700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07062953061396533, "compression_ratio": 1.7137096774193548, "no_speech_prob": 6.90056913299486e-06}, {"id": 182, "seek": 124900, "start": 1259.0, "end": 1265.0, "text": " Or we might find that it has more or less two pieces for each function we look at.", "tokens": [50364, 509, 1062, 519, 300, 1310, 257, 2452, 2533, 307, 516, 281, 2906, 512, 4664, 261, 46737, 11, 6179, 2445, 365, 3195, 293, 3195, 295, 8213, 3755, 572, 1871, 597, 3278, 3541, 291, 574, 294, 13, 50864, 50864, 1610, 321, 1062, 915, 300, 309, 575, 544, 420, 1570, 732, 3755, 337, 1184, 2445, 321, 574, 412, 13, 51164, 51164, 6947, 295, 264, 819, 19490, 322, 341, 7542, 307, 264, 450, 73, 1208, 337, 257, 819, 1508, 13, 51464, 51464, 492, 536, 300, 484, 412, 264, 28537, 295, 264, 7542, 300, 264, 17259, 1508, 307, 264, 881, 3700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07062953061396533, "compression_ratio": 1.7137096774193548, "no_speech_prob": 6.90056913299486e-06}, {"id": 183, "seek": 124900, "start": 1265.0, "end": 1271.0, "text": " Each of the different curves on this plot is the lojits for a different class.", "tokens": [50364, 509, 1062, 519, 300, 1310, 257, 2452, 2533, 307, 516, 281, 2906, 512, 4664, 261, 46737, 11, 6179, 2445, 365, 3195, 293, 3195, 295, 8213, 3755, 572, 1871, 597, 3278, 3541, 291, 574, 294, 13, 50864, 50864, 1610, 321, 1062, 915, 300, 309, 575, 544, 420, 1570, 732, 3755, 337, 1184, 2445, 321, 574, 412, 13, 51164, 51164, 6947, 295, 264, 819, 19490, 322, 341, 7542, 307, 264, 450, 73, 1208, 337, 257, 819, 1508, 13, 51464, 51464, 492, 536, 300, 484, 412, 264, 28537, 295, 264, 7542, 300, 264, 17259, 1508, 307, 264, 881, 3700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07062953061396533, "compression_ratio": 1.7137096774193548, "no_speech_prob": 6.90056913299486e-06}, {"id": 184, "seek": 124900, "start": 1271.0, "end": 1277.0, "text": " We see that out at the tails of the plot that the frog class is the most likely.", "tokens": [50364, 509, 1062, 519, 300, 1310, 257, 2452, 2533, 307, 516, 281, 2906, 512, 4664, 261, 46737, 11, 6179, 2445, 365, 3195, 293, 3195, 295, 8213, 3755, 572, 1871, 597, 3278, 3541, 291, 574, 294, 13, 50864, 50864, 1610, 321, 1062, 915, 300, 309, 575, 544, 420, 1570, 732, 3755, 337, 1184, 2445, 321, 574, 412, 13, 51164, 51164, 6947, 295, 264, 819, 19490, 322, 341, 7542, 307, 264, 450, 73, 1208, 337, 257, 819, 1508, 13, 51464, 51464, 492, 536, 300, 484, 412, 264, 28537, 295, 264, 7542, 300, 264, 17259, 1508, 307, 264, 881, 3700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07062953061396533, "compression_ratio": 1.7137096774193548, "no_speech_prob": 6.90056913299486e-06}, {"id": 185, "seek": 127700, "start": 1277.0, "end": 1282.0, "text": " The frog class basically looks like a big v-shaped function.", "tokens": [50364, 440, 17259, 1508, 1936, 1542, 411, 257, 955, 371, 12, 23103, 2445, 13, 50614, 50614, 440, 450, 73, 1208, 337, 264, 17259, 1508, 1813, 588, 1090, 562, 17889, 307, 3671, 2217, 420, 3353, 2217, 13, 50914, 50914, 814, 3270, 760, 293, 1813, 257, 707, 857, 3671, 562, 17889, 307, 1958, 13, 51164, 51164, 440, 1032, 1508, 10052, 382, 38809, 510, 11, 309, 311, 767, 1090, 294, 264, 2808, 293, 264, 1032, 307, 8944, 9823, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12451402446891688, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.665776214096695e-05}, {"id": 186, "seek": 127700, "start": 1282.0, "end": 1288.0, "text": " The lojits for the frog class become very high when epsilon is negative 30 or positive 30.", "tokens": [50364, 440, 17259, 1508, 1936, 1542, 411, 257, 955, 371, 12, 23103, 2445, 13, 50614, 50614, 440, 450, 73, 1208, 337, 264, 17259, 1508, 1813, 588, 1090, 562, 17889, 307, 3671, 2217, 420, 3353, 2217, 13, 50914, 50914, 814, 3270, 760, 293, 1813, 257, 707, 857, 3671, 562, 17889, 307, 1958, 13, 51164, 51164, 440, 1032, 1508, 10052, 382, 38809, 510, 11, 309, 311, 767, 1090, 294, 264, 2808, 293, 264, 1032, 307, 8944, 9823, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12451402446891688, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.665776214096695e-05}, {"id": 187, "seek": 127700, "start": 1288.0, "end": 1293.0, "text": " They drop down and become a little bit negative when epsilon is 0.", "tokens": [50364, 440, 17259, 1508, 1936, 1542, 411, 257, 955, 371, 12, 23103, 2445, 13, 50614, 50614, 440, 450, 73, 1208, 337, 264, 17259, 1508, 1813, 588, 1090, 562, 17889, 307, 3671, 2217, 420, 3353, 2217, 13, 50914, 50914, 814, 3270, 760, 293, 1813, 257, 707, 857, 3671, 562, 17889, 307, 1958, 13, 51164, 51164, 440, 1032, 1508, 10052, 382, 38809, 510, 11, 309, 311, 767, 1090, 294, 264, 2808, 293, 264, 1032, 307, 8944, 9823, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12451402446891688, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.665776214096695e-05}, {"id": 188, "seek": 127700, "start": 1293.0, "end": 1303.0, "text": " The car class listed as automobile here, it's actually high in the middle and the car is correctly recognized.", "tokens": [50364, 440, 17259, 1508, 1936, 1542, 411, 257, 955, 371, 12, 23103, 2445, 13, 50614, 50614, 440, 450, 73, 1208, 337, 264, 17259, 1508, 1813, 588, 1090, 562, 17889, 307, 3671, 2217, 420, 3353, 2217, 13, 50914, 50914, 814, 3270, 760, 293, 1813, 257, 707, 857, 3671, 562, 17889, 307, 1958, 13, 51164, 51164, 440, 1032, 1508, 10052, 382, 38809, 510, 11, 309, 311, 767, 1090, 294, 264, 2808, 293, 264, 1032, 307, 8944, 9823, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12451402446891688, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.665776214096695e-05}, {"id": 189, "seek": 130300, "start": 1303.0, "end": 1312.0, "text": " As we sweep out to very negative epsilon, the lojits for the car class do increase, but they don't increase nearly as quickly as the lojits for the frog class.", "tokens": [50364, 1018, 321, 22169, 484, 281, 588, 3671, 17889, 11, 264, 450, 73, 1208, 337, 264, 1032, 1508, 360, 3488, 11, 457, 436, 500, 380, 3488, 6217, 382, 2661, 382, 264, 450, 73, 1208, 337, 264, 17259, 1508, 13, 50814, 50814, 492, 1352, 257, 3513, 300, 311, 6615, 365, 264, 17259, 1508, 293, 382, 321, 1524, 309, 484, 281, 257, 7226, 2416, 40468, 399, 13, 51214, 51214, 492, 915, 300, 264, 2316, 48224, 1024, 43586, 293, 7338, 281, 652, 257, 588, 41730, 17630, 300, 264, 17259, 1508, 307, 4664, 3700, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08975392199577169, "compression_ratio": 1.7922077922077921, "no_speech_prob": 2.0860894437646493e-05}, {"id": 190, "seek": 130300, "start": 1312.0, "end": 1320.0, "text": " We found a direction that's associated with the frog class and as we follow it out to a relatively large perturbation.", "tokens": [50364, 1018, 321, 22169, 484, 281, 588, 3671, 17889, 11, 264, 450, 73, 1208, 337, 264, 1032, 1508, 360, 3488, 11, 457, 436, 500, 380, 3488, 6217, 382, 2661, 382, 264, 450, 73, 1208, 337, 264, 17259, 1508, 13, 50814, 50814, 492, 1352, 257, 3513, 300, 311, 6615, 365, 264, 17259, 1508, 293, 382, 321, 1524, 309, 484, 281, 257, 7226, 2416, 40468, 399, 13, 51214, 51214, 492, 915, 300, 264, 2316, 48224, 1024, 43586, 293, 7338, 281, 652, 257, 588, 41730, 17630, 300, 264, 17259, 1508, 307, 4664, 3700, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08975392199577169, "compression_ratio": 1.7922077922077921, "no_speech_prob": 2.0860894437646493e-05}, {"id": 191, "seek": 130300, "start": 1320.0, "end": 1328.0, "text": " We find that the model extrapolates linearly and begins to make a very unreasonable prediction that the frog class is extremely likely.", "tokens": [50364, 1018, 321, 22169, 484, 281, 588, 3671, 17889, 11, 264, 450, 73, 1208, 337, 264, 1032, 1508, 360, 3488, 11, 457, 436, 500, 380, 3488, 6217, 382, 2661, 382, 264, 450, 73, 1208, 337, 264, 17259, 1508, 13, 50814, 50814, 492, 1352, 257, 3513, 300, 311, 6615, 365, 264, 17259, 1508, 293, 382, 321, 1524, 309, 484, 281, 257, 7226, 2416, 40468, 399, 13, 51214, 51214, 492, 915, 300, 264, 2316, 48224, 1024, 43586, 293, 7338, 281, 652, 257, 588, 41730, 17630, 300, 264, 17259, 1508, 307, 4664, 3700, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08975392199577169, "compression_ratio": 1.7922077922077921, "no_speech_prob": 2.0860894437646493e-05}, {"id": 192, "seek": 132800, "start": 1328.0, "end": 1337.0, "text": " Just because we've moved for a long time in this direction that was locally associated with the frog class being more likely.", "tokens": [50364, 1449, 570, 321, 600, 4259, 337, 257, 938, 565, 294, 341, 3513, 300, 390, 16143, 6615, 365, 264, 17259, 1508, 885, 544, 3700, 13, 50814, 50814, 1133, 321, 767, 352, 293, 7690, 17641, 44745, 5110, 11, 321, 643, 281, 1604, 300, 321, 434, 1075, 281, 483, 1596, 257, 2416, 40468, 399, 1553, 4473, 264, 3256, 588, 709, 382, 1400, 382, 257, 1952, 885, 307, 5922, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08085265840802874, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.022462108987384e-06}, {"id": 193, "seek": 132800, "start": 1337.0, "end": 1351.0, "text": " When we actually go and construct adversarial examples, we need to remember that we're able to get quite a large perturbation without changing the image very much as far as a human being is concerned.", "tokens": [50364, 1449, 570, 321, 600, 4259, 337, 257, 938, 565, 294, 341, 3513, 300, 390, 16143, 6615, 365, 264, 17259, 1508, 885, 544, 3700, 13, 50814, 50814, 1133, 321, 767, 352, 293, 7690, 17641, 44745, 5110, 11, 321, 643, 281, 1604, 300, 321, 434, 1075, 281, 483, 1596, 257, 2416, 40468, 399, 1553, 4473, 264, 3256, 588, 709, 382, 1400, 382, 257, 1952, 885, 307, 5922, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08085265840802874, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.022462108987384e-06}, {"id": 194, "seek": 135100, "start": 1351.0, "end": 1361.0, "text": " Here I show you a hand-written digit 3 and I'm going to change it in several different ways. All of these changes have the same L2 norm perturbation.", "tokens": [50364, 1692, 286, 855, 291, 257, 1011, 12, 26859, 14293, 805, 293, 286, 478, 516, 281, 1319, 309, 294, 2940, 819, 2098, 13, 1057, 295, 613, 2962, 362, 264, 912, 441, 17, 2026, 40468, 399, 13, 50864, 50864, 682, 264, 1192, 5386, 11, 286, 478, 516, 281, 1319, 264, 805, 666, 257, 1614, 445, 538, 1237, 337, 264, 23831, 1614, 294, 264, 3097, 992, 13, 51214, 51214, 440, 2649, 1296, 729, 732, 307, 341, 3256, 300, 1542, 257, 707, 857, 411, 264, 1614, 14226, 294, 512, 2211, 3876, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11229890325795049, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7481095710536465e-05}, {"id": 195, "seek": 135100, "start": 1361.0, "end": 1368.0, "text": " In the top row, I'm going to change the 3 into a 7 just by looking for the nearest 7 in the training set.", "tokens": [50364, 1692, 286, 855, 291, 257, 1011, 12, 26859, 14293, 805, 293, 286, 478, 516, 281, 1319, 309, 294, 2940, 819, 2098, 13, 1057, 295, 613, 2962, 362, 264, 912, 441, 17, 2026, 40468, 399, 13, 50864, 50864, 682, 264, 1192, 5386, 11, 286, 478, 516, 281, 1319, 264, 805, 666, 257, 1614, 445, 538, 1237, 337, 264, 23831, 1614, 294, 264, 3097, 992, 13, 51214, 51214, 440, 2649, 1296, 729, 732, 307, 341, 3256, 300, 1542, 257, 707, 857, 411, 264, 1614, 14226, 294, 512, 2211, 3876, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11229890325795049, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7481095710536465e-05}, {"id": 196, "seek": 135100, "start": 1368.0, "end": 1376.0, "text": " The difference between those two is this image that looks a little bit like the 7 wrapped in some black lines.", "tokens": [50364, 1692, 286, 855, 291, 257, 1011, 12, 26859, 14293, 805, 293, 286, 478, 516, 281, 1319, 309, 294, 2940, 819, 2098, 13, 1057, 295, 613, 2962, 362, 264, 912, 441, 17, 2026, 40468, 399, 13, 50864, 50864, 682, 264, 1192, 5386, 11, 286, 478, 516, 281, 1319, 264, 805, 666, 257, 1614, 445, 538, 1237, 337, 264, 23831, 1614, 294, 264, 3097, 992, 13, 51214, 51214, 440, 2649, 1296, 729, 732, 307, 341, 3256, 300, 1542, 257, 707, 857, 411, 264, 1614, 14226, 294, 512, 2211, 3876, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11229890325795049, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7481095710536465e-05}, {"id": 197, "seek": 137600, "start": 1376.0, "end": 1389.0, "text": " Here white pixels in the middle image in the perturbation column, the white pixels represent adding something and black pixels represent subtracting something as you move from the left column to the right column.", "tokens": [50364, 1692, 2418, 18668, 294, 264, 2808, 3256, 294, 264, 40468, 399, 7738, 11, 264, 2418, 18668, 2906, 5127, 746, 293, 2211, 18668, 2906, 16390, 278, 746, 382, 291, 1286, 490, 264, 1411, 7738, 281, 264, 558, 7738, 13, 51014, 51014, 1133, 321, 747, 264, 805, 293, 321, 3079, 341, 40468, 399, 300, 35592, 309, 666, 257, 1614, 11, 321, 393, 3481, 264, 441, 17, 2026, 295, 300, 40468, 399, 13, 51414, 51414, 467, 4523, 484, 281, 362, 364, 441, 17, 2026, 295, 805, 13, 22962, 13, 51614, 51614, 663, 2709, 291, 257, 733, 295, 6408, 337, 577, 955, 613, 40468, 763, 393, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09110088701601382, "compression_ratio": 1.8167330677290836, "no_speech_prob": 6.496645255538169e-06}, {"id": 198, "seek": 137600, "start": 1389.0, "end": 1397.0, "text": " When we take the 3 and we apply this perturbation that transforms it into a 7, we can measure the L2 norm of that perturbation.", "tokens": [50364, 1692, 2418, 18668, 294, 264, 2808, 3256, 294, 264, 40468, 399, 7738, 11, 264, 2418, 18668, 2906, 5127, 746, 293, 2211, 18668, 2906, 16390, 278, 746, 382, 291, 1286, 490, 264, 1411, 7738, 281, 264, 558, 7738, 13, 51014, 51014, 1133, 321, 747, 264, 805, 293, 321, 3079, 341, 40468, 399, 300, 35592, 309, 666, 257, 1614, 11, 321, 393, 3481, 264, 441, 17, 2026, 295, 300, 40468, 399, 13, 51414, 51414, 467, 4523, 484, 281, 362, 364, 441, 17, 2026, 295, 805, 13, 22962, 13, 51614, 51614, 663, 2709, 291, 257, 733, 295, 6408, 337, 577, 955, 613, 40468, 763, 393, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09110088701601382, "compression_ratio": 1.8167330677290836, "no_speech_prob": 6.496645255538169e-06}, {"id": 199, "seek": 137600, "start": 1397.0, "end": 1401.0, "text": " It turns out to have an L2 norm of 3.96.", "tokens": [50364, 1692, 2418, 18668, 294, 264, 2808, 3256, 294, 264, 40468, 399, 7738, 11, 264, 2418, 18668, 2906, 5127, 746, 293, 2211, 18668, 2906, 16390, 278, 746, 382, 291, 1286, 490, 264, 1411, 7738, 281, 264, 558, 7738, 13, 51014, 51014, 1133, 321, 747, 264, 805, 293, 321, 3079, 341, 40468, 399, 300, 35592, 309, 666, 257, 1614, 11, 321, 393, 3481, 264, 441, 17, 2026, 295, 300, 40468, 399, 13, 51414, 51414, 467, 4523, 484, 281, 362, 364, 441, 17, 2026, 295, 805, 13, 22962, 13, 51614, 51614, 663, 2709, 291, 257, 733, 295, 6408, 337, 577, 955, 613, 40468, 763, 393, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09110088701601382, "compression_ratio": 1.8167330677290836, "no_speech_prob": 6.496645255538169e-06}, {"id": 200, "seek": 137600, "start": 1401.0, "end": 1405.0, "text": " That gives you a kind of reference for how big these perturbations can be.", "tokens": [50364, 1692, 2418, 18668, 294, 264, 2808, 3256, 294, 264, 40468, 399, 7738, 11, 264, 2418, 18668, 2906, 5127, 746, 293, 2211, 18668, 2906, 16390, 278, 746, 382, 291, 1286, 490, 264, 1411, 7738, 281, 264, 558, 7738, 13, 51014, 51014, 1133, 321, 747, 264, 805, 293, 321, 3079, 341, 40468, 399, 300, 35592, 309, 666, 257, 1614, 11, 321, 393, 3481, 264, 441, 17, 2026, 295, 300, 40468, 399, 13, 51414, 51414, 467, 4523, 484, 281, 362, 364, 441, 17, 2026, 295, 805, 13, 22962, 13, 51614, 51614, 663, 2709, 291, 257, 733, 295, 6408, 337, 577, 955, 613, 40468, 763, 393, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09110088701601382, "compression_ratio": 1.8167330677290836, "no_speech_prob": 6.496645255538169e-06}, {"id": 201, "seek": 140500, "start": 1405.0, "end": 1411.0, "text": " In the middle row, we apply a perturbation of exactly the same size, but with the direction chosen randomly.", "tokens": [50364, 682, 264, 2808, 5386, 11, 321, 3079, 257, 40468, 399, 295, 2293, 264, 912, 2744, 11, 457, 365, 264, 3513, 8614, 16979, 13, 50664, 50664, 682, 341, 1389, 11, 321, 500, 380, 767, 1319, 264, 1508, 295, 264, 805, 412, 439, 13, 492, 445, 483, 512, 4974, 5658, 300, 994, 380, 534, 1319, 264, 1508, 13, 51014, 51014, 316, 1952, 393, 920, 3612, 1401, 309, 382, 885, 257, 805, 13, 51214, 51214, 400, 550, 2721, 11, 412, 264, 588, 2767, 5386, 11, 321, 747, 264, 805, 293, 321, 445, 23525, 257, 2522, 295, 309, 365, 257, 40468, 399, 295, 264, 912, 2026, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07933267254695714, "compression_ratio": 1.7107438016528926, "no_speech_prob": 1.961644011316821e-05}, {"id": 202, "seek": 140500, "start": 1411.0, "end": 1418.0, "text": " In this case, we don't actually change the class of the 3 at all. We just get some random noise that didn't really change the class.", "tokens": [50364, 682, 264, 2808, 5386, 11, 321, 3079, 257, 40468, 399, 295, 2293, 264, 912, 2744, 11, 457, 365, 264, 3513, 8614, 16979, 13, 50664, 50664, 682, 341, 1389, 11, 321, 500, 380, 767, 1319, 264, 1508, 295, 264, 805, 412, 439, 13, 492, 445, 483, 512, 4974, 5658, 300, 994, 380, 534, 1319, 264, 1508, 13, 51014, 51014, 316, 1952, 393, 920, 3612, 1401, 309, 382, 885, 257, 805, 13, 51214, 51214, 400, 550, 2721, 11, 412, 264, 588, 2767, 5386, 11, 321, 747, 264, 805, 293, 321, 445, 23525, 257, 2522, 295, 309, 365, 257, 40468, 399, 295, 264, 912, 2026, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07933267254695714, "compression_ratio": 1.7107438016528926, "no_speech_prob": 1.961644011316821e-05}, {"id": 203, "seek": 140500, "start": 1418.0, "end": 1422.0, "text": " A human can still easily read it as being a 3.", "tokens": [50364, 682, 264, 2808, 5386, 11, 321, 3079, 257, 40468, 399, 295, 2293, 264, 912, 2744, 11, 457, 365, 264, 3513, 8614, 16979, 13, 50664, 50664, 682, 341, 1389, 11, 321, 500, 380, 767, 1319, 264, 1508, 295, 264, 805, 412, 439, 13, 492, 445, 483, 512, 4974, 5658, 300, 994, 380, 534, 1319, 264, 1508, 13, 51014, 51014, 316, 1952, 393, 920, 3612, 1401, 309, 382, 885, 257, 805, 13, 51214, 51214, 400, 550, 2721, 11, 412, 264, 588, 2767, 5386, 11, 321, 747, 264, 805, 293, 321, 445, 23525, 257, 2522, 295, 309, 365, 257, 40468, 399, 295, 264, 912, 2026, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07933267254695714, "compression_ratio": 1.7107438016528926, "no_speech_prob": 1.961644011316821e-05}, {"id": 204, "seek": 140500, "start": 1422.0, "end": 1429.0, "text": " And then finally, at the very bottom row, we take the 3 and we just erase a piece of it with a perturbation of the same norm.", "tokens": [50364, 682, 264, 2808, 5386, 11, 321, 3079, 257, 40468, 399, 295, 2293, 264, 912, 2744, 11, 457, 365, 264, 3513, 8614, 16979, 13, 50664, 50664, 682, 341, 1389, 11, 321, 500, 380, 767, 1319, 264, 1508, 295, 264, 805, 412, 439, 13, 492, 445, 483, 512, 4974, 5658, 300, 994, 380, 534, 1319, 264, 1508, 13, 51014, 51014, 316, 1952, 393, 920, 3612, 1401, 309, 382, 885, 257, 805, 13, 51214, 51214, 400, 550, 2721, 11, 412, 264, 588, 2767, 5386, 11, 321, 747, 264, 805, 293, 321, 445, 23525, 257, 2522, 295, 309, 365, 257, 40468, 399, 295, 264, 912, 2026, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07933267254695714, "compression_ratio": 1.7107438016528926, "no_speech_prob": 1.961644011316821e-05}, {"id": 205, "seek": 142900, "start": 1429.0, "end": 1436.0, "text": " And we turn it into something that doesn't have any class at all. It's not a 3. It's not a 7. It's just a defective input.", "tokens": [50364, 400, 321, 1261, 309, 666, 746, 300, 1177, 380, 362, 604, 1508, 412, 439, 13, 467, 311, 406, 257, 805, 13, 467, 311, 406, 257, 1614, 13, 467, 311, 445, 257, 16445, 488, 4846, 13, 50714, 50714, 1057, 295, 613, 2962, 393, 1051, 365, 264, 912, 441, 17, 2026, 40468, 399, 13, 50964, 50964, 400, 767, 11, 257, 688, 295, 264, 565, 365, 17641, 44745, 5110, 11, 291, 652, 40468, 763, 300, 362, 754, 4833, 441, 17, 2026, 13, 51264, 51264, 708, 311, 516, 322, 307, 300, 456, 366, 2940, 819, 18668, 322, 264, 3256, 11, 293, 370, 1359, 2962, 281, 2609, 18668, 393, 909, 493, 281, 7226, 2416, 18875, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06322022313657014, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.5084513506735675e-05}, {"id": 206, "seek": 142900, "start": 1436.0, "end": 1441.0, "text": " All of these changes can happen with the same L2 norm perturbation.", "tokens": [50364, 400, 321, 1261, 309, 666, 746, 300, 1177, 380, 362, 604, 1508, 412, 439, 13, 467, 311, 406, 257, 805, 13, 467, 311, 406, 257, 1614, 13, 467, 311, 445, 257, 16445, 488, 4846, 13, 50714, 50714, 1057, 295, 613, 2962, 393, 1051, 365, 264, 912, 441, 17, 2026, 40468, 399, 13, 50964, 50964, 400, 767, 11, 257, 688, 295, 264, 565, 365, 17641, 44745, 5110, 11, 291, 652, 40468, 763, 300, 362, 754, 4833, 441, 17, 2026, 13, 51264, 51264, 708, 311, 516, 322, 307, 300, 456, 366, 2940, 819, 18668, 322, 264, 3256, 11, 293, 370, 1359, 2962, 281, 2609, 18668, 393, 909, 493, 281, 7226, 2416, 18875, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06322022313657014, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.5084513506735675e-05}, {"id": 207, "seek": 142900, "start": 1441.0, "end": 1447.0, "text": " And actually, a lot of the time with adversarial examples, you make perturbations that have even larger L2 norm.", "tokens": [50364, 400, 321, 1261, 309, 666, 746, 300, 1177, 380, 362, 604, 1508, 412, 439, 13, 467, 311, 406, 257, 805, 13, 467, 311, 406, 257, 1614, 13, 467, 311, 445, 257, 16445, 488, 4846, 13, 50714, 50714, 1057, 295, 613, 2962, 393, 1051, 365, 264, 912, 441, 17, 2026, 40468, 399, 13, 50964, 50964, 400, 767, 11, 257, 688, 295, 264, 565, 365, 17641, 44745, 5110, 11, 291, 652, 40468, 763, 300, 362, 754, 4833, 441, 17, 2026, 13, 51264, 51264, 708, 311, 516, 322, 307, 300, 456, 366, 2940, 819, 18668, 322, 264, 3256, 11, 293, 370, 1359, 2962, 281, 2609, 18668, 393, 909, 493, 281, 7226, 2416, 18875, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06322022313657014, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.5084513506735675e-05}, {"id": 208, "seek": 142900, "start": 1447.0, "end": 1456.0, "text": " What's going on is that there are several different pixels on the image, and so small changes to individual pixels can add up to relatively large vectors.", "tokens": [50364, 400, 321, 1261, 309, 666, 746, 300, 1177, 380, 362, 604, 1508, 412, 439, 13, 467, 311, 406, 257, 805, 13, 467, 311, 406, 257, 1614, 13, 467, 311, 445, 257, 16445, 488, 4846, 13, 50714, 50714, 1057, 295, 613, 2962, 393, 1051, 365, 264, 912, 441, 17, 2026, 40468, 399, 13, 50964, 50964, 400, 767, 11, 257, 688, 295, 264, 565, 365, 17641, 44745, 5110, 11, 291, 652, 40468, 763, 300, 362, 754, 4833, 441, 17, 2026, 13, 51264, 51264, 708, 311, 516, 322, 307, 300, 456, 366, 2940, 819, 18668, 322, 264, 3256, 11, 293, 370, 1359, 2962, 281, 2609, 18668, 393, 909, 493, 281, 7226, 2416, 18875, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06322022313657014, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.5084513506735675e-05}, {"id": 209, "seek": 145600, "start": 1456.0, "end": 1467.0, "text": " For larger datasets, like image net, where there's even more pixels, you can make very small changes to each pixel, but travel very far in vector space, as measured by the L2 norm.", "tokens": [50364, 1171, 4833, 42856, 11, 411, 3256, 2533, 11, 689, 456, 311, 754, 544, 18668, 11, 291, 393, 652, 588, 1359, 2962, 281, 1184, 19261, 11, 457, 3147, 588, 1400, 294, 8062, 1901, 11, 382, 12690, 538, 264, 441, 17, 2026, 13, 50914, 50914, 663, 1355, 300, 291, 393, 767, 652, 2962, 300, 366, 1920, 10100, 1336, 964, 11, 457, 767, 1286, 291, 534, 1400, 293, 483, 257, 2416, 5893, 1674, 365, 264, 31994, 295, 264, 8213, 2445, 300, 264, 2316, 8855, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06708898655203886, "compression_ratio": 1.6317991631799162, "no_speech_prob": 5.09138590132352e-06}, {"id": 210, "seek": 145600, "start": 1467.0, "end": 1479.0, "text": " That means that you can actually make changes that are almost imperceptible, but actually move you really far and get a large dot product with the coefficients of the linear function that the model represents.", "tokens": [50364, 1171, 4833, 42856, 11, 411, 3256, 2533, 11, 689, 456, 311, 754, 544, 18668, 11, 291, 393, 652, 588, 1359, 2962, 281, 1184, 19261, 11, 457, 3147, 588, 1400, 294, 8062, 1901, 11, 382, 12690, 538, 264, 441, 17, 2026, 13, 50914, 50914, 663, 1355, 300, 291, 393, 767, 652, 2962, 300, 366, 1920, 10100, 1336, 964, 11, 457, 767, 1286, 291, 534, 1400, 293, 483, 257, 2416, 5893, 1674, 365, 264, 31994, 295, 264, 8213, 2445, 300, 264, 2316, 8855, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06708898655203886, "compression_ratio": 1.6317991631799162, "no_speech_prob": 5.09138590132352e-06}, {"id": 211, "seek": 147900, "start": 1479.0, "end": 1489.0, "text": " It also means that when we're constructing adversarial examples, we need to make sure that the adversarial example procedure isn't able to do what happened in the top row of this slide here.", "tokens": [50364, 467, 611, 1355, 300, 562, 321, 434, 39969, 17641, 44745, 5110, 11, 321, 643, 281, 652, 988, 300, 264, 17641, 44745, 1365, 10747, 1943, 380, 1075, 281, 360, 437, 2011, 294, 264, 1192, 5386, 295, 341, 4137, 510, 13, 50864, 50864, 407, 264, 1192, 5386, 295, 341, 4137, 11, 321, 1890, 257, 805, 293, 321, 767, 445, 3105, 309, 666, 257, 1614, 13, 51064, 51064, 407, 562, 264, 2316, 1619, 300, 264, 3256, 294, 264, 6597, 558, 307, 257, 1614, 11, 309, 311, 406, 257, 6146, 13, 492, 767, 445, 1319, 264, 4846, 1508, 13, 51364, 51364, 1133, 321, 1322, 17641, 44745, 5110, 11, 321, 528, 281, 652, 988, 300, 321, 434, 13389, 957, 8038, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.054471826553344725, "compression_ratio": 1.948616600790514, "no_speech_prob": 5.906847945880145e-06}, {"id": 212, "seek": 147900, "start": 1489.0, "end": 1493.0, "text": " So the top row of this slide, we took a 3 and we actually just changed it into a 7.", "tokens": [50364, 467, 611, 1355, 300, 562, 321, 434, 39969, 17641, 44745, 5110, 11, 321, 643, 281, 652, 988, 300, 264, 17641, 44745, 1365, 10747, 1943, 380, 1075, 281, 360, 437, 2011, 294, 264, 1192, 5386, 295, 341, 4137, 510, 13, 50864, 50864, 407, 264, 1192, 5386, 295, 341, 4137, 11, 321, 1890, 257, 805, 293, 321, 767, 445, 3105, 309, 666, 257, 1614, 13, 51064, 51064, 407, 562, 264, 2316, 1619, 300, 264, 3256, 294, 264, 6597, 558, 307, 257, 1614, 11, 309, 311, 406, 257, 6146, 13, 492, 767, 445, 1319, 264, 4846, 1508, 13, 51364, 51364, 1133, 321, 1322, 17641, 44745, 5110, 11, 321, 528, 281, 652, 988, 300, 321, 434, 13389, 957, 8038, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.054471826553344725, "compression_ratio": 1.948616600790514, "no_speech_prob": 5.906847945880145e-06}, {"id": 213, "seek": 147900, "start": 1493.0, "end": 1499.0, "text": " So when the model says that the image in the upper right is a 7, it's not a mistake. We actually just change the input class.", "tokens": [50364, 467, 611, 1355, 300, 562, 321, 434, 39969, 17641, 44745, 5110, 11, 321, 643, 281, 652, 988, 300, 264, 17641, 44745, 1365, 10747, 1943, 380, 1075, 281, 360, 437, 2011, 294, 264, 1192, 5386, 295, 341, 4137, 510, 13, 50864, 50864, 407, 264, 1192, 5386, 295, 341, 4137, 11, 321, 1890, 257, 805, 293, 321, 767, 445, 3105, 309, 666, 257, 1614, 13, 51064, 51064, 407, 562, 264, 2316, 1619, 300, 264, 3256, 294, 264, 6597, 558, 307, 257, 1614, 11, 309, 311, 406, 257, 6146, 13, 492, 767, 445, 1319, 264, 4846, 1508, 13, 51364, 51364, 1133, 321, 1322, 17641, 44745, 5110, 11, 321, 528, 281, 652, 988, 300, 321, 434, 13389, 957, 8038, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.054471826553344725, "compression_ratio": 1.948616600790514, "no_speech_prob": 5.906847945880145e-06}, {"id": 214, "seek": 147900, "start": 1499.0, "end": 1504.0, "text": " When we build adversarial examples, we want to make sure that we're measuring real mistakes.", "tokens": [50364, 467, 611, 1355, 300, 562, 321, 434, 39969, 17641, 44745, 5110, 11, 321, 643, 281, 652, 988, 300, 264, 17641, 44745, 1365, 10747, 1943, 380, 1075, 281, 360, 437, 2011, 294, 264, 1192, 5386, 295, 341, 4137, 510, 13, 50864, 50864, 407, 264, 1192, 5386, 295, 341, 4137, 11, 321, 1890, 257, 805, 293, 321, 767, 445, 3105, 309, 666, 257, 1614, 13, 51064, 51064, 407, 562, 264, 2316, 1619, 300, 264, 3256, 294, 264, 6597, 558, 307, 257, 1614, 11, 309, 311, 406, 257, 6146, 13, 492, 767, 445, 1319, 264, 4846, 1508, 13, 51364, 51364, 1133, 321, 1322, 17641, 44745, 5110, 11, 321, 528, 281, 652, 988, 300, 321, 434, 13389, 957, 8038, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.054471826553344725, "compression_ratio": 1.948616600790514, "no_speech_prob": 5.906847945880145e-06}, {"id": 215, "seek": 150400, "start": 1504.0, "end": 1512.0, "text": " If we're experimenter studying how easy a network is to fool, we want to make sure that we're actually fooling it and not just changing the input class.", "tokens": [50364, 759, 321, 434, 5120, 260, 7601, 577, 1858, 257, 3209, 307, 281, 7979, 11, 321, 528, 281, 652, 988, 300, 321, 434, 767, 7979, 278, 309, 293, 406, 445, 4473, 264, 4846, 1508, 13, 50764, 50764, 400, 498, 321, 434, 294, 257, 35871, 11, 321, 767, 528, 281, 652, 988, 300, 321, 434, 9853, 3346, 29437, 38387, 294, 264, 1185, 13, 51064, 51064, 1407, 360, 300, 11, 562, 321, 1322, 17641, 44745, 5110, 11, 321, 764, 264, 11469, 2026, 281, 1817, 7146, 264, 40468, 399, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08094724549187554, "compression_ratio": 1.6745283018867925, "no_speech_prob": 6.183235200296622e-06}, {"id": 216, "seek": 150400, "start": 1512.0, "end": 1518.0, "text": " And if we're in a attacker, we actually want to make sure that we're causing misbehavior in the system.", "tokens": [50364, 759, 321, 434, 5120, 260, 7601, 577, 1858, 257, 3209, 307, 281, 7979, 11, 321, 528, 281, 652, 988, 300, 321, 434, 767, 7979, 278, 309, 293, 406, 445, 4473, 264, 4846, 1508, 13, 50764, 50764, 400, 498, 321, 434, 294, 257, 35871, 11, 321, 767, 528, 281, 652, 988, 300, 321, 434, 9853, 3346, 29437, 38387, 294, 264, 1185, 13, 51064, 51064, 1407, 360, 300, 11, 562, 321, 1322, 17641, 44745, 5110, 11, 321, 764, 264, 11469, 2026, 281, 1817, 7146, 264, 40468, 399, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08094724549187554, "compression_ratio": 1.6745283018867925, "no_speech_prob": 6.183235200296622e-06}, {"id": 217, "seek": 150400, "start": 1518.0, "end": 1525.0, "text": " To do that, when we build adversarial examples, we use the max norm to constrain the perturbation.", "tokens": [50364, 759, 321, 434, 5120, 260, 7601, 577, 1858, 257, 3209, 307, 281, 7979, 11, 321, 528, 281, 652, 988, 300, 321, 434, 767, 7979, 278, 309, 293, 406, 445, 4473, 264, 4846, 1508, 13, 50764, 50764, 400, 498, 321, 434, 294, 257, 35871, 11, 321, 767, 528, 281, 652, 988, 300, 321, 434, 9853, 3346, 29437, 38387, 294, 264, 1185, 13, 51064, 51064, 1407, 360, 300, 11, 562, 321, 1322, 17641, 44745, 5110, 11, 321, 764, 264, 11469, 2026, 281, 1817, 7146, 264, 40468, 399, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08094724549187554, "compression_ratio": 1.6745283018867925, "no_speech_prob": 6.183235200296622e-06}, {"id": 218, "seek": 152500, "start": 1525.0, "end": 1537.0, "text": " Basically, this says that no pixel can change by more than some amount of epsilon. So the L2 norm can get really big, but you can't concentrate all the changes for that L2 norm to erase pieces of the digit.", "tokens": [50364, 8537, 11, 341, 1619, 300, 572, 19261, 393, 1319, 538, 544, 813, 512, 2372, 295, 17889, 13, 407, 264, 441, 17, 2026, 393, 483, 534, 955, 11, 457, 291, 393, 380, 18089, 439, 264, 2962, 337, 300, 441, 17, 2026, 281, 23525, 3755, 295, 264, 14293, 13, 50964, 50964, 1743, 294, 264, 2767, 5386, 11, 510, 321, 23525, 264, 1192, 295, 257, 805, 13, 51164, 51164, 1133, 588, 2370, 636, 281, 1322, 364, 17641, 44745, 1365, 307, 445, 281, 747, 264, 16235, 295, 264, 2063, 300, 291, 1143, 281, 3847, 264, 3209, 365, 3104, 281, 264, 4846, 11, 293, 550, 747, 264, 1465, 295, 300, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1030265103589307, "compression_ratio": 1.6727941176470589, "no_speech_prob": 2.5967960027628578e-05}, {"id": 219, "seek": 152500, "start": 1537.0, "end": 1541.0, "text": " Like in the bottom row, here we erase the top of a 3.", "tokens": [50364, 8537, 11, 341, 1619, 300, 572, 19261, 393, 1319, 538, 544, 813, 512, 2372, 295, 17889, 13, 407, 264, 441, 17, 2026, 393, 483, 534, 955, 11, 457, 291, 393, 380, 18089, 439, 264, 2962, 337, 300, 441, 17, 2026, 281, 23525, 3755, 295, 264, 14293, 13, 50964, 50964, 1743, 294, 264, 2767, 5386, 11, 510, 321, 23525, 264, 1192, 295, 257, 805, 13, 51164, 51164, 1133, 588, 2370, 636, 281, 1322, 364, 17641, 44745, 1365, 307, 445, 281, 747, 264, 16235, 295, 264, 2063, 300, 291, 1143, 281, 3847, 264, 3209, 365, 3104, 281, 264, 4846, 11, 293, 550, 747, 264, 1465, 295, 300, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1030265103589307, "compression_ratio": 1.6727941176470589, "no_speech_prob": 2.5967960027628578e-05}, {"id": 220, "seek": 152500, "start": 1541.0, "end": 1552.0, "text": " When very fast way to build an adversarial example is just to take the gradient of the cost that you used to train the network with respect to the input, and then take the sign of that gradient.", "tokens": [50364, 8537, 11, 341, 1619, 300, 572, 19261, 393, 1319, 538, 544, 813, 512, 2372, 295, 17889, 13, 407, 264, 441, 17, 2026, 393, 483, 534, 955, 11, 457, 291, 393, 380, 18089, 439, 264, 2962, 337, 300, 441, 17, 2026, 281, 23525, 3755, 295, 264, 14293, 13, 50964, 50964, 1743, 294, 264, 2767, 5386, 11, 510, 321, 23525, 264, 1192, 295, 257, 805, 13, 51164, 51164, 1133, 588, 2370, 636, 281, 1322, 364, 17641, 44745, 1365, 307, 445, 281, 747, 264, 16235, 295, 264, 2063, 300, 291, 1143, 281, 3847, 264, 3209, 365, 3104, 281, 264, 4846, 11, 293, 550, 747, 264, 1465, 295, 300, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1030265103589307, "compression_ratio": 1.6727941176470589, "no_speech_prob": 2.5967960027628578e-05}, {"id": 221, "seek": 155200, "start": 1552.0, "end": 1556.0, "text": " The sign is essentially enforcing the max norm constraint.", "tokens": [50364, 440, 1465, 307, 4476, 25495, 2175, 264, 11469, 2026, 25534, 13, 50564, 50564, 509, 434, 787, 4350, 281, 1319, 264, 4846, 538, 493, 281, 17889, 412, 1184, 19261, 13, 50814, 50814, 407, 498, 291, 445, 747, 264, 1465, 11, 309, 5112, 291, 1968, 291, 528, 281, 909, 493, 281, 17889, 420, 16390, 17889, 294, 1668, 281, 4607, 264, 3209, 13, 51114, 51114, 509, 393, 1910, 341, 382, 1940, 264, 14816, 300, 264, 3209, 307, 544, 420, 1570, 8213, 382, 321, 4712, 322, 341, 4137, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08466417334052954, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.81993434834294e-06}, {"id": 222, "seek": 155200, "start": 1556.0, "end": 1561.0, "text": " You're only allowed to change the input by up to epsilon at each pixel.", "tokens": [50364, 440, 1465, 307, 4476, 25495, 2175, 264, 11469, 2026, 25534, 13, 50564, 50564, 509, 434, 787, 4350, 281, 1319, 264, 4846, 538, 493, 281, 17889, 412, 1184, 19261, 13, 50814, 50814, 407, 498, 291, 445, 747, 264, 1465, 11, 309, 5112, 291, 1968, 291, 528, 281, 909, 493, 281, 17889, 420, 16390, 17889, 294, 1668, 281, 4607, 264, 3209, 13, 51114, 51114, 509, 393, 1910, 341, 382, 1940, 264, 14816, 300, 264, 3209, 307, 544, 420, 1570, 8213, 382, 321, 4712, 322, 341, 4137, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08466417334052954, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.81993434834294e-06}, {"id": 223, "seek": 155200, "start": 1561.0, "end": 1567.0, "text": " So if you just take the sign, it tells you whether you want to add up to epsilon or subtract epsilon in order to hurt the network.", "tokens": [50364, 440, 1465, 307, 4476, 25495, 2175, 264, 11469, 2026, 25534, 13, 50564, 50564, 509, 434, 787, 4350, 281, 1319, 264, 4846, 538, 493, 281, 17889, 412, 1184, 19261, 13, 50814, 50814, 407, 498, 291, 445, 747, 264, 1465, 11, 309, 5112, 291, 1968, 291, 528, 281, 909, 493, 281, 17889, 420, 16390, 17889, 294, 1668, 281, 4607, 264, 3209, 13, 51114, 51114, 509, 393, 1910, 341, 382, 1940, 264, 14816, 300, 264, 3209, 307, 544, 420, 1570, 8213, 382, 321, 4712, 322, 341, 4137, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08466417334052954, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.81993434834294e-06}, {"id": 224, "seek": 155200, "start": 1567.0, "end": 1573.0, "text": " You can view this as taking the observation that the network is more or less linear as we showed on this slide.", "tokens": [50364, 440, 1465, 307, 4476, 25495, 2175, 264, 11469, 2026, 25534, 13, 50564, 50564, 509, 434, 787, 4350, 281, 1319, 264, 4846, 538, 493, 281, 17889, 412, 1184, 19261, 13, 50814, 50814, 407, 498, 291, 445, 747, 264, 1465, 11, 309, 5112, 291, 1968, 291, 528, 281, 909, 493, 281, 17889, 420, 16390, 17889, 294, 1668, 281, 4607, 264, 3209, 13, 51114, 51114, 509, 393, 1910, 341, 382, 1940, 264, 14816, 300, 264, 3209, 307, 544, 420, 1570, 8213, 382, 321, 4712, 322, 341, 4137, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08466417334052954, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.81993434834294e-06}, {"id": 225, "seek": 157300, "start": 1573.0, "end": 1582.0, "text": " Using that to motivate building a first order Taylor series approximation of the neural networks cost.", "tokens": [50364, 11142, 300, 281, 28497, 2390, 257, 700, 1668, 12060, 2638, 28023, 295, 264, 18161, 9590, 2063, 13, 50814, 50814, 400, 550, 3983, 281, 300, 12060, 2638, 28023, 11, 321, 528, 281, 19874, 264, 2063, 3480, 341, 11469, 2026, 25534, 13, 51164, 51164, 400, 300, 2709, 505, 341, 6532, 300, 321, 818, 264, 2370, 16235, 1465, 3170, 13, 51364, 51364, 759, 291, 528, 281, 445, 483, 428, 2377, 9360, 293, 722, 1455, 17641, 44745, 5110, 534, 2661, 11, 420, 498, 291, 362, 364, 9284, 689, 291, 528, 281, 3847, 322, 17641, 44745, 5110, 294, 264, 7284, 6367, 295, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09183471642651604, "compression_ratio": 1.7793594306049823, "no_speech_prob": 3.738531086128205e-05}, {"id": 226, "seek": 157300, "start": 1582.0, "end": 1589.0, "text": " And then subject to that Taylor series approximation, we want to maximize the cost following this max norm constraint.", "tokens": [50364, 11142, 300, 281, 28497, 2390, 257, 700, 1668, 12060, 2638, 28023, 295, 264, 18161, 9590, 2063, 13, 50814, 50814, 400, 550, 3983, 281, 300, 12060, 2638, 28023, 11, 321, 528, 281, 19874, 264, 2063, 3480, 341, 11469, 2026, 25534, 13, 51164, 51164, 400, 300, 2709, 505, 341, 6532, 300, 321, 818, 264, 2370, 16235, 1465, 3170, 13, 51364, 51364, 759, 291, 528, 281, 445, 483, 428, 2377, 9360, 293, 722, 1455, 17641, 44745, 5110, 534, 2661, 11, 420, 498, 291, 362, 364, 9284, 689, 291, 528, 281, 3847, 322, 17641, 44745, 5110, 294, 264, 7284, 6367, 295, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09183471642651604, "compression_ratio": 1.7793594306049823, "no_speech_prob": 3.738531086128205e-05}, {"id": 227, "seek": 157300, "start": 1589.0, "end": 1593.0, "text": " And that gives us this technique that we call the fast gradient sign method.", "tokens": [50364, 11142, 300, 281, 28497, 2390, 257, 700, 1668, 12060, 2638, 28023, 295, 264, 18161, 9590, 2063, 13, 50814, 50814, 400, 550, 3983, 281, 300, 12060, 2638, 28023, 11, 321, 528, 281, 19874, 264, 2063, 3480, 341, 11469, 2026, 25534, 13, 51164, 51164, 400, 300, 2709, 505, 341, 6532, 300, 321, 818, 264, 2370, 16235, 1465, 3170, 13, 51364, 51364, 759, 291, 528, 281, 445, 483, 428, 2377, 9360, 293, 722, 1455, 17641, 44745, 5110, 534, 2661, 11, 420, 498, 291, 362, 364, 9284, 689, 291, 528, 281, 3847, 322, 17641, 44745, 5110, 294, 264, 7284, 6367, 295, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09183471642651604, "compression_ratio": 1.7793594306049823, "no_speech_prob": 3.738531086128205e-05}, {"id": 228, "seek": 157300, "start": 1593.0, "end": 1602.0, "text": " If you want to just get your hands dirty and start making adversarial examples really quickly, or if you have an algorithm where you want to train on adversarial examples in the inner loop of learning,", "tokens": [50364, 11142, 300, 281, 28497, 2390, 257, 700, 1668, 12060, 2638, 28023, 295, 264, 18161, 9590, 2063, 13, 50814, 50814, 400, 550, 3983, 281, 300, 12060, 2638, 28023, 11, 321, 528, 281, 19874, 264, 2063, 3480, 341, 11469, 2026, 25534, 13, 51164, 51164, 400, 300, 2709, 505, 341, 6532, 300, 321, 818, 264, 2370, 16235, 1465, 3170, 13, 51364, 51364, 759, 291, 528, 281, 445, 483, 428, 2377, 9360, 293, 722, 1455, 17641, 44745, 5110, 534, 2661, 11, 420, 498, 291, 362, 364, 9284, 689, 291, 528, 281, 3847, 322, 17641, 44745, 5110, 294, 264, 7284, 6367, 295, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09183471642651604, "compression_ratio": 1.7793594306049823, "no_speech_prob": 3.738531086128205e-05}, {"id": 229, "seek": 160200, "start": 1602.0, "end": 1606.0, "text": " this method will make adversarial examples for you very, very quickly.", "tokens": [50364, 341, 3170, 486, 652, 17641, 44745, 5110, 337, 291, 588, 11, 588, 2661, 13, 50564, 50564, 682, 3124, 11, 291, 820, 611, 764, 661, 7150, 411, 22924, 2741, 5045, 271, 2690, 2361, 322, 3866, 4439, 295, 264, 12018, 5028, 6545, 281, 652, 988, 300, 291, 362, 257, 588, 2068, 2690, 300, 291, 1565, 484, 562, 291, 519, 291, 362, 257, 2316, 300, 1062, 312, 544, 4005, 13, 51264, 51264, 316, 688, 295, 264, 565, 561, 915, 300, 436, 393, 11785, 264, 2370, 16235, 1465, 3170, 293, 519, 300, 436, 600, 3094, 257, 4406, 7654, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10685951541168522, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.3059279556036927e-05}, {"id": 230, "seek": 160200, "start": 1606.0, "end": 1620.0, "text": " In practice, you should also use other methods like Nicholas Carlinis attack based on multiple steps of the atom optimizer to make sure that you have a very strong attack that you bring out when you think you have a model that might be more powerful.", "tokens": [50364, 341, 3170, 486, 652, 17641, 44745, 5110, 337, 291, 588, 11, 588, 2661, 13, 50564, 50564, 682, 3124, 11, 291, 820, 611, 764, 661, 7150, 411, 22924, 2741, 5045, 271, 2690, 2361, 322, 3866, 4439, 295, 264, 12018, 5028, 6545, 281, 652, 988, 300, 291, 362, 257, 588, 2068, 2690, 300, 291, 1565, 484, 562, 291, 519, 291, 362, 257, 2316, 300, 1062, 312, 544, 4005, 13, 51264, 51264, 316, 688, 295, 264, 565, 561, 915, 300, 436, 393, 11785, 264, 2370, 16235, 1465, 3170, 293, 519, 300, 436, 600, 3094, 257, 4406, 7654, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10685951541168522, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.3059279556036927e-05}, {"id": 231, "seek": 160200, "start": 1620.0, "end": 1626.0, "text": " A lot of the time people find that they can defeat the fast gradient sign method and think that they've built a successful defense,", "tokens": [50364, 341, 3170, 486, 652, 17641, 44745, 5110, 337, 291, 588, 11, 588, 2661, 13, 50564, 50564, 682, 3124, 11, 291, 820, 611, 764, 661, 7150, 411, 22924, 2741, 5045, 271, 2690, 2361, 322, 3866, 4439, 295, 264, 12018, 5028, 6545, 281, 652, 988, 300, 291, 362, 257, 588, 2068, 2690, 300, 291, 1565, 484, 562, 291, 519, 291, 362, 257, 2316, 300, 1062, 312, 544, 4005, 13, 51264, 51264, 316, 688, 295, 264, 565, 561, 915, 300, 436, 393, 11785, 264, 2370, 16235, 1465, 3170, 293, 519, 300, 436, 600, 3094, 257, 4406, 7654, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10685951541168522, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.3059279556036927e-05}, {"id": 232, "seek": 162600, "start": 1626.0, "end": 1638.0, "text": " but then when you bring out a more powerful method that takes longer to evaluate, they find that they can't overcome the more computationally expensive attack.", "tokens": [50364, 457, 550, 562, 291, 1565, 484, 257, 544, 4005, 3170, 300, 2516, 2854, 281, 13059, 11, 436, 915, 300, 436, 393, 380, 10473, 264, 544, 24903, 379, 5124, 2690, 13, 50964, 50964, 286, 600, 1907, 291, 300, 17641, 44745, 5110, 1051, 570, 264, 2316, 307, 588, 8213, 11, 293, 550, 286, 1907, 291, 300, 291, 727, 764, 341, 8213, 1558, 15302, 281, 1322, 341, 2690, 11, 264, 2370, 16235, 1465, 3170, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08192675364644904, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.9298040469002444e-06}, {"id": 233, "seek": 162600, "start": 1638.0, "end": 1649.0, "text": " I've told you that adversarial examples happen because the model is very linear, and then I told you that you could use this linear idea assumption to build this attack, the fast gradient sign method.", "tokens": [50364, 457, 550, 562, 291, 1565, 484, 257, 544, 4005, 3170, 300, 2516, 2854, 281, 13059, 11, 436, 915, 300, 436, 393, 380, 10473, 264, 544, 24903, 379, 5124, 2690, 13, 50964, 50964, 286, 600, 1907, 291, 300, 17641, 44745, 5110, 1051, 570, 264, 2316, 307, 588, 8213, 11, 293, 550, 286, 1907, 291, 300, 291, 727, 764, 341, 8213, 1558, 15302, 281, 1322, 341, 2690, 11, 264, 2370, 16235, 1465, 3170, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08192675364644904, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.9298040469002444e-06}, {"id": 234, "seek": 164900, "start": 1649.0, "end": 1658.0, "text": " This method would apply to a regular neural network that doesn't have any special defenses, will get over a 99% attack success rate.", "tokens": [50364, 639, 3170, 576, 3079, 281, 257, 3890, 18161, 3209, 300, 1177, 380, 362, 604, 2121, 35989, 11, 486, 483, 670, 257, 11803, 4, 2690, 2245, 3314, 13, 50814, 50814, 407, 300, 2544, 281, 9064, 8344, 341, 17291, 300, 17641, 44745, 5110, 808, 490, 264, 2316, 885, 1400, 886, 8213, 293, 48224, 990, 294, 8213, 283, 1299, 626, 562, 309, 4659, 380, 13, 51364, 51364, 492, 393, 767, 352, 1237, 337, 512, 544, 4467, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10071520316295135, "compression_ratio": 1.5278969957081545, "no_speech_prob": 2.0109935576329008e-05}, {"id": 235, "seek": 164900, "start": 1658.0, "end": 1669.0, "text": " So that seems to confirm somewhat this hypothesis that adversarial examples come from the model being far too linear and extrapolating in linear fashions when it shouldn't.", "tokens": [50364, 639, 3170, 576, 3079, 281, 257, 3890, 18161, 3209, 300, 1177, 380, 362, 604, 2121, 35989, 11, 486, 483, 670, 257, 11803, 4, 2690, 2245, 3314, 13, 50814, 50814, 407, 300, 2544, 281, 9064, 8344, 341, 17291, 300, 17641, 44745, 5110, 808, 490, 264, 2316, 885, 1400, 886, 8213, 293, 48224, 990, 294, 8213, 283, 1299, 626, 562, 309, 4659, 380, 13, 51364, 51364, 492, 393, 767, 352, 1237, 337, 512, 544, 4467, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10071520316295135, "compression_ratio": 1.5278969957081545, "no_speech_prob": 2.0109935576329008e-05}, {"id": 236, "seek": 164900, "start": 1669.0, "end": 1672.0, "text": " We can actually go looking for some more evidence.", "tokens": [50364, 639, 3170, 576, 3079, 281, 257, 3890, 18161, 3209, 300, 1177, 380, 362, 604, 2121, 35989, 11, 486, 483, 670, 257, 11803, 4, 2690, 2245, 3314, 13, 50814, 50814, 407, 300, 2544, 281, 9064, 8344, 341, 17291, 300, 17641, 44745, 5110, 808, 490, 264, 2316, 885, 1400, 886, 8213, 293, 48224, 990, 294, 8213, 283, 1299, 626, 562, 309, 4659, 380, 13, 51364, 51364, 492, 393, 767, 352, 1237, 337, 512, 544, 4467, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10071520316295135, "compression_ratio": 1.5278969957081545, "no_speech_prob": 2.0109935576329008e-05}, {"id": 237, "seek": 167200, "start": 1672.0, "end": 1682.0, "text": " My friend David Ward-Farley and I built these maps of the decision boundaries of neural networks, and we found that they are consistent with the linearity hypothesis.", "tokens": [50364, 1222, 1277, 4389, 23794, 12, 37, 289, 3420, 293, 286, 3094, 613, 11317, 295, 264, 3537, 13180, 295, 18161, 9590, 11, 293, 321, 1352, 300, 436, 366, 8398, 365, 264, 8213, 507, 17291, 13, 50864, 50864, 407, 264, 479, 24446, 44, 307, 300, 2690, 3170, 300, 286, 7619, 294, 264, 3894, 4137, 689, 321, 747, 264, 1465, 295, 264, 16235, 13, 51214, 51214, 492, 1116, 411, 281, 1322, 257, 4471, 295, 257, 732, 12, 18759, 3278, 12, 11963, 295, 4846, 1901, 293, 855, 597, 5359, 366, 13279, 281, 264, 1412, 412, 1184, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09893996374947685, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.3428876374964602e-05}, {"id": 238, "seek": 167200, "start": 1682.0, "end": 1689.0, "text": " So the FGSM is that attack method that I described in the previous slide where we take the sign of the gradient.", "tokens": [50364, 1222, 1277, 4389, 23794, 12, 37, 289, 3420, 293, 286, 3094, 613, 11317, 295, 264, 3537, 13180, 295, 18161, 9590, 11, 293, 321, 1352, 300, 436, 366, 8398, 365, 264, 8213, 507, 17291, 13, 50864, 50864, 407, 264, 479, 24446, 44, 307, 300, 2690, 3170, 300, 286, 7619, 294, 264, 3894, 4137, 689, 321, 747, 264, 1465, 295, 264, 16235, 13, 51214, 51214, 492, 1116, 411, 281, 1322, 257, 4471, 295, 257, 732, 12, 18759, 3278, 12, 11963, 295, 4846, 1901, 293, 855, 597, 5359, 366, 13279, 281, 264, 1412, 412, 1184, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09893996374947685, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.3428876374964602e-05}, {"id": 239, "seek": 167200, "start": 1689.0, "end": 1699.0, "text": " We'd like to build a map of a two-dimensional cross-section of input space and show which classes are assigned to the data at each point.", "tokens": [50364, 1222, 1277, 4389, 23794, 12, 37, 289, 3420, 293, 286, 3094, 613, 11317, 295, 264, 3537, 13180, 295, 18161, 9590, 11, 293, 321, 1352, 300, 436, 366, 8398, 365, 264, 8213, 507, 17291, 13, 50864, 50864, 407, 264, 479, 24446, 44, 307, 300, 2690, 3170, 300, 286, 7619, 294, 264, 3894, 4137, 689, 321, 747, 264, 1465, 295, 264, 16235, 13, 51214, 51214, 492, 1116, 411, 281, 1322, 257, 4471, 295, 257, 732, 12, 18759, 3278, 12, 11963, 295, 4846, 1901, 293, 855, 597, 5359, 366, 13279, 281, 264, 1412, 412, 1184, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09893996374947685, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.3428876374964602e-05}, {"id": 240, "seek": 169900, "start": 1699.0, "end": 1713.0, "text": " In the grid on the right, each different cell, each little square within the grid, is a map of a C-Far10 classifier's decision boundary with each cell corresponding to a different C-Far10 test example.", "tokens": [50364, 682, 264, 10748, 322, 264, 558, 11, 1184, 819, 2815, 11, 1184, 707, 3732, 1951, 264, 10748, 11, 307, 257, 4471, 295, 257, 383, 12, 37, 289, 3279, 1508, 9902, 311, 3537, 12866, 365, 1184, 2815, 11760, 281, 257, 819, 383, 12, 37, 289, 3279, 1500, 1365, 13, 51064, 51064, 1282, 264, 1411, 11, 286, 855, 291, 257, 707, 9451, 689, 291, 393, 1223, 437, 1184, 2815, 1355, 13, 51314, 51314, 440, 588, 3056, 295, 1184, 2815, 23249, 281, 264, 3380, 1365, 490, 264, 383, 12, 37, 289, 3279, 1412, 992, 365, 572, 26747, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10067402232776988, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.3503514310286846e-05}, {"id": 241, "seek": 169900, "start": 1713.0, "end": 1718.0, "text": " On the left, I show you a little legend where you can understand what each cell means.", "tokens": [50364, 682, 264, 10748, 322, 264, 558, 11, 1184, 819, 2815, 11, 1184, 707, 3732, 1951, 264, 10748, 11, 307, 257, 4471, 295, 257, 383, 12, 37, 289, 3279, 1508, 9902, 311, 3537, 12866, 365, 1184, 2815, 11760, 281, 257, 819, 383, 12, 37, 289, 3279, 1500, 1365, 13, 51064, 51064, 1282, 264, 1411, 11, 286, 855, 291, 257, 707, 9451, 689, 291, 393, 1223, 437, 1184, 2815, 1355, 13, 51314, 51314, 440, 588, 3056, 295, 1184, 2815, 23249, 281, 264, 3380, 1365, 490, 264, 383, 12, 37, 289, 3279, 1412, 992, 365, 572, 26747, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10067402232776988, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.3503514310286846e-05}, {"id": 242, "seek": 169900, "start": 1718.0, "end": 1726.0, "text": " The very center of each cell corresponds to the original example from the C-Far10 data set with no modification.", "tokens": [50364, 682, 264, 10748, 322, 264, 558, 11, 1184, 819, 2815, 11, 1184, 707, 3732, 1951, 264, 10748, 11, 307, 257, 4471, 295, 257, 383, 12, 37, 289, 3279, 1508, 9902, 311, 3537, 12866, 365, 1184, 2815, 11760, 281, 257, 819, 383, 12, 37, 289, 3279, 1500, 1365, 13, 51064, 51064, 1282, 264, 1411, 11, 286, 855, 291, 257, 707, 9451, 689, 291, 393, 1223, 437, 1184, 2815, 1355, 13, 51314, 51314, 440, 588, 3056, 295, 1184, 2815, 23249, 281, 264, 3380, 1365, 490, 264, 383, 12, 37, 289, 3279, 1412, 992, 365, 572, 26747, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10067402232776988, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.3503514310286846e-05}, {"id": 243, "seek": 172600, "start": 1726.0, "end": 1733.0, "text": " As we move left to right in the cell, we're moving in the direction of the fast gradient sign method attack, so just the sign of the gradient.", "tokens": [50364, 1018, 321, 1286, 1411, 281, 558, 294, 264, 2815, 11, 321, 434, 2684, 294, 264, 3513, 295, 264, 2370, 16235, 1465, 3170, 2690, 11, 370, 445, 264, 1465, 295, 264, 16235, 13, 50714, 50714, 1018, 321, 1286, 493, 293, 760, 1951, 264, 2815, 11, 321, 434, 2684, 294, 257, 4974, 3513, 300, 311, 41488, 281, 264, 2370, 16235, 1465, 3170, 3513, 13, 51114, 51114, 407, 321, 483, 281, 536, 257, 3278, 12, 11963, 11, 257, 568, 35, 3278, 12, 11963, 295, 383, 12, 37, 289, 3279, 3537, 1901, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06723228064916467, "compression_ratio": 1.8795811518324608, "no_speech_prob": 1.0811955689860042e-05}, {"id": 244, "seek": 172600, "start": 1733.0, "end": 1741.0, "text": " As we move up and down within the cell, we're moving in a random direction that's orthogonal to the fast gradient sign method direction.", "tokens": [50364, 1018, 321, 1286, 1411, 281, 558, 294, 264, 2815, 11, 321, 434, 2684, 294, 264, 3513, 295, 264, 2370, 16235, 1465, 3170, 2690, 11, 370, 445, 264, 1465, 295, 264, 16235, 13, 50714, 50714, 1018, 321, 1286, 493, 293, 760, 1951, 264, 2815, 11, 321, 434, 2684, 294, 257, 4974, 3513, 300, 311, 41488, 281, 264, 2370, 16235, 1465, 3170, 3513, 13, 51114, 51114, 407, 321, 483, 281, 536, 257, 3278, 12, 11963, 11, 257, 568, 35, 3278, 12, 11963, 295, 383, 12, 37, 289, 3279, 3537, 1901, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06723228064916467, "compression_ratio": 1.8795811518324608, "no_speech_prob": 1.0811955689860042e-05}, {"id": 245, "seek": 172600, "start": 1741.0, "end": 1748.0, "text": " So we get to see a cross-section, a 2D cross-section of C-Far10 decision space.", "tokens": [50364, 1018, 321, 1286, 1411, 281, 558, 294, 264, 2815, 11, 321, 434, 2684, 294, 264, 3513, 295, 264, 2370, 16235, 1465, 3170, 2690, 11, 370, 445, 264, 1465, 295, 264, 16235, 13, 50714, 50714, 1018, 321, 1286, 493, 293, 760, 1951, 264, 2815, 11, 321, 434, 2684, 294, 257, 4974, 3513, 300, 311, 41488, 281, 264, 2370, 16235, 1465, 3170, 3513, 13, 51114, 51114, 407, 321, 483, 281, 536, 257, 3278, 12, 11963, 11, 257, 568, 35, 3278, 12, 11963, 295, 383, 12, 37, 289, 3279, 3537, 1901, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06723228064916467, "compression_ratio": 1.8795811518324608, "no_speech_prob": 1.0811955689860042e-05}, {"id": 246, "seek": 174800, "start": 1748.0, "end": 1763.0, "text": " At each pixel within this map, we plot a color that tells us which classes is assigned there. We use white pixels to indicate that the correct class was chosen, and then we use different colors to represent all of the other incorrect classes.", "tokens": [50364, 1711, 1184, 19261, 1951, 341, 4471, 11, 321, 7542, 257, 2017, 300, 5112, 505, 597, 5359, 307, 13279, 456, 13, 492, 764, 2418, 18668, 281, 13330, 300, 264, 3006, 1508, 390, 8614, 11, 293, 550, 321, 764, 819, 4577, 281, 2906, 439, 295, 264, 661, 18424, 5359, 13, 51114, 51114, 509, 393, 536, 300, 294, 6217, 439, 295, 264, 10748, 5438, 322, 264, 558, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 307, 2418, 13, 51464, 51464, 407, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 575, 668, 8944, 20627, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07767344524985866, "compression_ratio": 1.8421052631578947, "no_speech_prob": 5.87553267905605e-06}, {"id": 247, "seek": 174800, "start": 1763.0, "end": 1770.0, "text": " You can see that in nearly all of the grid cells on the right, roughly the left half of the image is white.", "tokens": [50364, 1711, 1184, 19261, 1951, 341, 4471, 11, 321, 7542, 257, 2017, 300, 5112, 505, 597, 5359, 307, 13279, 456, 13, 492, 764, 2418, 18668, 281, 13330, 300, 264, 3006, 1508, 390, 8614, 11, 293, 550, 321, 764, 819, 4577, 281, 2906, 439, 295, 264, 661, 18424, 5359, 13, 51114, 51114, 509, 393, 536, 300, 294, 6217, 439, 295, 264, 10748, 5438, 322, 264, 558, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 307, 2418, 13, 51464, 51464, 407, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 575, 668, 8944, 20627, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07767344524985866, "compression_ratio": 1.8421052631578947, "no_speech_prob": 5.87553267905605e-06}, {"id": 248, "seek": 174800, "start": 1770.0, "end": 1775.0, "text": " So, roughly the left half of the image has been correctly classified.", "tokens": [50364, 1711, 1184, 19261, 1951, 341, 4471, 11, 321, 7542, 257, 2017, 300, 5112, 505, 597, 5359, 307, 13279, 456, 13, 492, 764, 2418, 18668, 281, 13330, 300, 264, 3006, 1508, 390, 8614, 11, 293, 550, 321, 764, 819, 4577, 281, 2906, 439, 295, 264, 661, 18424, 5359, 13, 51114, 51114, 509, 393, 536, 300, 294, 6217, 439, 295, 264, 10748, 5438, 322, 264, 558, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 307, 2418, 13, 51464, 51464, 407, 11, 9810, 264, 1411, 1922, 295, 264, 3256, 575, 668, 8944, 20627, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07767344524985866, "compression_ratio": 1.8421052631578947, "no_speech_prob": 5.87553267905605e-06}, {"id": 249, "seek": 177500, "start": 1775.0, "end": 1784.0, "text": " As we move to the right, we see that there's usually a different color on the right half, and the boundaries between these regions are approximately linear.", "tokens": [50364, 1018, 321, 1286, 281, 264, 558, 11, 321, 536, 300, 456, 311, 2673, 257, 819, 2017, 322, 264, 558, 1922, 11, 293, 264, 13180, 1296, 613, 10682, 366, 10447, 8213, 13, 50814, 50814, 708, 311, 516, 322, 510, 307, 300, 264, 2370, 16235, 1465, 3170, 575, 9234, 257, 3513, 689, 498, 321, 483, 257, 2416, 5893, 1674, 365, 300, 3513, 11, 321, 393, 483, 364, 17641, 44745, 1365, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.05462391735756234, "compression_ratio": 1.6213592233009708, "no_speech_prob": 4.189461833448149e-06}, {"id": 250, "seek": 177500, "start": 1784.0, "end": 1794.0, "text": " What's going on here is that the fast gradient sign method has identified a direction where if we get a large dot product with that direction, we can get an adversarial example.", "tokens": [50364, 1018, 321, 1286, 281, 264, 558, 11, 321, 536, 300, 456, 311, 2673, 257, 819, 2017, 322, 264, 558, 1922, 11, 293, 264, 13180, 1296, 613, 10682, 366, 10447, 8213, 13, 50814, 50814, 708, 311, 516, 322, 510, 307, 300, 264, 2370, 16235, 1465, 3170, 575, 9234, 257, 3513, 689, 498, 321, 483, 257, 2416, 5893, 1674, 365, 300, 3513, 11, 321, 393, 483, 364, 17641, 44745, 1365, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.05462391735756234, "compression_ratio": 1.6213592233009708, "no_speech_prob": 4.189461833448149e-06}, {"id": 251, "seek": 179400, "start": 1794.0, "end": 1805.0, "text": " From this, we see that adversarial examples live more or less in linear subspaces. When we first discovered adversarial examples, we thought that they might live in little tiny pockets.", "tokens": [50364, 3358, 341, 11, 321, 536, 300, 17641, 44745, 5110, 1621, 544, 420, 1570, 294, 8213, 2090, 79, 2116, 13, 1133, 321, 700, 6941, 17641, 44745, 5110, 11, 321, 1194, 300, 436, 1062, 1621, 294, 707, 5870, 16491, 13, 50914, 50914, 400, 264, 700, 3035, 321, 767, 1608, 6987, 300, 1310, 456, 366, 257, 707, 857, 411, 264, 15090, 3547, 11, 10596, 484, 2721, 20590, 3654, 264, 957, 3547, 11, 365, 6217, 633, 957, 1230, 885, 2651, 257, 15090, 1230, 13, 51464, 51464, 492, 1194, 300, 570, 321, 645, 1075, 281, 915, 364, 17641, 44745, 1365, 11760, 281, 633, 2541, 1365, 300, 321, 13210, 666, 264, 3209, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10995260874430339, "compression_ratio": 1.931407942238267, "no_speech_prob": 2.1831268895766698e-05}, {"id": 252, "seek": 179400, "start": 1805.0, "end": 1816.0, "text": " And the first paper we actually speculated that maybe there are a little bit like the rational numbers, hiding out finally tile among the real numbers, with nearly every real number being near a rational number.", "tokens": [50364, 3358, 341, 11, 321, 536, 300, 17641, 44745, 5110, 1621, 544, 420, 1570, 294, 8213, 2090, 79, 2116, 13, 1133, 321, 700, 6941, 17641, 44745, 5110, 11, 321, 1194, 300, 436, 1062, 1621, 294, 707, 5870, 16491, 13, 50914, 50914, 400, 264, 700, 3035, 321, 767, 1608, 6987, 300, 1310, 456, 366, 257, 707, 857, 411, 264, 15090, 3547, 11, 10596, 484, 2721, 20590, 3654, 264, 957, 3547, 11, 365, 6217, 633, 957, 1230, 885, 2651, 257, 15090, 1230, 13, 51464, 51464, 492, 1194, 300, 570, 321, 645, 1075, 281, 915, 364, 17641, 44745, 1365, 11760, 281, 633, 2541, 1365, 300, 321, 13210, 666, 264, 3209, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10995260874430339, "compression_ratio": 1.931407942238267, "no_speech_prob": 2.1831268895766698e-05}, {"id": 253, "seek": 179400, "start": 1816.0, "end": 1823.0, "text": " We thought that because we were able to find an adversarial example corresponding to every clean example that we loaded into the network.", "tokens": [50364, 3358, 341, 11, 321, 536, 300, 17641, 44745, 5110, 1621, 544, 420, 1570, 294, 8213, 2090, 79, 2116, 13, 1133, 321, 700, 6941, 17641, 44745, 5110, 11, 321, 1194, 300, 436, 1062, 1621, 294, 707, 5870, 16491, 13, 50914, 50914, 400, 264, 700, 3035, 321, 767, 1608, 6987, 300, 1310, 456, 366, 257, 707, 857, 411, 264, 15090, 3547, 11, 10596, 484, 2721, 20590, 3654, 264, 957, 3547, 11, 365, 6217, 633, 957, 1230, 885, 2651, 257, 15090, 1230, 13, 51464, 51464, 492, 1194, 300, 570, 321, 645, 1075, 281, 915, 364, 17641, 44745, 1365, 11760, 281, 633, 2541, 1365, 300, 321, 13210, 666, 264, 3209, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10995260874430339, "compression_ratio": 1.931407942238267, "no_speech_prob": 2.1831268895766698e-05}, {"id": 254, "seek": 182300, "start": 1823.0, "end": 1834.0, "text": " After doing this further analysis, we found that what's happening is that every real example is near one of these linear decision batteries, where you cross over into an adversarial subspace.", "tokens": [50364, 2381, 884, 341, 3052, 5215, 11, 321, 1352, 300, 437, 311, 2737, 307, 300, 633, 957, 1365, 307, 2651, 472, 295, 613, 8213, 3537, 13070, 11, 689, 291, 3278, 670, 666, 364, 17641, 44745, 2090, 17940, 13, 50914, 50914, 400, 1564, 291, 434, 294, 300, 17641, 44745, 2090, 17940, 11, 439, 264, 661, 2793, 11184, 366, 611, 17641, 44745, 5110, 300, 486, 312, 3346, 11665, 2587, 13, 51314, 51314, 639, 575, 3825, 16602, 11, 570, 309, 1355, 291, 787, 643, 281, 483, 264, 3513, 558, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07403130531311035, "compression_ratio": 1.689795918367347, "no_speech_prob": 5.512659754458582e-06}, {"id": 255, "seek": 182300, "start": 1834.0, "end": 1842.0, "text": " And once you're in that adversarial subspace, all the other points nearby are also adversarial examples that will be misclassified.", "tokens": [50364, 2381, 884, 341, 3052, 5215, 11, 321, 1352, 300, 437, 311, 2737, 307, 300, 633, 957, 1365, 307, 2651, 472, 295, 613, 8213, 3537, 13070, 11, 689, 291, 3278, 670, 666, 364, 17641, 44745, 2090, 17940, 13, 50914, 50914, 400, 1564, 291, 434, 294, 300, 17641, 44745, 2090, 17940, 11, 439, 264, 661, 2793, 11184, 366, 611, 17641, 44745, 5110, 300, 486, 312, 3346, 11665, 2587, 13, 51314, 51314, 639, 575, 3825, 16602, 11, 570, 309, 1355, 291, 787, 643, 281, 483, 264, 3513, 558, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07403130531311035, "compression_ratio": 1.689795918367347, "no_speech_prob": 5.512659754458582e-06}, {"id": 256, "seek": 182300, "start": 1842.0, "end": 1847.0, "text": " This has security implications, because it means you only need to get the direction right.", "tokens": [50364, 2381, 884, 341, 3052, 5215, 11, 321, 1352, 300, 437, 311, 2737, 307, 300, 633, 957, 1365, 307, 2651, 472, 295, 613, 8213, 3537, 13070, 11, 689, 291, 3278, 670, 666, 364, 17641, 44745, 2090, 17940, 13, 50914, 50914, 400, 1564, 291, 434, 294, 300, 17641, 44745, 2090, 17940, 11, 439, 264, 661, 2793, 11184, 366, 611, 17641, 44745, 5110, 300, 486, 312, 3346, 11665, 2587, 13, 51314, 51314, 639, 575, 3825, 16602, 11, 570, 309, 1355, 291, 787, 643, 281, 483, 264, 3513, 558, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07403130531311035, "compression_ratio": 1.689795918367347, "no_speech_prob": 5.512659754458582e-06}, {"id": 257, "seek": 184700, "start": 1847.0, "end": 1855.0, "text": " You don't need to find an exact coordinate in space. You just need to find a direction that has a large dot product with the sign of the gradient.", "tokens": [50364, 509, 500, 380, 643, 281, 915, 364, 1900, 15670, 294, 1901, 13, 509, 445, 643, 281, 915, 257, 3513, 300, 575, 257, 2416, 5893, 1674, 365, 264, 1465, 295, 264, 16235, 13, 50764, 50764, 400, 1564, 291, 1286, 544, 420, 1570, 10447, 294, 300, 3513, 11, 291, 393, 7979, 264, 2316, 13, 51114, 51114, 492, 611, 1027, 1071, 3278, 3541, 689, 934, 1228, 264, 1411, 558, 10298, 382, 264, 2370, 16235, 1465, 3170, 11, 321, 2956, 337, 257, 1150, 3513, 300, 575, 1090, 5893, 1674, 365, 264, 16235, 370, 300, 321, 727, 652, 1293, 35387, 17641, 44745, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0885332425435384, "compression_ratio": 1.7876447876447876, "no_speech_prob": 1.601655276317615e-05}, {"id": 258, "seek": 184700, "start": 1855.0, "end": 1862.0, "text": " And once you move more or less approximately in that direction, you can fool the model.", "tokens": [50364, 509, 500, 380, 643, 281, 915, 364, 1900, 15670, 294, 1901, 13, 509, 445, 643, 281, 915, 257, 3513, 300, 575, 257, 2416, 5893, 1674, 365, 264, 1465, 295, 264, 16235, 13, 50764, 50764, 400, 1564, 291, 1286, 544, 420, 1570, 10447, 294, 300, 3513, 11, 291, 393, 7979, 264, 2316, 13, 51114, 51114, 492, 611, 1027, 1071, 3278, 3541, 689, 934, 1228, 264, 1411, 558, 10298, 382, 264, 2370, 16235, 1465, 3170, 11, 321, 2956, 337, 257, 1150, 3513, 300, 575, 1090, 5893, 1674, 365, 264, 16235, 370, 300, 321, 727, 652, 1293, 35387, 17641, 44745, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0885332425435384, "compression_ratio": 1.7876447876447876, "no_speech_prob": 1.601655276317615e-05}, {"id": 259, "seek": 184700, "start": 1862.0, "end": 1876.0, "text": " We also made another cross section where after using the left right axis as the fast gradient sign method, we looked for a second direction that has high dot product with the gradient so that we could make both axes adversarial.", "tokens": [50364, 509, 500, 380, 643, 281, 915, 364, 1900, 15670, 294, 1901, 13, 509, 445, 643, 281, 915, 257, 3513, 300, 575, 257, 2416, 5893, 1674, 365, 264, 1465, 295, 264, 16235, 13, 50764, 50764, 400, 1564, 291, 1286, 544, 420, 1570, 10447, 294, 300, 3513, 11, 291, 393, 7979, 264, 2316, 13, 51114, 51114, 492, 611, 1027, 1071, 3278, 3541, 689, 934, 1228, 264, 1411, 558, 10298, 382, 264, 2370, 16235, 1465, 3170, 11, 321, 2956, 337, 257, 1150, 3513, 300, 575, 1090, 5893, 1674, 365, 264, 16235, 370, 300, 321, 727, 652, 1293, 35387, 17641, 44745, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0885332425435384, "compression_ratio": 1.7876447876447876, "no_speech_prob": 1.601655276317615e-05}, {"id": 260, "seek": 187600, "start": 1876.0, "end": 1891.0, "text": " In this case, we see that we get linear decision boundaries. They're now oriented diagonally rather than vertically, but we can see that there's actually this two dimensional subspace of adversarial examples that we can cross into.", "tokens": [50364, 682, 341, 1389, 11, 321, 536, 300, 321, 483, 8213, 3537, 13180, 13, 814, 434, 586, 21841, 17405, 379, 2831, 813, 28450, 11, 457, 321, 393, 536, 300, 456, 311, 767, 341, 732, 18795, 2090, 17940, 295, 17641, 44745, 5110, 300, 321, 393, 3278, 666, 13, 51114, 51114, 6288, 11, 309, 311, 1021, 281, 1604, 300, 17641, 44745, 5110, 366, 406, 5658, 13, 509, 393, 909, 257, 688, 295, 5658, 281, 364, 17641, 44745, 1365, 293, 309, 486, 1754, 17641, 44745, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08478406418201535, "compression_ratio": 1.7692307692307692, "no_speech_prob": 1.4593358173442539e-05}, {"id": 261, "seek": 187600, "start": 1891.0, "end": 1899.0, "text": " Finally, it's important to remember that adversarial examples are not noise. You can add a lot of noise to an adversarial example and it will stay adversarial.", "tokens": [50364, 682, 341, 1389, 11, 321, 536, 300, 321, 483, 8213, 3537, 13180, 13, 814, 434, 586, 21841, 17405, 379, 2831, 813, 28450, 11, 457, 321, 393, 536, 300, 456, 311, 767, 341, 732, 18795, 2090, 17940, 295, 17641, 44745, 5110, 300, 321, 393, 3278, 666, 13, 51114, 51114, 6288, 11, 309, 311, 1021, 281, 1604, 300, 17641, 44745, 5110, 366, 406, 5658, 13, 509, 393, 909, 257, 688, 295, 5658, 281, 364, 17641, 44745, 1365, 293, 309, 486, 1754, 17641, 44745, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08478406418201535, "compression_ratio": 1.7692307692307692, "no_speech_prob": 1.4593358173442539e-05}, {"id": 262, "seek": 189900, "start": 1899.0, "end": 1906.0, "text": " How do a lot of noise to a clean example? It'll stay clean. Here we make random cross sections where both axes are randomly chosen directions.", "tokens": [50364, 1012, 360, 257, 688, 295, 5658, 281, 257, 2541, 1365, 30, 467, 603, 1754, 2541, 13, 1692, 321, 652, 4974, 3278, 10863, 689, 1293, 35387, 366, 16979, 8614, 11095, 13, 50714, 50714, 400, 291, 536, 300, 322, 383, 19, 3279, 11, 881, 295, 264, 5438, 366, 2584, 2418, 11, 3620, 300, 436, 434, 8944, 20627, 281, 722, 365, 13, 51064, 51064, 400, 562, 291, 909, 5658, 11, 436, 1754, 8944, 20627, 13, 492, 611, 536, 300, 264, 2316, 1669, 512, 8038, 11, 570, 341, 307, 264, 1500, 992, 13, 51414, 51414, 400, 5101, 498, 257, 1500, 1365, 3719, 484, 3346, 11665, 2587, 11, 5127, 264, 5658, 1177, 380, 1319, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1429764457370924, "compression_ratio": 1.7544483985765125, "no_speech_prob": 2.7383513952372596e-05}, {"id": 263, "seek": 189900, "start": 1906.0, "end": 1913.0, "text": " And you see that on C410, most of the cells are completely white, meaning that they're correctly classified to start with.", "tokens": [50364, 1012, 360, 257, 688, 295, 5658, 281, 257, 2541, 1365, 30, 467, 603, 1754, 2541, 13, 1692, 321, 652, 4974, 3278, 10863, 689, 1293, 35387, 366, 16979, 8614, 11095, 13, 50714, 50714, 400, 291, 536, 300, 322, 383, 19, 3279, 11, 881, 295, 264, 5438, 366, 2584, 2418, 11, 3620, 300, 436, 434, 8944, 20627, 281, 722, 365, 13, 51064, 51064, 400, 562, 291, 909, 5658, 11, 436, 1754, 8944, 20627, 13, 492, 611, 536, 300, 264, 2316, 1669, 512, 8038, 11, 570, 341, 307, 264, 1500, 992, 13, 51414, 51414, 400, 5101, 498, 257, 1500, 1365, 3719, 484, 3346, 11665, 2587, 11, 5127, 264, 5658, 1177, 380, 1319, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1429764457370924, "compression_ratio": 1.7544483985765125, "no_speech_prob": 2.7383513952372596e-05}, {"id": 264, "seek": 189900, "start": 1913.0, "end": 1920.0, "text": " And when you add noise, they stay correctly classified. We also see that the model makes some mistakes, because this is the test set.", "tokens": [50364, 1012, 360, 257, 688, 295, 5658, 281, 257, 2541, 1365, 30, 467, 603, 1754, 2541, 13, 1692, 321, 652, 4974, 3278, 10863, 689, 1293, 35387, 366, 16979, 8614, 11095, 13, 50714, 50714, 400, 291, 536, 300, 322, 383, 19, 3279, 11, 881, 295, 264, 5438, 366, 2584, 2418, 11, 3620, 300, 436, 434, 8944, 20627, 281, 722, 365, 13, 51064, 51064, 400, 562, 291, 909, 5658, 11, 436, 1754, 8944, 20627, 13, 492, 611, 536, 300, 264, 2316, 1669, 512, 8038, 11, 570, 341, 307, 264, 1500, 992, 13, 51414, 51414, 400, 5101, 498, 257, 1500, 1365, 3719, 484, 3346, 11665, 2587, 11, 5127, 264, 5658, 1177, 380, 1319, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1429764457370924, "compression_ratio": 1.7544483985765125, "no_speech_prob": 2.7383513952372596e-05}, {"id": 265, "seek": 189900, "start": 1920.0, "end": 1925.0, "text": " And generally if a test example starts out misclassified, adding the noise doesn't change it.", "tokens": [50364, 1012, 360, 257, 688, 295, 5658, 281, 257, 2541, 1365, 30, 467, 603, 1754, 2541, 13, 1692, 321, 652, 4974, 3278, 10863, 689, 1293, 35387, 366, 16979, 8614, 11095, 13, 50714, 50714, 400, 291, 536, 300, 322, 383, 19, 3279, 11, 881, 295, 264, 5438, 366, 2584, 2418, 11, 3620, 300, 436, 434, 8944, 20627, 281, 722, 365, 13, 51064, 51064, 400, 562, 291, 909, 5658, 11, 436, 1754, 8944, 20627, 13, 492, 611, 536, 300, 264, 2316, 1669, 512, 8038, 11, 570, 341, 307, 264, 1500, 992, 13, 51414, 51414, 400, 5101, 498, 257, 1500, 1365, 3719, 484, 3346, 11665, 2587, 11, 5127, 264, 5658, 1177, 380, 1319, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1429764457370924, "compression_ratio": 1.7544483985765125, "no_speech_prob": 2.7383513952372596e-05}, {"id": 266, "seek": 192500, "start": 1925.0, "end": 1936.0, "text": " There are a few exceptions where if you look in the third row and third column, noise actually can make the model misclassify the example for especially large noise values.", "tokens": [50364, 821, 366, 257, 1326, 22847, 689, 498, 291, 574, 294, 264, 2636, 5386, 293, 2636, 7738, 11, 5658, 767, 393, 652, 264, 2316, 3346, 11665, 2505, 264, 1365, 337, 2318, 2416, 5658, 4190, 13, 50914, 50914, 400, 456, 311, 754, 4079, 294, 264, 1192, 5386, 11, 456, 311, 472, 1365, 291, 393, 536, 689, 264, 2316, 307, 3346, 11665, 5489, 264, 1500, 1365, 281, 722, 365, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15400736096879125, "compression_ratio": 1.729281767955801, "no_speech_prob": 1.7416226910427213e-05}, {"id": 267, "seek": 192500, "start": 1936.0, "end": 1945.0, "text": " And there's even somewhere in the top row, there's one example you can see where the model is misclassifying the test example to start with.", "tokens": [50364, 821, 366, 257, 1326, 22847, 689, 498, 291, 574, 294, 264, 2636, 5386, 293, 2636, 7738, 11, 5658, 767, 393, 652, 264, 2316, 3346, 11665, 2505, 264, 1365, 337, 2318, 2416, 5658, 4190, 13, 50914, 50914, 400, 456, 311, 754, 4079, 294, 264, 1192, 5386, 11, 456, 311, 472, 1365, 291, 393, 536, 689, 264, 2316, 307, 3346, 11665, 5489, 264, 1500, 1365, 281, 722, 365, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15400736096879125, "compression_ratio": 1.729281767955801, "no_speech_prob": 1.7416226910427213e-05}, {"id": 268, "seek": 194500, "start": 1945.0, "end": 1956.0, "text": " And then the noise is changing to be correctly classified. But for the most part, noise has very little effect on the classification decision compared to adversarial examples.", "tokens": [50364, 400, 550, 264, 5658, 307, 4473, 281, 312, 8944, 20627, 13, 583, 337, 264, 881, 644, 11, 5658, 575, 588, 707, 1802, 322, 264, 21538, 3537, 5347, 281, 17641, 44745, 5110, 13, 50914, 50914, 708, 311, 516, 322, 510, 307, 300, 294, 1090, 18795, 7673, 11, 498, 291, 2826, 512, 6408, 8062, 293, 550, 291, 2826, 257, 4974, 8062, 294, 300, 1090, 18795, 1901, 11, 264, 4974, 8062, 486, 322, 4274, 362, 4018, 5893, 1674, 365, 264, 6408, 8062, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.23938381104242235, "compression_ratio": 1.798283261802575, "no_speech_prob": 3.4837546991184354e-05}, {"id": 269, "seek": 194500, "start": 1956.0, "end": 1972.0, "text": " What's going on here is that in high dimensional spaces, if you choose some reference vector and then you choose a random vector in that high dimensional space, the random vector will on average have zero dot product with the reference vector.", "tokens": [50364, 400, 550, 264, 5658, 307, 4473, 281, 312, 8944, 20627, 13, 583, 337, 264, 881, 644, 11, 5658, 575, 588, 707, 1802, 322, 264, 21538, 3537, 5347, 281, 17641, 44745, 5110, 13, 50914, 50914, 708, 311, 516, 322, 510, 307, 300, 294, 1090, 18795, 7673, 11, 498, 291, 2826, 512, 6408, 8062, 293, 550, 291, 2826, 257, 4974, 8062, 294, 300, 1090, 18795, 1901, 11, 264, 4974, 8062, 486, 322, 4274, 362, 4018, 5893, 1674, 365, 264, 6408, 8062, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.23938381104242235, "compression_ratio": 1.798283261802575, "no_speech_prob": 3.4837546991184354e-05}, {"id": 270, "seek": 197200, "start": 1972.0, "end": 1986.0, "text": " So, if you take a first order Taylor series approximation of your cost and thinking about how your Taylor series approximation predicts that random vectors will change your cost, you see that random vectors on average have no effect on the cost.", "tokens": [50364, 407, 11, 498, 291, 747, 257, 700, 1668, 12060, 2638, 28023, 295, 428, 2063, 293, 1953, 466, 577, 428, 12060, 2638, 28023, 6069, 82, 300, 4974, 18875, 486, 1319, 428, 2063, 11, 291, 536, 300, 4974, 18875, 322, 4274, 362, 572, 1802, 322, 264, 2063, 13, 51064, 51064, 583, 17641, 44745, 5110, 366, 8614, 281, 19874, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.18681853817355248, "compression_ratio": 1.697142857142857, "no_speech_prob": 2.4687802579137497e-05}, {"id": 271, "seek": 197200, "start": 1986.0, "end": 1992.0, "text": " But adversarial examples are chosen to maximize it.", "tokens": [50364, 407, 11, 498, 291, 747, 257, 700, 1668, 12060, 2638, 28023, 295, 428, 2063, 293, 1953, 466, 577, 428, 12060, 2638, 28023, 6069, 82, 300, 4974, 18875, 486, 1319, 428, 2063, 11, 291, 536, 300, 4974, 18875, 322, 4274, 362, 572, 1802, 322, 264, 2063, 13, 51064, 51064, 583, 17641, 44745, 5110, 366, 8614, 281, 19874, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.18681853817355248, "compression_ratio": 1.697142857142857, "no_speech_prob": 2.4687802579137497e-05}, {"id": 272, "seek": 199200, "start": 1992.0, "end": 2007.0, "text": " So, we looked in two dimensions, more recently, Florio Chamey here at Stanford got interested in finding out just how many dimensions there are to these subspaces where the adversarial examples lie in a thick contiguous region.", "tokens": [50364, 407, 11, 321, 2956, 294, 732, 12819, 11, 544, 3938, 11, 8328, 1004, 761, 529, 88, 510, 412, 20374, 658, 3102, 294, 5006, 484, 445, 577, 867, 12819, 456, 366, 281, 613, 2090, 79, 2116, 689, 264, 17641, 44745, 5110, 4544, 294, 257, 5060, 660, 30525, 4458, 13, 51114, 51114, 400, 321, 1361, 493, 365, 364, 9284, 1214, 689, 291, 767, 574, 337, 2940, 819, 41488, 18875, 300, 439, 362, 257, 2416, 5893, 1674, 365, 264, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1846259628854147, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.000125071412185207}, {"id": 273, "seek": 199200, "start": 2007.0, "end": 2017.0, "text": " And we came up with an algorithm together where you actually look for several different orthogonal vectors that all have a large dot product with the gradient.", "tokens": [50364, 407, 11, 321, 2956, 294, 732, 12819, 11, 544, 3938, 11, 8328, 1004, 761, 529, 88, 510, 412, 20374, 658, 3102, 294, 5006, 484, 445, 577, 867, 12819, 456, 366, 281, 613, 2090, 79, 2116, 689, 264, 17641, 44745, 5110, 4544, 294, 257, 5060, 660, 30525, 4458, 13, 51114, 51114, 400, 321, 1361, 493, 365, 364, 9284, 1214, 689, 291, 767, 574, 337, 2940, 819, 41488, 18875, 300, 439, 362, 257, 2416, 5893, 1674, 365, 264, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1846259628854147, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.000125071412185207}, {"id": 274, "seek": 201700, "start": 2017.0, "end": 2027.0, "text": " By looking in several different orthogonal directions simultaneously, we could map out this kind of polytope where many different adversarial examples live.", "tokens": [50364, 3146, 1237, 294, 2940, 819, 41488, 11095, 16561, 11, 321, 727, 4471, 484, 341, 733, 295, 6754, 83, 1114, 689, 867, 819, 17641, 44745, 5110, 1621, 13, 50864, 50864, 492, 1352, 484, 300, 341, 17641, 44745, 4458, 575, 322, 4274, 466, 3552, 12819, 13, 759, 291, 574, 412, 819, 5110, 11, 291, 603, 915, 819, 3547, 295, 17641, 44745, 12819, 11, 457, 322, 4274, 322, 376, 45, 19756, 11, 321, 1352, 309, 307, 466, 3552, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.158933424949646, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.3365551239985507e-05}, {"id": 275, "seek": 201700, "start": 2027.0, "end": 2041.0, "text": " We found out that this adversarial region has on average about 25 dimensions. If you look at different examples, you'll find different numbers of adversarial dimensions, but on average on MNIST, we found it is about 25.", "tokens": [50364, 3146, 1237, 294, 2940, 819, 41488, 11095, 16561, 11, 321, 727, 4471, 484, 341, 733, 295, 6754, 83, 1114, 689, 867, 819, 17641, 44745, 5110, 1621, 13, 50864, 50864, 492, 1352, 484, 300, 341, 17641, 44745, 4458, 575, 322, 4274, 466, 3552, 12819, 13, 759, 291, 574, 412, 819, 5110, 11, 291, 603, 915, 819, 3547, 295, 17641, 44745, 12819, 11, 457, 322, 4274, 322, 376, 45, 19756, 11, 321, 1352, 309, 307, 466, 3552, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.158933424949646, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.3365551239985507e-05}, {"id": 276, "seek": 204100, "start": 2041.0, "end": 2050.0, "text": " Interesting here is the dimensionality actually tells you something about how likely you are to find an adversarial example by generating random noise.", "tokens": [50364, 14711, 510, 307, 264, 10139, 1860, 767, 5112, 291, 746, 466, 577, 3700, 291, 366, 281, 915, 364, 17641, 44745, 1365, 538, 17746, 4974, 5658, 13, 50814, 50814, 759, 633, 3513, 645, 17641, 44745, 11, 550, 604, 1319, 576, 3082, 552, 382, 21538, 13, 51114, 51114, 759, 881, 295, 264, 11095, 645, 17641, 44745, 11, 550, 4974, 11095, 576, 917, 493, 885, 17641, 44745, 445, 538, 6398, 881, 295, 264, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09181608024396394, "compression_ratio": 1.784688995215311, "no_speech_prob": 2.5514827939332463e-05}, {"id": 277, "seek": 204100, "start": 2050.0, "end": 2056.0, "text": " If every direction were adversarial, then any change would cause them as classification.", "tokens": [50364, 14711, 510, 307, 264, 10139, 1860, 767, 5112, 291, 746, 466, 577, 3700, 291, 366, 281, 915, 364, 17641, 44745, 1365, 538, 17746, 4974, 5658, 13, 50814, 50814, 759, 633, 3513, 645, 17641, 44745, 11, 550, 604, 1319, 576, 3082, 552, 382, 21538, 13, 51114, 51114, 759, 881, 295, 264, 11095, 645, 17641, 44745, 11, 550, 4974, 11095, 576, 917, 493, 885, 17641, 44745, 445, 538, 6398, 881, 295, 264, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09181608024396394, "compression_ratio": 1.784688995215311, "no_speech_prob": 2.5514827939332463e-05}, {"id": 278, "seek": 204100, "start": 2056.0, "end": 2064.0, "text": " If most of the directions were adversarial, then random directions would end up being adversarial just by accident most of the time.", "tokens": [50364, 14711, 510, 307, 264, 10139, 1860, 767, 5112, 291, 746, 466, 577, 3700, 291, 366, 281, 915, 364, 17641, 44745, 1365, 538, 17746, 4974, 5658, 13, 50814, 50814, 759, 633, 3513, 645, 17641, 44745, 11, 550, 604, 1319, 576, 3082, 552, 382, 21538, 13, 51114, 51114, 759, 881, 295, 264, 11095, 645, 17641, 44745, 11, 550, 4974, 11095, 576, 917, 493, 885, 17641, 44745, 445, 538, 6398, 881, 295, 264, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09181608024396394, "compression_ratio": 1.784688995215311, "no_speech_prob": 2.5514827939332463e-05}, {"id": 279, "seek": 206400, "start": 2064.0, "end": 2075.0, "text": " And then if there was only one adversarial direction, you'd almost never find that direction just by adding random noise. When there's 25, you have a chance of doing it sometimes.", "tokens": [50364, 400, 550, 498, 456, 390, 787, 472, 17641, 44745, 3513, 11, 291, 1116, 1920, 1128, 915, 300, 3513, 445, 538, 5127, 4974, 5658, 13, 1133, 456, 311, 3552, 11, 291, 362, 257, 2931, 295, 884, 309, 2171, 13, 50914, 50914, 3996, 1880, 551, 307, 300, 819, 5245, 486, 2049, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 13, 51214, 51214, 440, 2090, 17940, 10139, 1860, 295, 264, 17641, 44745, 2090, 17940, 16155, 281, 300, 5003, 4707, 13, 440, 4833, 264, 10139, 1860, 295, 264, 2090, 17940, 11, 264, 544, 3700, 309, 307, 300, 264, 2090, 79, 2116, 337, 732, 5245, 486, 27815, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05693773911378094, "compression_ratio": 1.8432835820895523, "no_speech_prob": 9.5192499429686e-06}, {"id": 280, "seek": 206400, "start": 2075.0, "end": 2081.0, "text": " Another interesting thing is that different models will often misclassify the same adversarial examples.", "tokens": [50364, 400, 550, 498, 456, 390, 787, 472, 17641, 44745, 3513, 11, 291, 1116, 1920, 1128, 915, 300, 3513, 445, 538, 5127, 4974, 5658, 13, 1133, 456, 311, 3552, 11, 291, 362, 257, 2931, 295, 884, 309, 2171, 13, 50914, 50914, 3996, 1880, 551, 307, 300, 819, 5245, 486, 2049, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 13, 51214, 51214, 440, 2090, 17940, 10139, 1860, 295, 264, 17641, 44745, 2090, 17940, 16155, 281, 300, 5003, 4707, 13, 440, 4833, 264, 10139, 1860, 295, 264, 2090, 17940, 11, 264, 544, 3700, 309, 307, 300, 264, 2090, 79, 2116, 337, 732, 5245, 486, 27815, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05693773911378094, "compression_ratio": 1.8432835820895523, "no_speech_prob": 9.5192499429686e-06}, {"id": 281, "seek": 206400, "start": 2081.0, "end": 2093.0, "text": " The subspace dimensionality of the adversarial subspace relates to that transfer property. The larger the dimensionality of the subspace, the more likely it is that the subspaces for two models will intersect.", "tokens": [50364, 400, 550, 498, 456, 390, 787, 472, 17641, 44745, 3513, 11, 291, 1116, 1920, 1128, 915, 300, 3513, 445, 538, 5127, 4974, 5658, 13, 1133, 456, 311, 3552, 11, 291, 362, 257, 2931, 295, 884, 309, 2171, 13, 50914, 50914, 3996, 1880, 551, 307, 300, 819, 5245, 486, 2049, 3346, 11665, 2505, 264, 912, 17641, 44745, 5110, 13, 51214, 51214, 440, 2090, 17940, 10139, 1860, 295, 264, 17641, 44745, 2090, 17940, 16155, 281, 300, 5003, 4707, 13, 440, 4833, 264, 10139, 1860, 295, 264, 2090, 17940, 11, 264, 544, 3700, 309, 307, 300, 264, 2090, 79, 2116, 337, 732, 5245, 486, 27815, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05693773911378094, "compression_ratio": 1.8432835820895523, "no_speech_prob": 9.5192499429686e-06}, {"id": 282, "seek": 209300, "start": 2093.0, "end": 2101.0, "text": " So if you have two different models that have a very large adversarial subspace, you know that you can probably transfer adversarial examples from one to the other.", "tokens": [50364, 407, 498, 291, 362, 732, 819, 5245, 300, 362, 257, 588, 2416, 17641, 44745, 2090, 17940, 11, 291, 458, 300, 291, 393, 1391, 5003, 17641, 44745, 5110, 490, 472, 281, 264, 661, 13, 50764, 50764, 583, 498, 641, 17641, 44745, 2090, 79, 2116, 588, 1359, 11, 550, 5969, 456, 311, 512, 733, 295, 534, 27249, 1802, 19030, 552, 281, 2073, 2293, 264, 912, 2090, 17940, 11, 51214, 51214, 309, 2544, 1570, 3700, 300, 291, 603, 312, 1075, 281, 5003, 5110, 445, 3462, 281, 264, 2090, 79, 2116, 16979, 419, 9676, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07580197484869707, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00013230839977040887}, {"id": 283, "seek": 209300, "start": 2101.0, "end": 2110.0, "text": " But if their adversarial subspaces very small, then unless there's some kind of really systematic effect forcing them to share exactly the same subspace,", "tokens": [50364, 407, 498, 291, 362, 732, 819, 5245, 300, 362, 257, 588, 2416, 17641, 44745, 2090, 17940, 11, 291, 458, 300, 291, 393, 1391, 5003, 17641, 44745, 5110, 490, 472, 281, 264, 661, 13, 50764, 50764, 583, 498, 641, 17641, 44745, 2090, 79, 2116, 588, 1359, 11, 550, 5969, 456, 311, 512, 733, 295, 534, 27249, 1802, 19030, 552, 281, 2073, 2293, 264, 912, 2090, 17940, 11, 51214, 51214, 309, 2544, 1570, 3700, 300, 291, 603, 312, 1075, 281, 5003, 5110, 445, 3462, 281, 264, 2090, 79, 2116, 16979, 419, 9676, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07580197484869707, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00013230839977040887}, {"id": 284, "seek": 209300, "start": 2110.0, "end": 2118.0, "text": " it seems less likely that you'll be able to transfer examples just due to the subspaces randomly aligning.", "tokens": [50364, 407, 498, 291, 362, 732, 819, 5245, 300, 362, 257, 588, 2416, 17641, 44745, 2090, 17940, 11, 291, 458, 300, 291, 393, 1391, 5003, 17641, 44745, 5110, 490, 472, 281, 264, 661, 13, 50764, 50764, 583, 498, 641, 17641, 44745, 2090, 79, 2116, 588, 1359, 11, 550, 5969, 456, 311, 512, 733, 295, 534, 27249, 1802, 19030, 552, 281, 2073, 2293, 264, 912, 2090, 17940, 11, 51214, 51214, 309, 2544, 1570, 3700, 300, 291, 603, 312, 1075, 281, 5003, 5110, 445, 3462, 281, 264, 2090, 79, 2116, 16979, 419, 9676, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07580197484869707, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00013230839977040887}, {"id": 285, "seek": 211800, "start": 2118.0, "end": 2131.0, "text": " A lot of the time in the adversarial example research community, we refer back to the story of Clever Hans. This comes from an essay by Bob Stern called Clever Hans Clever Algorithms,", "tokens": [50364, 316, 688, 295, 264, 565, 294, 264, 17641, 44745, 1365, 2132, 1768, 11, 321, 2864, 646, 281, 264, 1657, 295, 8834, 331, 17926, 13, 639, 1487, 490, 364, 16238, 538, 6085, 39538, 1219, 8834, 331, 17926, 8834, 331, 35014, 6819, 2592, 11, 51014, 51014, 570, 8834, 331, 17926, 307, 257, 1238, 665, 19157, 337, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 51264, 51264, 407, 8834, 331, 17926, 390, 257, 6832, 300, 5152, 294, 264, 2440, 28898, 82, 13, 2812, 7289, 8895, 796, 281, 360, 42973, 2740, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10757037867670474, "compression_ratio": 1.6090534979423867, "no_speech_prob": 1.8261174773215316e-05}, {"id": 286, "seek": 211800, "start": 2131.0, "end": 2136.0, "text": " because Clever Hans is a pretty good metaphor for what's happening with machine learning algorithms.", "tokens": [50364, 316, 688, 295, 264, 565, 294, 264, 17641, 44745, 1365, 2132, 1768, 11, 321, 2864, 646, 281, 264, 1657, 295, 8834, 331, 17926, 13, 639, 1487, 490, 364, 16238, 538, 6085, 39538, 1219, 8834, 331, 17926, 8834, 331, 35014, 6819, 2592, 11, 51014, 51014, 570, 8834, 331, 17926, 307, 257, 1238, 665, 19157, 337, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 51264, 51264, 407, 8834, 331, 17926, 390, 257, 6832, 300, 5152, 294, 264, 2440, 28898, 82, 13, 2812, 7289, 8895, 796, 281, 360, 42973, 2740, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10757037867670474, "compression_ratio": 1.6090534979423867, "no_speech_prob": 1.8261174773215316e-05}, {"id": 287, "seek": 211800, "start": 2136.0, "end": 2144.0, "text": " So Clever Hans was a horse that lived in the early 1900s. His owner trained him to do arithmetic problems.", "tokens": [50364, 316, 688, 295, 264, 565, 294, 264, 17641, 44745, 1365, 2132, 1768, 11, 321, 2864, 646, 281, 264, 1657, 295, 8834, 331, 17926, 13, 639, 1487, 490, 364, 16238, 538, 6085, 39538, 1219, 8834, 331, 17926, 8834, 331, 35014, 6819, 2592, 11, 51014, 51014, 570, 8834, 331, 17926, 307, 257, 1238, 665, 19157, 337, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 51264, 51264, 407, 8834, 331, 17926, 390, 257, 6832, 300, 5152, 294, 264, 2440, 28898, 82, 13, 2812, 7289, 8895, 796, 281, 360, 42973, 2740, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10757037867670474, "compression_ratio": 1.6090534979423867, "no_speech_prob": 1.8261174773215316e-05}, {"id": 288, "seek": 214400, "start": 2144.0, "end": 2153.0, "text": " So you could ask him Clever Hans, what's two plus one? And he would answer by tapping his hoof.", "tokens": [50364, 407, 291, 727, 1029, 796, 8834, 331, 17926, 11, 437, 311, 732, 1804, 472, 30, 400, 415, 576, 1867, 538, 21444, 702, 44974, 13, 50814, 50814, 400, 934, 264, 2636, 5119, 11, 2201, 576, 722, 11060, 293, 19978, 293, 1237, 2919, 570, 415, 1116, 767, 1096, 364, 42973, 1154, 13, 51164, 51164, 1042, 11, 309, 311, 2198, 300, 415, 8782, 380, 767, 3264, 281, 360, 42973, 11, 457, 309, 390, 767, 1238, 1152, 281, 2573, 484, 437, 390, 516, 322, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11868400573730468, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.700259291392285e-06}, {"id": 289, "seek": 214400, "start": 2153.0, "end": 2160.0, "text": " And after the third tap, everybody would start cheering and clapping and looking excited because he'd actually done an arithmetic problem.", "tokens": [50364, 407, 291, 727, 1029, 796, 8834, 331, 17926, 11, 437, 311, 732, 1804, 472, 30, 400, 415, 576, 1867, 538, 21444, 702, 44974, 13, 50814, 50814, 400, 934, 264, 2636, 5119, 11, 2201, 576, 722, 11060, 293, 19978, 293, 1237, 2919, 570, 415, 1116, 767, 1096, 364, 42973, 1154, 13, 51164, 51164, 1042, 11, 309, 311, 2198, 300, 415, 8782, 380, 767, 3264, 281, 360, 42973, 11, 457, 309, 390, 767, 1238, 1152, 281, 2573, 484, 437, 390, 516, 322, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11868400573730468, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.700259291392285e-06}, {"id": 290, "seek": 214400, "start": 2160.0, "end": 2167.0, "text": " Well, it's heard that he hadn't actually learned to do arithmetic, but it was actually pretty hard to figure out what was going on.", "tokens": [50364, 407, 291, 727, 1029, 796, 8834, 331, 17926, 11, 437, 311, 732, 1804, 472, 30, 400, 415, 576, 1867, 538, 21444, 702, 44974, 13, 50814, 50814, 400, 934, 264, 2636, 5119, 11, 2201, 576, 722, 11060, 293, 19978, 293, 1237, 2919, 570, 415, 1116, 767, 1096, 364, 42973, 1154, 13, 51164, 51164, 1042, 11, 309, 311, 2198, 300, 415, 8782, 380, 767, 3264, 281, 360, 42973, 11, 457, 309, 390, 767, 1238, 1152, 281, 2573, 484, 437, 390, 516, 322, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11868400573730468, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.700259291392285e-06}, {"id": 291, "seek": 216700, "start": 2167.0, "end": 2178.0, "text": " His owner was not trying to defried anybody. His owner actually believed he could do arithmetic. And presumably Clever Hans himself was not trying to trick anybody.", "tokens": [50364, 2812, 7289, 390, 406, 1382, 281, 1060, 2428, 4472, 13, 2812, 7289, 767, 7847, 415, 727, 360, 42973, 13, 400, 26742, 8834, 331, 17926, 3647, 390, 406, 1382, 281, 4282, 4472, 13, 50914, 50914, 583, 4728, 257, 29514, 30972, 796, 293, 1352, 300, 498, 415, 390, 829, 294, 257, 1808, 3312, 1553, 364, 4034, 11, 51314, 51314, 293, 264, 954, 3365, 264, 1651, 11, 1516, 257, 6094, 11, 415, 2809, 380, 2573, 484, 562, 281, 1590, 21444, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13504965712384479, "compression_ratio": 1.6238938053097345, "no_speech_prob": 8.622868335805833e-06}, {"id": 292, "seek": 216700, "start": 2178.0, "end": 2186.0, "text": " But eventually a psychologist examined him and found that if he was put in a room alone without an audience,", "tokens": [50364, 2812, 7289, 390, 406, 1382, 281, 1060, 2428, 4472, 13, 2812, 7289, 767, 7847, 415, 727, 360, 42973, 13, 400, 26742, 8834, 331, 17926, 3647, 390, 406, 1382, 281, 4282, 4472, 13, 50914, 50914, 583, 4728, 257, 29514, 30972, 796, 293, 1352, 300, 498, 415, 390, 829, 294, 257, 1808, 3312, 1553, 364, 4034, 11, 51314, 51314, 293, 264, 954, 3365, 264, 1651, 11, 1516, 257, 6094, 11, 415, 2809, 380, 2573, 484, 562, 281, 1590, 21444, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13504965712384479, "compression_ratio": 1.6238938053097345, "no_speech_prob": 8.622868335805833e-06}, {"id": 293, "seek": 216700, "start": 2186.0, "end": 2191.0, "text": " and the person asking the questions, war a mask, he couldn't figure out when to stop tapping.", "tokens": [50364, 2812, 7289, 390, 406, 1382, 281, 1060, 2428, 4472, 13, 2812, 7289, 767, 7847, 415, 727, 360, 42973, 13, 400, 26742, 8834, 331, 17926, 3647, 390, 406, 1382, 281, 4282, 4472, 13, 50914, 50914, 583, 4728, 257, 29514, 30972, 796, 293, 1352, 300, 498, 415, 390, 829, 294, 257, 1808, 3312, 1553, 364, 4034, 11, 51314, 51314, 293, 264, 954, 3365, 264, 1651, 11, 1516, 257, 6094, 11, 415, 2809, 380, 2573, 484, 562, 281, 1590, 21444, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13504965712384479, "compression_ratio": 1.6238938053097345, "no_speech_prob": 8.622868335805833e-06}, {"id": 294, "seek": 219100, "start": 2191.0, "end": 2202.0, "text": " You'd ask him Clever Hans, what's one plus one? And he'd just keep staring at your face waiting for you to give him some sign that he was done tapping.", "tokens": [50364, 509, 1116, 1029, 796, 8834, 331, 17926, 11, 437, 311, 472, 1804, 472, 30, 400, 415, 1116, 445, 1066, 18043, 412, 428, 1851, 3806, 337, 291, 281, 976, 796, 512, 1465, 300, 415, 390, 1096, 21444, 13, 50914, 50914, 407, 2201, 294, 341, 2590, 390, 1382, 281, 360, 264, 558, 551, 13, 8834, 331, 17926, 307, 1382, 281, 360, 2035, 309, 1890, 281, 483, 264, 10606, 300, 702, 7289, 576, 976, 796, 562, 415, 10103, 364, 42973, 1154, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0774287315736334, "compression_ratio": 1.6261682242990654, "no_speech_prob": 2.2319130948744714e-05}, {"id": 295, "seek": 219100, "start": 2202.0, "end": 2213.0, "text": " So everybody in this situation was trying to do the right thing. Clever Hans is trying to do whatever it took to get the apple that his owner would give him when he answered an arithmetic problem.", "tokens": [50364, 509, 1116, 1029, 796, 8834, 331, 17926, 11, 437, 311, 472, 1804, 472, 30, 400, 415, 1116, 445, 1066, 18043, 412, 428, 1851, 3806, 337, 291, 281, 976, 796, 512, 1465, 300, 415, 390, 1096, 21444, 13, 50914, 50914, 407, 2201, 294, 341, 2590, 390, 1382, 281, 360, 264, 558, 551, 13, 8834, 331, 17926, 307, 1382, 281, 360, 2035, 309, 1890, 281, 483, 264, 10606, 300, 702, 7289, 576, 976, 796, 562, 415, 10103, 364, 42973, 1154, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0774287315736334, "compression_ratio": 1.6261682242990654, "no_speech_prob": 2.2319130948744714e-05}, {"id": 296, "seek": 221300, "start": 2213.0, "end": 2221.0, "text": " His owner did his best to train him correctly with real arithmetic questions and real rewards for correct answers.", "tokens": [50364, 2812, 7289, 630, 702, 1151, 281, 3847, 796, 8944, 365, 957, 42973, 1651, 293, 957, 17203, 337, 3006, 6338, 13, 50764, 50764, 400, 437, 2011, 390, 300, 8834, 331, 17926, 49152, 2276, 5178, 322, 264, 2085, 18639, 13, 51064, 51064, 634, 1352, 341, 18639, 295, 561, 311, 2093, 12215, 300, 727, 49927, 854, 796, 5039, 264, 1154, 13, 51364, 51364, 583, 550, 309, 994, 380, 2674, 1125, 281, 257, 1500, 992, 689, 291, 22062, 1890, 300, 18639, 1314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07180779813283897, "compression_ratio": 1.6050420168067228, "no_speech_prob": 2.299254629178904e-05}, {"id": 297, "seek": 221300, "start": 2221.0, "end": 2227.0, "text": " And what happened was that Clever Hans inadvertently focused on the wrong queue.", "tokens": [50364, 2812, 7289, 630, 702, 1151, 281, 3847, 796, 8944, 365, 957, 42973, 1651, 293, 957, 17203, 337, 3006, 6338, 13, 50764, 50764, 400, 437, 2011, 390, 300, 8834, 331, 17926, 49152, 2276, 5178, 322, 264, 2085, 18639, 13, 51064, 51064, 634, 1352, 341, 18639, 295, 561, 311, 2093, 12215, 300, 727, 49927, 854, 796, 5039, 264, 1154, 13, 51364, 51364, 583, 550, 309, 994, 380, 2674, 1125, 281, 257, 1500, 992, 689, 291, 22062, 1890, 300, 18639, 1314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07180779813283897, "compression_ratio": 1.6050420168067228, "no_speech_prob": 2.299254629178904e-05}, {"id": 298, "seek": 221300, "start": 2227.0, "end": 2233.0, "text": " He found this queue of people's social reactions that could reliably help him solve the problem.", "tokens": [50364, 2812, 7289, 630, 702, 1151, 281, 3847, 796, 8944, 365, 957, 42973, 1651, 293, 957, 17203, 337, 3006, 6338, 13, 50764, 50764, 400, 437, 2011, 390, 300, 8834, 331, 17926, 49152, 2276, 5178, 322, 264, 2085, 18639, 13, 51064, 51064, 634, 1352, 341, 18639, 295, 561, 311, 2093, 12215, 300, 727, 49927, 854, 796, 5039, 264, 1154, 13, 51364, 51364, 583, 550, 309, 994, 380, 2674, 1125, 281, 257, 1500, 992, 689, 291, 22062, 1890, 300, 18639, 1314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07180779813283897, "compression_ratio": 1.6050420168067228, "no_speech_prob": 2.299254629178904e-05}, {"id": 299, "seek": 221300, "start": 2233.0, "end": 2238.0, "text": " But then it didn't generalize to a test set where you intentionally took that queue away.", "tokens": [50364, 2812, 7289, 630, 702, 1151, 281, 3847, 796, 8944, 365, 957, 42973, 1651, 293, 957, 17203, 337, 3006, 6338, 13, 50764, 50764, 400, 437, 2011, 390, 300, 8834, 331, 17926, 49152, 2276, 5178, 322, 264, 2085, 18639, 13, 51064, 51064, 634, 1352, 341, 18639, 295, 561, 311, 2093, 12215, 300, 727, 49927, 854, 796, 5039, 264, 1154, 13, 51364, 51364, 583, 550, 309, 994, 380, 2674, 1125, 281, 257, 1500, 992, 689, 291, 22062, 1890, 300, 18639, 1314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07180779813283897, "compression_ratio": 1.6050420168067228, "no_speech_prob": 2.299254629178904e-05}, {"id": 300, "seek": 223800, "start": 2238.0, "end": 2244.0, "text": " It did generalize to a naturally occurring test set where he had an audience.", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 301, "seek": 223800, "start": 2244.0, "end": 2247.0, "text": " So that's more or less what's happening with machine learning algorithms.", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 302, "seek": 223800, "start": 2247.0, "end": 2251.0, "text": " They found these very linear patterns that can fit the training data.", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 303, "seek": 223800, "start": 2251.0, "end": 2255.0, "text": " And these linear patterns even generalize to the test data.", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 304, "seek": 223800, "start": 2255.0, "end": 2261.0, "text": " They've learned to handle any example that comes from the same distribution as their training data.", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 305, "seek": 223800, "start": 2261.0, "end": 2264.0, "text": " But then if you shift the distribution that you test them on,", "tokens": [50364, 467, 630, 2674, 1125, 281, 257, 8195, 18386, 1500, 992, 689, 415, 632, 364, 4034, 13, 50664, 50664, 407, 300, 311, 544, 420, 1570, 437, 311, 2737, 365, 3479, 2539, 14642, 13, 50814, 50814, 814, 1352, 613, 588, 8213, 8294, 300, 393, 3318, 264, 3097, 1412, 13, 51014, 51014, 400, 613, 8213, 8294, 754, 2674, 1125, 281, 264, 1500, 1412, 13, 51214, 51214, 814, 600, 3264, 281, 4813, 604, 1365, 300, 1487, 490, 264, 912, 7316, 382, 641, 3097, 1412, 13, 51514, 51514, 583, 550, 498, 291, 5513, 264, 7316, 300, 291, 1500, 552, 322, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050351123809814456, "compression_ratio": 1.7791164658634537, "no_speech_prob": 2.6589954359224066e-05}, {"id": 306, "seek": 226400, "start": 2264.0, "end": 2272.0, "text": " if a malicious adversary actually creates examples that are intended to fool them, they're very easily fooled.", "tokens": [50364, 498, 257, 33496, 48222, 767, 7829, 5110, 300, 366, 10226, 281, 7979, 552, 11, 436, 434, 588, 3612, 33372, 13, 50764, 50764, 682, 1186, 11, 321, 915, 300, 4363, 3479, 2539, 14642, 366, 2085, 1920, 5315, 13, 51014, 51014, 492, 3928, 281, 519, 295, 552, 382, 885, 3006, 881, 295, 264, 565, 570, 562, 321, 1190, 552, 322, 8195, 18386, 15743, 11, 51314, 51314, 436, 4584, 588, 1090, 14170, 42270, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05009362538655599, "compression_ratio": 1.6036036036036037, "no_speech_prob": 8.345245078089647e-06}, {"id": 307, "seek": 226400, "start": 2272.0, "end": 2277.0, "text": " In fact, we find that modern machine learning algorithms are wrong almost everywhere.", "tokens": [50364, 498, 257, 33496, 48222, 767, 7829, 5110, 300, 366, 10226, 281, 7979, 552, 11, 436, 434, 588, 3612, 33372, 13, 50764, 50764, 682, 1186, 11, 321, 915, 300, 4363, 3479, 2539, 14642, 366, 2085, 1920, 5315, 13, 51014, 51014, 492, 3928, 281, 519, 295, 552, 382, 885, 3006, 881, 295, 264, 565, 570, 562, 321, 1190, 552, 322, 8195, 18386, 15743, 11, 51314, 51314, 436, 4584, 588, 1090, 14170, 42270, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05009362538655599, "compression_ratio": 1.6036036036036037, "no_speech_prob": 8.345245078089647e-06}, {"id": 308, "seek": 226400, "start": 2277.0, "end": 2283.0, "text": " We tend to think of them as being correct most of the time because when we run them on naturally occurring inputs,", "tokens": [50364, 498, 257, 33496, 48222, 767, 7829, 5110, 300, 366, 10226, 281, 7979, 552, 11, 436, 434, 588, 3612, 33372, 13, 50764, 50764, 682, 1186, 11, 321, 915, 300, 4363, 3479, 2539, 14642, 366, 2085, 1920, 5315, 13, 51014, 51014, 492, 3928, 281, 519, 295, 552, 382, 885, 3006, 881, 295, 264, 565, 570, 562, 321, 1190, 552, 322, 8195, 18386, 15743, 11, 51314, 51314, 436, 4584, 588, 1090, 14170, 42270, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05009362538655599, "compression_ratio": 1.6036036036036037, "no_speech_prob": 8.345245078089647e-06}, {"id": 309, "seek": 226400, "start": 2283.0, "end": 2287.0, "text": " they achieve very high accuracy percentages.", "tokens": [50364, 498, 257, 33496, 48222, 767, 7829, 5110, 300, 366, 10226, 281, 7979, 552, 11, 436, 434, 588, 3612, 33372, 13, 50764, 50764, 682, 1186, 11, 321, 915, 300, 4363, 3479, 2539, 14642, 366, 2085, 1920, 5315, 13, 51014, 51014, 492, 3928, 281, 519, 295, 552, 382, 885, 3006, 881, 295, 264, 565, 570, 562, 321, 1190, 552, 322, 8195, 18386, 15743, 11, 51314, 51314, 436, 4584, 588, 1090, 14170, 42270, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05009362538655599, "compression_ratio": 1.6036036036036037, "no_speech_prob": 8.345245078089647e-06}, {"id": 310, "seek": 228700, "start": 2287.0, "end": 2298.0, "text": " But if we look instead of as the percentage of samples from an IID test set, if we look at the percentage of the space in RN that is correctly classified,", "tokens": [50364, 583, 498, 321, 574, 2602, 295, 382, 264, 9668, 295, 10938, 490, 364, 286, 2777, 1500, 992, 11, 498, 321, 574, 412, 264, 9668, 295, 264, 1901, 294, 497, 45, 300, 307, 8944, 20627, 11, 50914, 50914, 321, 915, 300, 264, 3346, 11665, 2587, 1920, 1203, 11, 293, 436, 15158, 23551, 787, 322, 257, 588, 5862, 47138, 11498, 264, 1412, 300, 321, 3847, 552, 322, 13, 51414, 51414, 682, 341, 7542, 11, 286, 855, 291, 2940, 819, 5110, 295, 39148, 5658, 300, 286, 600, 1190, 807, 257, 383, 19, 3279, 1508, 9902, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10657029299391914, "compression_ratio": 1.640316205533597, "no_speech_prob": 3.937244036933407e-05}, {"id": 311, "seek": 228700, "start": 2298.0, "end": 2308.0, "text": " we find that the misclassified almost everything, and they behave reasonably only on a very thin manifold surrounding the data that we train them on.", "tokens": [50364, 583, 498, 321, 574, 2602, 295, 382, 264, 9668, 295, 10938, 490, 364, 286, 2777, 1500, 992, 11, 498, 321, 574, 412, 264, 9668, 295, 264, 1901, 294, 497, 45, 300, 307, 8944, 20627, 11, 50914, 50914, 321, 915, 300, 264, 3346, 11665, 2587, 1920, 1203, 11, 293, 436, 15158, 23551, 787, 322, 257, 588, 5862, 47138, 11498, 264, 1412, 300, 321, 3847, 552, 322, 13, 51414, 51414, 682, 341, 7542, 11, 286, 855, 291, 2940, 819, 5110, 295, 39148, 5658, 300, 286, 600, 1190, 807, 257, 383, 19, 3279, 1508, 9902, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10657029299391914, "compression_ratio": 1.640316205533597, "no_speech_prob": 3.937244036933407e-05}, {"id": 312, "seek": 228700, "start": 2308.0, "end": 2316.0, "text": " In this plot, I show you several different examples of Gaussian noise that I've run through a C410 classifier.", "tokens": [50364, 583, 498, 321, 574, 2602, 295, 382, 264, 9668, 295, 10938, 490, 364, 286, 2777, 1500, 992, 11, 498, 321, 574, 412, 264, 9668, 295, 264, 1901, 294, 497, 45, 300, 307, 8944, 20627, 11, 50914, 50914, 321, 915, 300, 264, 3346, 11665, 2587, 1920, 1203, 11, 293, 436, 15158, 23551, 787, 322, 257, 588, 5862, 47138, 11498, 264, 1412, 300, 321, 3847, 552, 322, 13, 51414, 51414, 682, 341, 7542, 11, 286, 855, 291, 2940, 819, 5110, 295, 39148, 5658, 300, 286, 600, 1190, 807, 257, 383, 19, 3279, 1508, 9902, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10657029299391914, "compression_ratio": 1.640316205533597, "no_speech_prob": 3.937244036933407e-05}, {"id": 313, "seek": 231600, "start": 2316.0, "end": 2321.0, "text": " Everywhere that there's a pink box, the classifier thinks that there is something rather than nothing.", "tokens": [50364, 37322, 300, 456, 311, 257, 7022, 2424, 11, 264, 1508, 9902, 7309, 300, 456, 307, 746, 2831, 813, 1825, 13, 50614, 50614, 286, 603, 808, 646, 281, 437, 300, 1355, 294, 257, 1150, 13, 50764, 50764, 37322, 300, 456, 307, 257, 5566, 2424, 11, 472, 1823, 295, 264, 2370, 16235, 1465, 3170, 390, 1075, 281, 31781, 264, 2316, 300, 309, 390, 1237, 4682, 412, 364, 17130, 13, 51214, 51214, 286, 5111, 264, 17130, 1508, 570, 309, 390, 264, 472, 365, 264, 12437, 2245, 3314, 13, 51414, 51414, 467, 632, 466, 257, 3552, 4, 2245, 3314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08255748748779297, "compression_ratio": 1.7407407407407407, "no_speech_prob": 3.083505362155847e-05}, {"id": 314, "seek": 231600, "start": 2321.0, "end": 2324.0, "text": " I'll come back to what that means in a second.", "tokens": [50364, 37322, 300, 456, 311, 257, 7022, 2424, 11, 264, 1508, 9902, 7309, 300, 456, 307, 746, 2831, 813, 1825, 13, 50614, 50614, 286, 603, 808, 646, 281, 437, 300, 1355, 294, 257, 1150, 13, 50764, 50764, 37322, 300, 456, 307, 257, 5566, 2424, 11, 472, 1823, 295, 264, 2370, 16235, 1465, 3170, 390, 1075, 281, 31781, 264, 2316, 300, 309, 390, 1237, 4682, 412, 364, 17130, 13, 51214, 51214, 286, 5111, 264, 17130, 1508, 570, 309, 390, 264, 472, 365, 264, 12437, 2245, 3314, 13, 51414, 51414, 467, 632, 466, 257, 3552, 4, 2245, 3314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08255748748779297, "compression_ratio": 1.7407407407407407, "no_speech_prob": 3.083505362155847e-05}, {"id": 315, "seek": 231600, "start": 2324.0, "end": 2333.0, "text": " Everywhere that there is a yellow box, one step of the fast gradient sign method was able to persuade the model that it was looking specifically at an airplane.", "tokens": [50364, 37322, 300, 456, 311, 257, 7022, 2424, 11, 264, 1508, 9902, 7309, 300, 456, 307, 746, 2831, 813, 1825, 13, 50614, 50614, 286, 603, 808, 646, 281, 437, 300, 1355, 294, 257, 1150, 13, 50764, 50764, 37322, 300, 456, 307, 257, 5566, 2424, 11, 472, 1823, 295, 264, 2370, 16235, 1465, 3170, 390, 1075, 281, 31781, 264, 2316, 300, 309, 390, 1237, 4682, 412, 364, 17130, 13, 51214, 51214, 286, 5111, 264, 17130, 1508, 570, 309, 390, 264, 472, 365, 264, 12437, 2245, 3314, 13, 51414, 51414, 467, 632, 466, 257, 3552, 4, 2245, 3314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08255748748779297, "compression_ratio": 1.7407407407407407, "no_speech_prob": 3.083505362155847e-05}, {"id": 316, "seek": 231600, "start": 2333.0, "end": 2337.0, "text": " I chose the airplane class because it was the one with the lowest success rate.", "tokens": [50364, 37322, 300, 456, 311, 257, 7022, 2424, 11, 264, 1508, 9902, 7309, 300, 456, 307, 746, 2831, 813, 1825, 13, 50614, 50614, 286, 603, 808, 646, 281, 437, 300, 1355, 294, 257, 1150, 13, 50764, 50764, 37322, 300, 456, 307, 257, 5566, 2424, 11, 472, 1823, 295, 264, 2370, 16235, 1465, 3170, 390, 1075, 281, 31781, 264, 2316, 300, 309, 390, 1237, 4682, 412, 364, 17130, 13, 51214, 51214, 286, 5111, 264, 17130, 1508, 570, 309, 390, 264, 472, 365, 264, 12437, 2245, 3314, 13, 51414, 51414, 467, 632, 466, 257, 3552, 4, 2245, 3314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08255748748779297, "compression_ratio": 1.7407407407407407, "no_speech_prob": 3.083505362155847e-05}, {"id": 317, "seek": 231600, "start": 2337.0, "end": 2339.0, "text": " It had about a 25% success rate.", "tokens": [50364, 37322, 300, 456, 311, 257, 7022, 2424, 11, 264, 1508, 9902, 7309, 300, 456, 307, 746, 2831, 813, 1825, 13, 50614, 50614, 286, 603, 808, 646, 281, 437, 300, 1355, 294, 257, 1150, 13, 50764, 50764, 37322, 300, 456, 307, 257, 5566, 2424, 11, 472, 1823, 295, 264, 2370, 16235, 1465, 3170, 390, 1075, 281, 31781, 264, 2316, 300, 309, 390, 1237, 4682, 412, 364, 17130, 13, 51214, 51214, 286, 5111, 264, 17130, 1508, 570, 309, 390, 264, 472, 365, 264, 12437, 2245, 3314, 13, 51414, 51414, 467, 632, 466, 257, 3552, 4, 2245, 3314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08255748748779297, "compression_ratio": 1.7407407407407407, "no_speech_prob": 3.083505362155847e-05}, {"id": 318, "seek": 233900, "start": 2339.0, "end": 2347.0, "text": " It means an attacker would need four chances to get noise recognized as an airplane on this model.", "tokens": [50364, 467, 1355, 364, 35871, 576, 643, 1451, 10486, 281, 483, 5658, 9823, 382, 364, 17130, 322, 341, 2316, 13, 50764, 50764, 1107, 1880, 551, 293, 6854, 1547, 2212, 264, 1657, 295, 8834, 331, 17926, 11, 307, 300, 341, 2316, 1352, 300, 466, 5285, 4, 295, 497, 45, 390, 20627, 382, 257, 6832, 13, 51314, 51314, 407, 286, 2835, 300, 341, 2316, 486, 584, 300, 5658, 307, 746, 2831, 813, 1825, 11, 293, 309, 311, 767, 733, 295, 1021, 281, 519, 466, 577, 321, 13059, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10804615020751954, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.779018910194281e-06}, {"id": 319, "seek": 233900, "start": 2347.0, "end": 2358.0, "text": " An interesting thing and appropriate enough given the story of Clever Hans, is that this model found that about 70% of RN was classified as a horse.", "tokens": [50364, 467, 1355, 364, 35871, 576, 643, 1451, 10486, 281, 483, 5658, 9823, 382, 364, 17130, 322, 341, 2316, 13, 50764, 50764, 1107, 1880, 551, 293, 6854, 1547, 2212, 264, 1657, 295, 8834, 331, 17926, 11, 307, 300, 341, 2316, 1352, 300, 466, 5285, 4, 295, 497, 45, 390, 20627, 382, 257, 6832, 13, 51314, 51314, 407, 286, 2835, 300, 341, 2316, 486, 584, 300, 5658, 307, 746, 2831, 813, 1825, 11, 293, 309, 311, 767, 733, 295, 1021, 281, 519, 466, 577, 321, 13059, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10804615020751954, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.779018910194281e-06}, {"id": 320, "seek": 233900, "start": 2358.0, "end": 2367.0, "text": " So I mentioned that this model will say that noise is something rather than nothing, and it's actually kind of important to think about how we evaluate that.", "tokens": [50364, 467, 1355, 364, 35871, 576, 643, 1451, 10486, 281, 483, 5658, 9823, 382, 364, 17130, 322, 341, 2316, 13, 50764, 50764, 1107, 1880, 551, 293, 6854, 1547, 2212, 264, 1657, 295, 8834, 331, 17926, 11, 307, 300, 341, 2316, 1352, 300, 466, 5285, 4, 295, 497, 45, 390, 20627, 382, 257, 6832, 13, 51314, 51314, 407, 286, 2835, 300, 341, 2316, 486, 584, 300, 5658, 307, 746, 2831, 813, 1825, 11, 293, 309, 311, 767, 733, 295, 1021, 281, 519, 466, 577, 321, 13059, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10804615020751954, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.779018910194281e-06}, {"id": 321, "seek": 236700, "start": 2367.0, "end": 2375.0, "text": " If you have a softmax classifier, it has to give you a distribution over the end different classes that you train it on.", "tokens": [50364, 759, 291, 362, 257, 2787, 41167, 1508, 9902, 11, 309, 575, 281, 976, 291, 257, 7316, 670, 264, 917, 819, 5359, 300, 291, 3847, 309, 322, 13, 50764, 50764, 407, 456, 311, 257, 1326, 2098, 300, 291, 393, 9695, 300, 264, 2316, 307, 3585, 291, 300, 456, 311, 746, 2831, 813, 1825, 13, 51014, 51014, 1485, 307, 291, 393, 584, 11, 498, 309, 6269, 82, 746, 411, 4289, 4, 281, 472, 1729, 1508, 11, 300, 2544, 281, 312, 10419, 337, 300, 1508, 885, 456, 13, 51364, 51364, 492, 1116, 709, 2831, 536, 309, 976, 505, 746, 411, 257, 9452, 7316, 1566, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06154668556069428, "compression_ratio": 1.8081632653061224, "no_speech_prob": 5.3127678256714717e-05}, {"id": 322, "seek": 236700, "start": 2375.0, "end": 2380.0, "text": " So there's a few ways that you can argue that the model is telling you that there's something rather than nothing.", "tokens": [50364, 759, 291, 362, 257, 2787, 41167, 1508, 9902, 11, 309, 575, 281, 976, 291, 257, 7316, 670, 264, 917, 819, 5359, 300, 291, 3847, 309, 322, 13, 50764, 50764, 407, 456, 311, 257, 1326, 2098, 300, 291, 393, 9695, 300, 264, 2316, 307, 3585, 291, 300, 456, 311, 746, 2831, 813, 1825, 13, 51014, 51014, 1485, 307, 291, 393, 584, 11, 498, 309, 6269, 82, 746, 411, 4289, 4, 281, 472, 1729, 1508, 11, 300, 2544, 281, 312, 10419, 337, 300, 1508, 885, 456, 13, 51364, 51364, 492, 1116, 709, 2831, 536, 309, 976, 505, 746, 411, 257, 9452, 7316, 1566, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06154668556069428, "compression_ratio": 1.8081632653061224, "no_speech_prob": 5.3127678256714717e-05}, {"id": 323, "seek": 236700, "start": 2380.0, "end": 2387.0, "text": " One is you can say, if it assigns something like 90% to one particular class, that seems to be voting for that class being there.", "tokens": [50364, 759, 291, 362, 257, 2787, 41167, 1508, 9902, 11, 309, 575, 281, 976, 291, 257, 7316, 670, 264, 917, 819, 5359, 300, 291, 3847, 309, 322, 13, 50764, 50764, 407, 456, 311, 257, 1326, 2098, 300, 291, 393, 9695, 300, 264, 2316, 307, 3585, 291, 300, 456, 311, 746, 2831, 813, 1825, 13, 51014, 51014, 1485, 307, 291, 393, 584, 11, 498, 309, 6269, 82, 746, 411, 4289, 4, 281, 472, 1729, 1508, 11, 300, 2544, 281, 312, 10419, 337, 300, 1508, 885, 456, 13, 51364, 51364, 492, 1116, 709, 2831, 536, 309, 976, 505, 746, 411, 257, 9452, 7316, 1566, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06154668556069428, "compression_ratio": 1.8081632653061224, "no_speech_prob": 5.3127678256714717e-05}, {"id": 324, "seek": 236700, "start": 2387.0, "end": 2391.0, "text": " We'd much rather see it give us something like a uniform distribution saying,", "tokens": [50364, 759, 291, 362, 257, 2787, 41167, 1508, 9902, 11, 309, 575, 281, 976, 291, 257, 7316, 670, 264, 917, 819, 5359, 300, 291, 3847, 309, 322, 13, 50764, 50764, 407, 456, 311, 257, 1326, 2098, 300, 291, 393, 9695, 300, 264, 2316, 307, 3585, 291, 300, 456, 311, 746, 2831, 813, 1825, 13, 51014, 51014, 1485, 307, 291, 393, 584, 11, 498, 309, 6269, 82, 746, 411, 4289, 4, 281, 472, 1729, 1508, 11, 300, 2544, 281, 312, 10419, 337, 300, 1508, 885, 456, 13, 51364, 51364, 492, 1116, 709, 2831, 536, 309, 976, 505, 746, 411, 257, 9452, 7316, 1566, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06154668556069428, "compression_ratio": 1.8081632653061224, "no_speech_prob": 5.3127678256714717e-05}, {"id": 325, "seek": 239100, "start": 2391.0, "end": 2397.0, "text": " this noise doesn't look like anything in the training set, so it's equally likely to be a horse or a car.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 326, "seek": 239100, "start": 2397.0, "end": 2402.0, "text": " And that's not what the model does. It will say this is very definitely a horse.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 327, "seek": 239100, "start": 2402.0, "end": 2406.0, "text": " Another thing that you can do is you can replace the last layer of the model.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 328, "seek": 239100, "start": 2406.0, "end": 2411.0, "text": " For example, you can use a sigmoid output for each class.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 329, "seek": 239100, "start": 2411.0, "end": 2415.0, "text": " And then the model is actually capable of telling you that any subset of classes is present.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 330, "seek": 239100, "start": 2415.0, "end": 2418.0, "text": " It could actually tell you that an image is both a horse and a car.", "tokens": [50364, 341, 5658, 1177, 380, 574, 411, 1340, 294, 264, 3097, 992, 11, 370, 309, 311, 12309, 3700, 281, 312, 257, 6832, 420, 257, 1032, 13, 50664, 50664, 400, 300, 311, 406, 437, 264, 2316, 775, 13, 467, 486, 584, 341, 307, 588, 2138, 257, 6832, 13, 50914, 50914, 3996, 551, 300, 291, 393, 360, 307, 291, 393, 7406, 264, 1036, 4583, 295, 264, 2316, 13, 51114, 51114, 1171, 1365, 11, 291, 393, 764, 257, 4556, 3280, 327, 5598, 337, 1184, 1508, 13, 51364, 51364, 400, 550, 264, 2316, 307, 767, 8189, 295, 3585, 291, 300, 604, 25993, 295, 5359, 307, 1974, 13, 51564, 51564, 467, 727, 767, 980, 291, 300, 364, 3256, 307, 1293, 257, 6832, 293, 257, 1032, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09216316284671906, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.3453156245523132e-05}, {"id": 331, "seek": 241800, "start": 2418.0, "end": 2422.0, "text": " And what we'd like it to do for the noise is tell us that none of the classes is present.", "tokens": [50364, 400, 437, 321, 1116, 411, 309, 281, 360, 337, 264, 5658, 307, 980, 505, 300, 6022, 295, 264, 5359, 307, 1974, 13, 50564, 50564, 663, 439, 264, 4556, 3280, 3742, 820, 362, 257, 2158, 295, 1570, 813, 472, 1922, 13, 50764, 50764, 400, 472, 1922, 1943, 380, 754, 4098, 257, 2295, 14678, 13, 50964, 50964, 492, 727, 23551, 2066, 300, 439, 264, 4556, 3280, 3742, 576, 312, 1570, 813, 1958, 13, 10607, 337, 1270, 257, 16445, 488, 4846, 382, 341, 13, 51264, 51264, 583, 437, 321, 915, 2602, 307, 300, 264, 4556, 3280, 3742, 3928, 281, 362, 412, 1935, 472, 1508, 1974, 11, 51514, 51514, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09304731542413885, "compression_ratio": 1.7173913043478262, "no_speech_prob": 3.957674834964564e-06}, {"id": 332, "seek": 241800, "start": 2422.0, "end": 2426.0, "text": " That all the sigmoids should have a value of less than one half.", "tokens": [50364, 400, 437, 321, 1116, 411, 309, 281, 360, 337, 264, 5658, 307, 980, 505, 300, 6022, 295, 264, 5359, 307, 1974, 13, 50564, 50564, 663, 439, 264, 4556, 3280, 3742, 820, 362, 257, 2158, 295, 1570, 813, 472, 1922, 13, 50764, 50764, 400, 472, 1922, 1943, 380, 754, 4098, 257, 2295, 14678, 13, 50964, 50964, 492, 727, 23551, 2066, 300, 439, 264, 4556, 3280, 3742, 576, 312, 1570, 813, 1958, 13, 10607, 337, 1270, 257, 16445, 488, 4846, 382, 341, 13, 51264, 51264, 583, 437, 321, 915, 2602, 307, 300, 264, 4556, 3280, 3742, 3928, 281, 362, 412, 1935, 472, 1508, 1974, 11, 51514, 51514, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09304731542413885, "compression_ratio": 1.7173913043478262, "no_speech_prob": 3.957674834964564e-06}, {"id": 333, "seek": 241800, "start": 2426.0, "end": 2430.0, "text": " And one half isn't even particularly a low threshold.", "tokens": [50364, 400, 437, 321, 1116, 411, 309, 281, 360, 337, 264, 5658, 307, 980, 505, 300, 6022, 295, 264, 5359, 307, 1974, 13, 50564, 50564, 663, 439, 264, 4556, 3280, 3742, 820, 362, 257, 2158, 295, 1570, 813, 472, 1922, 13, 50764, 50764, 400, 472, 1922, 1943, 380, 754, 4098, 257, 2295, 14678, 13, 50964, 50964, 492, 727, 23551, 2066, 300, 439, 264, 4556, 3280, 3742, 576, 312, 1570, 813, 1958, 13, 10607, 337, 1270, 257, 16445, 488, 4846, 382, 341, 13, 51264, 51264, 583, 437, 321, 915, 2602, 307, 300, 264, 4556, 3280, 3742, 3928, 281, 362, 412, 1935, 472, 1508, 1974, 11, 51514, 51514, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09304731542413885, "compression_ratio": 1.7173913043478262, "no_speech_prob": 3.957674834964564e-06}, {"id": 334, "seek": 241800, "start": 2430.0, "end": 2436.0, "text": " We could reasonably expect that all the sigmoids would be less than 0.01 for such a defective input as this.", "tokens": [50364, 400, 437, 321, 1116, 411, 309, 281, 360, 337, 264, 5658, 307, 980, 505, 300, 6022, 295, 264, 5359, 307, 1974, 13, 50564, 50564, 663, 439, 264, 4556, 3280, 3742, 820, 362, 257, 2158, 295, 1570, 813, 472, 1922, 13, 50764, 50764, 400, 472, 1922, 1943, 380, 754, 4098, 257, 2295, 14678, 13, 50964, 50964, 492, 727, 23551, 2066, 300, 439, 264, 4556, 3280, 3742, 576, 312, 1570, 813, 1958, 13, 10607, 337, 1270, 257, 16445, 488, 4846, 382, 341, 13, 51264, 51264, 583, 437, 321, 915, 2602, 307, 300, 264, 4556, 3280, 3742, 3928, 281, 362, 412, 1935, 472, 1508, 1974, 11, 51514, 51514, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09304731542413885, "compression_ratio": 1.7173913043478262, "no_speech_prob": 3.957674834964564e-06}, {"id": 335, "seek": 241800, "start": 2436.0, "end": 2441.0, "text": " But what we find instead is that the sigmoids tend to have at least one class present,", "tokens": [50364, 400, 437, 321, 1116, 411, 309, 281, 360, 337, 264, 5658, 307, 980, 505, 300, 6022, 295, 264, 5359, 307, 1974, 13, 50564, 50564, 663, 439, 264, 4556, 3280, 3742, 820, 362, 257, 2158, 295, 1570, 813, 472, 1922, 13, 50764, 50764, 400, 472, 1922, 1943, 380, 754, 4098, 257, 2295, 14678, 13, 50964, 50964, 492, 727, 23551, 2066, 300, 439, 264, 4556, 3280, 3742, 576, 312, 1570, 813, 1958, 13, 10607, 337, 1270, 257, 16445, 488, 4846, 382, 341, 13, 51264, 51264, 583, 437, 321, 915, 2602, 307, 300, 264, 4556, 3280, 3742, 3928, 281, 362, 412, 1935, 472, 1508, 1974, 11, 51514, 51514, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09304731542413885, "compression_ratio": 1.7173913043478262, "no_speech_prob": 3.957674834964564e-06}, {"id": 336, "seek": 244100, "start": 2441.0, "end": 2449.0, "text": " just when we run Gaussian noise of sufficient norm through the model.", "tokens": [50364, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 50764, 50764, 492, 600, 611, 1352, 300, 321, 393, 360, 17641, 44745, 5110, 337, 29280, 2539, 13, 50964, 50964, 400, 456, 311, 257, 960, 337, 341, 13, 286, 603, 6580, 264, 9788, 934, 264, 751, 293, 291, 393, 1524, 264, 2113, 13, 51164, 51164, 8590, 11, 286, 2067, 380, 1075, 281, 483, 264, 14035, 12, 13229, 281, 589, 11, 370, 286, 393, 380, 855, 291, 264, 960, 18947, 13, 51364, 51364, 583, 286, 393, 6786, 1936, 437, 311, 516, 322, 490, 341, 920, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07612165282754337, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.459248687751824e-05}, {"id": 337, "seek": 244100, "start": 2449.0, "end": 2453.0, "text": " We've also found that we can do adversarial examples for reinforcement learning.", "tokens": [50364, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 50764, 50764, 492, 600, 611, 1352, 300, 321, 393, 360, 17641, 44745, 5110, 337, 29280, 2539, 13, 50964, 50964, 400, 456, 311, 257, 960, 337, 341, 13, 286, 603, 6580, 264, 9788, 934, 264, 751, 293, 291, 393, 1524, 264, 2113, 13, 51164, 51164, 8590, 11, 286, 2067, 380, 1075, 281, 483, 264, 14035, 12, 13229, 281, 589, 11, 370, 286, 393, 380, 855, 291, 264, 960, 18947, 13, 51364, 51364, 583, 286, 393, 6786, 1936, 437, 311, 516, 322, 490, 341, 920, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07612165282754337, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.459248687751824e-05}, {"id": 338, "seek": 244100, "start": 2453.0, "end": 2457.0, "text": " And there's a video for this. I'll upload the slides after the talk and you can follow the link.", "tokens": [50364, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 50764, 50764, 492, 600, 611, 1352, 300, 321, 393, 360, 17641, 44745, 5110, 337, 29280, 2539, 13, 50964, 50964, 400, 456, 311, 257, 960, 337, 341, 13, 286, 603, 6580, 264, 9788, 934, 264, 751, 293, 291, 393, 1524, 264, 2113, 13, 51164, 51164, 8590, 11, 286, 2067, 380, 1075, 281, 483, 264, 14035, 12, 13229, 281, 589, 11, 370, 286, 393, 380, 855, 291, 264, 960, 18947, 13, 51364, 51364, 583, 286, 393, 6786, 1936, 437, 311, 516, 322, 490, 341, 920, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07612165282754337, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.459248687751824e-05}, {"id": 339, "seek": 244100, "start": 2457.0, "end": 2461.0, "text": " Unfortunately, I wasn't able to get the Wi-Fi to work, so I can't show you the video animated.", "tokens": [50364, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 50764, 50764, 492, 600, 611, 1352, 300, 321, 393, 360, 17641, 44745, 5110, 337, 29280, 2539, 13, 50964, 50964, 400, 456, 311, 257, 960, 337, 341, 13, 286, 603, 6580, 264, 9788, 934, 264, 751, 293, 291, 393, 1524, 264, 2113, 13, 51164, 51164, 8590, 11, 286, 2067, 380, 1075, 281, 483, 264, 14035, 12, 13229, 281, 589, 11, 370, 286, 393, 380, 855, 291, 264, 960, 18947, 13, 51364, 51364, 583, 286, 393, 6786, 1936, 437, 311, 516, 322, 490, 341, 920, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07612165282754337, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.459248687751824e-05}, {"id": 340, "seek": 244100, "start": 2461.0, "end": 2465.0, "text": " But I can describe basically what's going on from this still here.", "tokens": [50364, 445, 562, 321, 1190, 39148, 5658, 295, 11563, 2026, 807, 264, 2316, 13, 50764, 50764, 492, 600, 611, 1352, 300, 321, 393, 360, 17641, 44745, 5110, 337, 29280, 2539, 13, 50964, 50964, 400, 456, 311, 257, 960, 337, 341, 13, 286, 603, 6580, 264, 9788, 934, 264, 751, 293, 291, 393, 1524, 264, 2113, 13, 51164, 51164, 8590, 11, 286, 2067, 380, 1075, 281, 483, 264, 14035, 12, 13229, 281, 589, 11, 370, 286, 393, 380, 855, 291, 264, 960, 18947, 13, 51364, 51364, 583, 286, 393, 6786, 1936, 437, 311, 516, 322, 490, 341, 920, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07612165282754337, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.459248687751824e-05}, {"id": 341, "seek": 246500, "start": 2465.0, "end": 2472.0, "text": " There's a game sequest on Atari where you can train reinforcement learning agents to play that game.", "tokens": [50364, 821, 311, 257, 1216, 5123, 377, 322, 41381, 689, 291, 393, 3847, 29280, 2539, 12554, 281, 862, 300, 1216, 13, 50714, 50714, 400, 291, 393, 747, 264, 8936, 4846, 18668, 293, 291, 393, 747, 264, 2370, 16235, 1465, 3170, 420, 661, 8122, 300, 764, 661, 24357, 11868, 264, 11469, 2026, 13, 51264, 51264, 400, 14722, 40468, 763, 300, 366, 10226, 281, 1319, 264, 3069, 300, 264, 3897, 576, 3048, 13, 51514, 51514, 407, 264, 29280, 2539, 3897, 11, 291, 393, 519, 295, 309, 382, 445, 885, 411, 257, 1508, 9902, 300, 1542, 412, 257, 3920, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10998490333557129, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5842068023630418e-05}, {"id": 342, "seek": 246500, "start": 2472.0, "end": 2483.0, "text": " And you can take the raw input pixels and you can take the fast gradient sign method or other attacks that use other norms besides the max norm.", "tokens": [50364, 821, 311, 257, 1216, 5123, 377, 322, 41381, 689, 291, 393, 3847, 29280, 2539, 12554, 281, 862, 300, 1216, 13, 50714, 50714, 400, 291, 393, 747, 264, 8936, 4846, 18668, 293, 291, 393, 747, 264, 2370, 16235, 1465, 3170, 420, 661, 8122, 300, 764, 661, 24357, 11868, 264, 11469, 2026, 13, 51264, 51264, 400, 14722, 40468, 763, 300, 366, 10226, 281, 1319, 264, 3069, 300, 264, 3897, 576, 3048, 13, 51514, 51514, 407, 264, 29280, 2539, 3897, 11, 291, 393, 519, 295, 309, 382, 445, 885, 411, 257, 1508, 9902, 300, 1542, 412, 257, 3920, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10998490333557129, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5842068023630418e-05}, {"id": 343, "seek": 246500, "start": 2483.0, "end": 2488.0, "text": " And compute perturbations that are intended to change the action that the policy would select.", "tokens": [50364, 821, 311, 257, 1216, 5123, 377, 322, 41381, 689, 291, 393, 3847, 29280, 2539, 12554, 281, 862, 300, 1216, 13, 50714, 50714, 400, 291, 393, 747, 264, 8936, 4846, 18668, 293, 291, 393, 747, 264, 2370, 16235, 1465, 3170, 420, 661, 8122, 300, 764, 661, 24357, 11868, 264, 11469, 2026, 13, 51264, 51264, 400, 14722, 40468, 763, 300, 366, 10226, 281, 1319, 264, 3069, 300, 264, 3897, 576, 3048, 13, 51514, 51514, 407, 264, 29280, 2539, 3897, 11, 291, 393, 519, 295, 309, 382, 445, 885, 411, 257, 1508, 9902, 300, 1542, 412, 257, 3920, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10998490333557129, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5842068023630418e-05}, {"id": 344, "seek": 246500, "start": 2488.0, "end": 2494.0, "text": " So the reinforcement learning policy, you can think of it as just being like a classifier that looks at a frame.", "tokens": [50364, 821, 311, 257, 1216, 5123, 377, 322, 41381, 689, 291, 393, 3847, 29280, 2539, 12554, 281, 862, 300, 1216, 13, 50714, 50714, 400, 291, 393, 747, 264, 8936, 4846, 18668, 293, 291, 393, 747, 264, 2370, 16235, 1465, 3170, 420, 661, 8122, 300, 764, 661, 24357, 11868, 264, 11469, 2026, 13, 51264, 51264, 400, 14722, 40468, 763, 300, 366, 10226, 281, 1319, 264, 3069, 300, 264, 3897, 576, 3048, 13, 51514, 51514, 407, 264, 29280, 2539, 3897, 11, 291, 393, 519, 295, 309, 382, 445, 885, 411, 257, 1508, 9902, 300, 1542, 412, 257, 3920, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10998490333557129, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5842068023630418e-05}, {"id": 345, "seek": 249400, "start": 2494.0, "end": 2501.0, "text": " And instead of categorizing the input into a particular category, it gives you a softmax distribution over actions to take.", "tokens": [50364, 400, 2602, 295, 19250, 3319, 264, 4846, 666, 257, 1729, 7719, 11, 309, 2709, 291, 257, 2787, 41167, 7316, 670, 5909, 281, 747, 13, 50714, 50714, 407, 498, 321, 445, 747, 300, 293, 584, 300, 264, 881, 3700, 3069, 820, 362, 1080, 14170, 312, 24436, 538, 264, 48222, 13, 51064, 51064, 407, 286, 1116, 362, 1080, 8482, 312, 24436, 538, 264, 48222, 13, 51264, 51264, 509, 603, 483, 613, 40468, 763, 295, 4846, 12083, 300, 291, 393, 550, 3079, 293, 3082, 264, 9461, 281, 862, 819, 5909, 300, 309, 576, 362, 5911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0679307573849393, "compression_ratio": 1.7519685039370079, "no_speech_prob": 2.469807986926753e-05}, {"id": 346, "seek": 249400, "start": 2501.0, "end": 2508.0, "text": " So if we just take that and say that the most likely action should have its accuracy be decreased by the adversary.", "tokens": [50364, 400, 2602, 295, 19250, 3319, 264, 4846, 666, 257, 1729, 7719, 11, 309, 2709, 291, 257, 2787, 41167, 7316, 670, 5909, 281, 747, 13, 50714, 50714, 407, 498, 321, 445, 747, 300, 293, 584, 300, 264, 881, 3700, 3069, 820, 362, 1080, 14170, 312, 24436, 538, 264, 48222, 13, 51064, 51064, 407, 286, 1116, 362, 1080, 8482, 312, 24436, 538, 264, 48222, 13, 51264, 51264, 509, 603, 483, 613, 40468, 763, 295, 4846, 12083, 300, 291, 393, 550, 3079, 293, 3082, 264, 9461, 281, 862, 819, 5909, 300, 309, 576, 362, 5911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0679307573849393, "compression_ratio": 1.7519685039370079, "no_speech_prob": 2.469807986926753e-05}, {"id": 347, "seek": 249400, "start": 2508.0, "end": 2512.0, "text": " So I'd have its probability be decreased by the adversary.", "tokens": [50364, 400, 2602, 295, 19250, 3319, 264, 4846, 666, 257, 1729, 7719, 11, 309, 2709, 291, 257, 2787, 41167, 7316, 670, 5909, 281, 747, 13, 50714, 50714, 407, 498, 321, 445, 747, 300, 293, 584, 300, 264, 881, 3700, 3069, 820, 362, 1080, 14170, 312, 24436, 538, 264, 48222, 13, 51064, 51064, 407, 286, 1116, 362, 1080, 8482, 312, 24436, 538, 264, 48222, 13, 51264, 51264, 509, 603, 483, 613, 40468, 763, 295, 4846, 12083, 300, 291, 393, 550, 3079, 293, 3082, 264, 9461, 281, 862, 819, 5909, 300, 309, 576, 362, 5911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0679307573849393, "compression_ratio": 1.7519685039370079, "no_speech_prob": 2.469807986926753e-05}, {"id": 348, "seek": 249400, "start": 2512.0, "end": 2519.0, "text": " You'll get these perturbations of input frames that you can then apply and cause the agent to play different actions that it would have otherwise.", "tokens": [50364, 400, 2602, 295, 19250, 3319, 264, 4846, 666, 257, 1729, 7719, 11, 309, 2709, 291, 257, 2787, 41167, 7316, 670, 5909, 281, 747, 13, 50714, 50714, 407, 498, 321, 445, 747, 300, 293, 584, 300, 264, 881, 3700, 3069, 820, 362, 1080, 14170, 312, 24436, 538, 264, 48222, 13, 51064, 51064, 407, 286, 1116, 362, 1080, 8482, 312, 24436, 538, 264, 48222, 13, 51264, 51264, 509, 603, 483, 613, 40468, 763, 295, 4846, 12083, 300, 291, 393, 550, 3079, 293, 3082, 264, 9461, 281, 862, 819, 5909, 300, 309, 576, 362, 5911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0679307573849393, "compression_ratio": 1.7519685039370079, "no_speech_prob": 2.469807986926753e-05}, {"id": 349, "seek": 251900, "start": 2519.0, "end": 2524.0, "text": " And using this you can make the agent play sequest very, very badly.", "tokens": [50364, 400, 1228, 341, 291, 393, 652, 264, 9461, 862, 5123, 377, 588, 11, 588, 13425, 13, 50614, 50614, 467, 311, 1310, 406, 264, 881, 1880, 1944, 551, 13, 50764, 50764, 708, 321, 1116, 534, 411, 307, 364, 2823, 689, 456, 366, 867, 819, 7782, 6828, 2435, 337, 505, 281, 2979, 13, 51014, 51014, 407, 337, 1365, 11, 498, 291, 632, 257, 7881, 300, 390, 10226, 281, 2543, 49127, 6466, 293, 291, 632, 257, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 49127, 6466, 13, 51464, 51464, 400, 291, 632, 1071, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 6215, 5908, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08276169186546689, "compression_ratio": 1.8404669260700388, "no_speech_prob": 1.740657171467319e-05}, {"id": 350, "seek": 251900, "start": 2524.0, "end": 2527.0, "text": " It's maybe not the most interesting possible thing.", "tokens": [50364, 400, 1228, 341, 291, 393, 652, 264, 9461, 862, 5123, 377, 588, 11, 588, 13425, 13, 50614, 50614, 467, 311, 1310, 406, 264, 881, 1880, 1944, 551, 13, 50764, 50764, 708, 321, 1116, 534, 411, 307, 364, 2823, 689, 456, 366, 867, 819, 7782, 6828, 2435, 337, 505, 281, 2979, 13, 51014, 51014, 407, 337, 1365, 11, 498, 291, 632, 257, 7881, 300, 390, 10226, 281, 2543, 49127, 6466, 293, 291, 632, 257, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 49127, 6466, 13, 51464, 51464, 400, 291, 632, 1071, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 6215, 5908, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08276169186546689, "compression_ratio": 1.8404669260700388, "no_speech_prob": 1.740657171467319e-05}, {"id": 351, "seek": 251900, "start": 2527.0, "end": 2532.0, "text": " What we'd really like is an environment where there are many different reward functions available for us to study.", "tokens": [50364, 400, 1228, 341, 291, 393, 652, 264, 9461, 862, 5123, 377, 588, 11, 588, 13425, 13, 50614, 50614, 467, 311, 1310, 406, 264, 881, 1880, 1944, 551, 13, 50764, 50764, 708, 321, 1116, 534, 411, 307, 364, 2823, 689, 456, 366, 867, 819, 7782, 6828, 2435, 337, 505, 281, 2979, 13, 51014, 51014, 407, 337, 1365, 11, 498, 291, 632, 257, 7881, 300, 390, 10226, 281, 2543, 49127, 6466, 293, 291, 632, 257, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 49127, 6466, 13, 51464, 51464, 400, 291, 632, 1071, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 6215, 5908, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08276169186546689, "compression_ratio": 1.8404669260700388, "no_speech_prob": 1.740657171467319e-05}, {"id": 352, "seek": 251900, "start": 2532.0, "end": 2541.0, "text": " So for example, if you had a robot that was intended to cook scrambled eggs and you had a reward function measuring how well it's cooking scrambled eggs.", "tokens": [50364, 400, 1228, 341, 291, 393, 652, 264, 9461, 862, 5123, 377, 588, 11, 588, 13425, 13, 50614, 50614, 467, 311, 1310, 406, 264, 881, 1880, 1944, 551, 13, 50764, 50764, 708, 321, 1116, 534, 411, 307, 364, 2823, 689, 456, 366, 867, 819, 7782, 6828, 2435, 337, 505, 281, 2979, 13, 51014, 51014, 407, 337, 1365, 11, 498, 291, 632, 257, 7881, 300, 390, 10226, 281, 2543, 49127, 6466, 293, 291, 632, 257, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 49127, 6466, 13, 51464, 51464, 400, 291, 632, 1071, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 6215, 5908, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08276169186546689, "compression_ratio": 1.8404669260700388, "no_speech_prob": 1.740657171467319e-05}, {"id": 353, "seek": 251900, "start": 2541.0, "end": 2546.0, "text": " And you had another reward function measuring how well it's cooking chocolate cake.", "tokens": [50364, 400, 1228, 341, 291, 393, 652, 264, 9461, 862, 5123, 377, 588, 11, 588, 13425, 13, 50614, 50614, 467, 311, 1310, 406, 264, 881, 1880, 1944, 551, 13, 50764, 50764, 708, 321, 1116, 534, 411, 307, 364, 2823, 689, 456, 366, 867, 819, 7782, 6828, 2435, 337, 505, 281, 2979, 13, 51014, 51014, 407, 337, 1365, 11, 498, 291, 632, 257, 7881, 300, 390, 10226, 281, 2543, 49127, 6466, 293, 291, 632, 257, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 49127, 6466, 13, 51464, 51464, 400, 291, 632, 1071, 7782, 2445, 13389, 577, 731, 309, 311, 6361, 6215, 5908, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08276169186546689, "compression_ratio": 1.8404669260700388, "no_speech_prob": 1.740657171467319e-05}, {"id": 354, "seek": 254600, "start": 2546.0, "end": 2555.0, "text": " It would be really interesting if we could make adversarial examples that cause the robot to make a chocolate cake when the user intended for it to make scrambled eggs.", "tokens": [50364, 467, 576, 312, 534, 1880, 498, 321, 727, 652, 17641, 44745, 5110, 300, 3082, 264, 7881, 281, 652, 257, 6215, 5908, 562, 264, 4195, 10226, 337, 309, 281, 652, 49127, 6466, 13, 50814, 50814, 663, 311, 570, 309, 311, 588, 2252, 281, 7754, 412, 746, 13, 50964, 50964, 467, 311, 7226, 15325, 281, 652, 257, 1185, 3061, 13, 51114, 51114, 407, 558, 586, 17641, 44745, 5110, 337, 497, 10740, 366, 588, 665, 412, 4099, 300, 321, 393, 652, 497, 10740, 12554, 3061, 11, 457, 321, 2378, 380, 1939, 668, 1075, 281, 10625, 501, 552, 293, 652, 552, 360, 257, 6179, 5633, 300, 311, 819, 490, 437, 436, 434, 49902, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07600792667321991, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.9632052246597596e-05}, {"id": 355, "seek": 254600, "start": 2555.0, "end": 2558.0, "text": " That's because it's very difficult to succeed at something.", "tokens": [50364, 467, 576, 312, 534, 1880, 498, 321, 727, 652, 17641, 44745, 5110, 300, 3082, 264, 7881, 281, 652, 257, 6215, 5908, 562, 264, 4195, 10226, 337, 309, 281, 652, 49127, 6466, 13, 50814, 50814, 663, 311, 570, 309, 311, 588, 2252, 281, 7754, 412, 746, 13, 50964, 50964, 467, 311, 7226, 15325, 281, 652, 257, 1185, 3061, 13, 51114, 51114, 407, 558, 586, 17641, 44745, 5110, 337, 497, 10740, 366, 588, 665, 412, 4099, 300, 321, 393, 652, 497, 10740, 12554, 3061, 11, 457, 321, 2378, 380, 1939, 668, 1075, 281, 10625, 501, 552, 293, 652, 552, 360, 257, 6179, 5633, 300, 311, 819, 490, 437, 436, 434, 49902, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07600792667321991, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.9632052246597596e-05}, {"id": 356, "seek": 254600, "start": 2558.0, "end": 2561.0, "text": " It's relatively straightforward to make a system fail.", "tokens": [50364, 467, 576, 312, 534, 1880, 498, 321, 727, 652, 17641, 44745, 5110, 300, 3082, 264, 7881, 281, 652, 257, 6215, 5908, 562, 264, 4195, 10226, 337, 309, 281, 652, 49127, 6466, 13, 50814, 50814, 663, 311, 570, 309, 311, 588, 2252, 281, 7754, 412, 746, 13, 50964, 50964, 467, 311, 7226, 15325, 281, 652, 257, 1185, 3061, 13, 51114, 51114, 407, 558, 586, 17641, 44745, 5110, 337, 497, 10740, 366, 588, 665, 412, 4099, 300, 321, 393, 652, 497, 10740, 12554, 3061, 11, 457, 321, 2378, 380, 1939, 668, 1075, 281, 10625, 501, 552, 293, 652, 552, 360, 257, 6179, 5633, 300, 311, 819, 490, 437, 436, 434, 49902, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07600792667321991, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.9632052246597596e-05}, {"id": 357, "seek": 254600, "start": 2561.0, "end": 2572.0, "text": " So right now adversarial examples for RRL are very good at showing that we can make RRL agents fail, but we haven't yet been able to hijack them and make them do a complicated task that's different from what they're unintended.", "tokens": [50364, 467, 576, 312, 534, 1880, 498, 321, 727, 652, 17641, 44745, 5110, 300, 3082, 264, 7881, 281, 652, 257, 6215, 5908, 562, 264, 4195, 10226, 337, 309, 281, 652, 49127, 6466, 13, 50814, 50814, 663, 311, 570, 309, 311, 588, 2252, 281, 7754, 412, 746, 13, 50964, 50964, 467, 311, 7226, 15325, 281, 652, 257, 1185, 3061, 13, 51114, 51114, 407, 558, 586, 17641, 44745, 5110, 337, 497, 10740, 366, 588, 665, 412, 4099, 300, 321, 393, 652, 497, 10740, 12554, 3061, 11, 457, 321, 2378, 380, 1939, 668, 1075, 281, 10625, 501, 552, 293, 652, 552, 360, 257, 6179, 5633, 300, 311, 819, 490, 437, 436, 434, 49902, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07600792667321991, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.9632052246597596e-05}, {"id": 358, "seek": 257200, "start": 2572.0, "end": 2579.0, "text": " It seems like it's one of the next steps in adversarial example research though.", "tokens": [50364, 467, 2544, 411, 309, 311, 472, 295, 264, 958, 4439, 294, 17641, 44745, 1365, 2132, 1673, 13, 50714, 50714, 759, 321, 574, 412, 1090, 18795, 8213, 5245, 11, 321, 1116, 767, 536, 300, 257, 688, 295, 341, 307, 588, 2199, 293, 15325, 13, 51014, 51014, 1692, 321, 362, 257, 3565, 3142, 24590, 2316, 300, 1508, 11221, 1614, 82, 293, 805, 82, 13, 51314, 51314, 407, 264, 1379, 2316, 393, 312, 7619, 445, 538, 257, 3364, 8062, 293, 257, 2167, 39684, 12577, 1433, 13, 51614, 51614, 492, 500, 380, 534, 643, 281, 536, 264, 12577, 1433, 337, 341, 5380, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09097059490611252, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.1740649824787397e-05}, {"id": 359, "seek": 257200, "start": 2579.0, "end": 2585.0, "text": " If we look at high dimensional linear models, we'd actually see that a lot of this is very simple and straightforward.", "tokens": [50364, 467, 2544, 411, 309, 311, 472, 295, 264, 958, 4439, 294, 17641, 44745, 1365, 2132, 1673, 13, 50714, 50714, 759, 321, 574, 412, 1090, 18795, 8213, 5245, 11, 321, 1116, 767, 536, 300, 257, 688, 295, 341, 307, 588, 2199, 293, 15325, 13, 51014, 51014, 1692, 321, 362, 257, 3565, 3142, 24590, 2316, 300, 1508, 11221, 1614, 82, 293, 805, 82, 13, 51314, 51314, 407, 264, 1379, 2316, 393, 312, 7619, 445, 538, 257, 3364, 8062, 293, 257, 2167, 39684, 12577, 1433, 13, 51614, 51614, 492, 500, 380, 534, 643, 281, 536, 264, 12577, 1433, 337, 341, 5380, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09097059490611252, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.1740649824787397e-05}, {"id": 360, "seek": 257200, "start": 2585.0, "end": 2591.0, "text": " Here we have a logistic regression model that classifies 7s and 3s.", "tokens": [50364, 467, 2544, 411, 309, 311, 472, 295, 264, 958, 4439, 294, 17641, 44745, 1365, 2132, 1673, 13, 50714, 50714, 759, 321, 574, 412, 1090, 18795, 8213, 5245, 11, 321, 1116, 767, 536, 300, 257, 688, 295, 341, 307, 588, 2199, 293, 15325, 13, 51014, 51014, 1692, 321, 362, 257, 3565, 3142, 24590, 2316, 300, 1508, 11221, 1614, 82, 293, 805, 82, 13, 51314, 51314, 407, 264, 1379, 2316, 393, 312, 7619, 445, 538, 257, 3364, 8062, 293, 257, 2167, 39684, 12577, 1433, 13, 51614, 51614, 492, 500, 380, 534, 643, 281, 536, 264, 12577, 1433, 337, 341, 5380, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09097059490611252, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.1740649824787397e-05}, {"id": 361, "seek": 257200, "start": 2591.0, "end": 2597.0, "text": " So the whole model can be described just by a weight vector and a single scalar bias term.", "tokens": [50364, 467, 2544, 411, 309, 311, 472, 295, 264, 958, 4439, 294, 17641, 44745, 1365, 2132, 1673, 13, 50714, 50714, 759, 321, 574, 412, 1090, 18795, 8213, 5245, 11, 321, 1116, 767, 536, 300, 257, 688, 295, 341, 307, 588, 2199, 293, 15325, 13, 51014, 51014, 1692, 321, 362, 257, 3565, 3142, 24590, 2316, 300, 1508, 11221, 1614, 82, 293, 805, 82, 13, 51314, 51314, 407, 264, 1379, 2316, 393, 312, 7619, 445, 538, 257, 3364, 8062, 293, 257, 2167, 39684, 12577, 1433, 13, 51614, 51614, 492, 500, 380, 534, 643, 281, 536, 264, 12577, 1433, 337, 341, 5380, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09097059490611252, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.1740649824787397e-05}, {"id": 362, "seek": 257200, "start": 2597.0, "end": 2601.0, "text": " We don't really need to see the bias term for this exercise.", "tokens": [50364, 467, 2544, 411, 309, 311, 472, 295, 264, 958, 4439, 294, 17641, 44745, 1365, 2132, 1673, 13, 50714, 50714, 759, 321, 574, 412, 1090, 18795, 8213, 5245, 11, 321, 1116, 767, 536, 300, 257, 688, 295, 341, 307, 588, 2199, 293, 15325, 13, 51014, 51014, 1692, 321, 362, 257, 3565, 3142, 24590, 2316, 300, 1508, 11221, 1614, 82, 293, 805, 82, 13, 51314, 51314, 407, 264, 1379, 2316, 393, 312, 7619, 445, 538, 257, 3364, 8062, 293, 257, 2167, 39684, 12577, 1433, 13, 51614, 51614, 492, 500, 380, 534, 643, 281, 536, 264, 12577, 1433, 337, 341, 5380, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09097059490611252, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.1740649824787397e-05}, {"id": 363, "seek": 260100, "start": 2601.0, "end": 2605.0, "text": " If you look on the left, I've plotted the weights that we use to discriminate 7s and 3s.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 286, 600, 43288, 264, 17443, 300, 321, 764, 281, 47833, 1614, 82, 293, 805, 82, 13, 50564, 50564, 440, 17443, 820, 574, 257, 707, 857, 411, 264, 2649, 1296, 264, 4274, 1614, 293, 264, 4274, 805, 13, 50814, 50814, 400, 550, 760, 412, 264, 2767, 321, 600, 2726, 264, 1465, 295, 264, 17443, 13, 51014, 51014, 407, 264, 16235, 337, 3565, 3142, 24590, 2316, 307, 516, 281, 312, 24969, 281, 264, 17443, 13, 51264, 51264, 400, 550, 264, 1465, 295, 264, 17443, 2709, 291, 4476, 264, 1465, 295, 264, 16235, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0725246036753935, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.0442102393426467e-05}, {"id": 364, "seek": 260100, "start": 2605.0, "end": 2610.0, "text": " The weights should look a little bit like the difference between the average 7 and the average 3.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 286, 600, 43288, 264, 17443, 300, 321, 764, 281, 47833, 1614, 82, 293, 805, 82, 13, 50564, 50564, 440, 17443, 820, 574, 257, 707, 857, 411, 264, 2649, 1296, 264, 4274, 1614, 293, 264, 4274, 805, 13, 50814, 50814, 400, 550, 760, 412, 264, 2767, 321, 600, 2726, 264, 1465, 295, 264, 17443, 13, 51014, 51014, 407, 264, 16235, 337, 3565, 3142, 24590, 2316, 307, 516, 281, 312, 24969, 281, 264, 17443, 13, 51264, 51264, 400, 550, 264, 1465, 295, 264, 17443, 2709, 291, 4476, 264, 1465, 295, 264, 16235, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0725246036753935, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.0442102393426467e-05}, {"id": 365, "seek": 260100, "start": 2610.0, "end": 2614.0, "text": " And then down at the bottom we've taken the sign of the weights.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 286, 600, 43288, 264, 17443, 300, 321, 764, 281, 47833, 1614, 82, 293, 805, 82, 13, 50564, 50564, 440, 17443, 820, 574, 257, 707, 857, 411, 264, 2649, 1296, 264, 4274, 1614, 293, 264, 4274, 805, 13, 50814, 50814, 400, 550, 760, 412, 264, 2767, 321, 600, 2726, 264, 1465, 295, 264, 17443, 13, 51014, 51014, 407, 264, 16235, 337, 3565, 3142, 24590, 2316, 307, 516, 281, 312, 24969, 281, 264, 17443, 13, 51264, 51264, 400, 550, 264, 1465, 295, 264, 17443, 2709, 291, 4476, 264, 1465, 295, 264, 16235, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0725246036753935, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.0442102393426467e-05}, {"id": 366, "seek": 260100, "start": 2614.0, "end": 2619.0, "text": " So the gradient for logistic regression model is going to be proportional to the weights.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 286, 600, 43288, 264, 17443, 300, 321, 764, 281, 47833, 1614, 82, 293, 805, 82, 13, 50564, 50564, 440, 17443, 820, 574, 257, 707, 857, 411, 264, 2649, 1296, 264, 4274, 1614, 293, 264, 4274, 805, 13, 50814, 50814, 400, 550, 760, 412, 264, 2767, 321, 600, 2726, 264, 1465, 295, 264, 17443, 13, 51014, 51014, 407, 264, 16235, 337, 3565, 3142, 24590, 2316, 307, 516, 281, 312, 24969, 281, 264, 17443, 13, 51264, 51264, 400, 550, 264, 1465, 295, 264, 17443, 2709, 291, 4476, 264, 1465, 295, 264, 16235, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0725246036753935, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.0442102393426467e-05}, {"id": 367, "seek": 260100, "start": 2619.0, "end": 2624.0, "text": " And then the sign of the weights gives you essentially the sign of the gradient.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 286, 600, 43288, 264, 17443, 300, 321, 764, 281, 47833, 1614, 82, 293, 805, 82, 13, 50564, 50564, 440, 17443, 820, 574, 257, 707, 857, 411, 264, 2649, 1296, 264, 4274, 1614, 293, 264, 4274, 805, 13, 50814, 50814, 400, 550, 760, 412, 264, 2767, 321, 600, 2726, 264, 1465, 295, 264, 17443, 13, 51014, 51014, 407, 264, 16235, 337, 3565, 3142, 24590, 2316, 307, 516, 281, 312, 24969, 281, 264, 17443, 13, 51264, 51264, 400, 550, 264, 1465, 295, 264, 17443, 2709, 291, 4476, 264, 1465, 295, 264, 16235, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0725246036753935, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.0442102393426467e-05}, {"id": 368, "seek": 262400, "start": 2624.0, "end": 2631.0, "text": " So we can do the fast gradient sign method to attack this model just by looking at its weights.", "tokens": [50364, 407, 321, 393, 360, 264, 2370, 16235, 1465, 3170, 281, 2690, 341, 2316, 445, 538, 1237, 412, 1080, 17443, 13, 50714, 50714, 682, 264, 5110, 294, 264, 4831, 11, 264, 1150, 7738, 490, 264, 1411, 11, 321, 393, 536, 2541, 5110, 13, 50964, 50964, 400, 550, 322, 264, 558, 321, 600, 445, 3869, 420, 16390, 292, 341, 3256, 295, 264, 1465, 295, 264, 17443, 766, 295, 552, 13, 51264, 51264, 1407, 291, 293, 385, 293, 382, 1952, 48090, 11, 264, 1465, 295, 264, 17443, 307, 445, 411, 14150, 300, 311, 294, 264, 646, 4149, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15565448337131077, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.8255377653986216e-05}, {"id": 369, "seek": 262400, "start": 2631.0, "end": 2636.0, "text": " In the examples in the panel, the second column from the left, we can see clean examples.", "tokens": [50364, 407, 321, 393, 360, 264, 2370, 16235, 1465, 3170, 281, 2690, 341, 2316, 445, 538, 1237, 412, 1080, 17443, 13, 50714, 50714, 682, 264, 5110, 294, 264, 4831, 11, 264, 1150, 7738, 490, 264, 1411, 11, 321, 393, 536, 2541, 5110, 13, 50964, 50964, 400, 550, 322, 264, 558, 321, 600, 445, 3869, 420, 16390, 292, 341, 3256, 295, 264, 1465, 295, 264, 17443, 766, 295, 552, 13, 51264, 51264, 1407, 291, 293, 385, 293, 382, 1952, 48090, 11, 264, 1465, 295, 264, 17443, 307, 445, 411, 14150, 300, 311, 294, 264, 646, 4149, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15565448337131077, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.8255377653986216e-05}, {"id": 370, "seek": 262400, "start": 2636.0, "end": 2642.0, "text": " And then on the right we've just added or subtracted this image of the sign of the weights off of them.", "tokens": [50364, 407, 321, 393, 360, 264, 2370, 16235, 1465, 3170, 281, 2690, 341, 2316, 445, 538, 1237, 412, 1080, 17443, 13, 50714, 50714, 682, 264, 5110, 294, 264, 4831, 11, 264, 1150, 7738, 490, 264, 1411, 11, 321, 393, 536, 2541, 5110, 13, 50964, 50964, 400, 550, 322, 264, 558, 321, 600, 445, 3869, 420, 16390, 292, 341, 3256, 295, 264, 1465, 295, 264, 17443, 766, 295, 552, 13, 51264, 51264, 1407, 291, 293, 385, 293, 382, 1952, 48090, 11, 264, 1465, 295, 264, 17443, 307, 445, 411, 14150, 300, 311, 294, 264, 646, 4149, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15565448337131077, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.8255377653986216e-05}, {"id": 371, "seek": 262400, "start": 2642.0, "end": 2649.0, "text": " To you and me and as human observers, the sign of the weights is just like garbage that's in the backrest.", "tokens": [50364, 407, 321, 393, 360, 264, 2370, 16235, 1465, 3170, 281, 2690, 341, 2316, 445, 538, 1237, 412, 1080, 17443, 13, 50714, 50714, 682, 264, 5110, 294, 264, 4831, 11, 264, 1150, 7738, 490, 264, 1411, 11, 321, 393, 536, 2541, 5110, 13, 50964, 50964, 400, 550, 322, 264, 558, 321, 600, 445, 3869, 420, 16390, 292, 341, 3256, 295, 264, 1465, 295, 264, 17443, 766, 295, 552, 13, 51264, 51264, 1407, 291, 293, 385, 293, 382, 1952, 48090, 11, 264, 1465, 295, 264, 17443, 307, 445, 411, 14150, 300, 311, 294, 264, 646, 4149, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15565448337131077, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.8255377653986216e-05}, {"id": 372, "seek": 264900, "start": 2649.0, "end": 2656.0, "text": " And more or less filter it out. It doesn't look particularly interesting to us. It doesn't grab our attention.", "tokens": [50364, 400, 544, 420, 1570, 6608, 309, 484, 13, 467, 1177, 380, 574, 4098, 1880, 281, 505, 13, 467, 1177, 380, 4444, 527, 3202, 13, 50714, 50714, 1407, 264, 3565, 3142, 24590, 2316, 11, 341, 3256, 295, 264, 1465, 295, 264, 17443, 307, 264, 881, 1845, 1196, 551, 300, 727, 1562, 4204, 294, 264, 3256, 13, 51114, 51114, 1133, 309, 311, 3353, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 1614, 13, 51314, 51314, 1133, 309, 311, 3671, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 805, 13, 51464, 51464, 400, 370, 264, 2316, 1669, 1080, 3537, 1920, 7696, 2361, 322, 341, 40468, 399, 300, 321, 3869, 281, 264, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07950130559630313, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.2002068615402095e-05}, {"id": 373, "seek": 264900, "start": 2656.0, "end": 2664.0, "text": " To the logistic regression model, this image of the sign of the weights is the most salient thing that could ever appear in the image.", "tokens": [50364, 400, 544, 420, 1570, 6608, 309, 484, 13, 467, 1177, 380, 574, 4098, 1880, 281, 505, 13, 467, 1177, 380, 4444, 527, 3202, 13, 50714, 50714, 1407, 264, 3565, 3142, 24590, 2316, 11, 341, 3256, 295, 264, 1465, 295, 264, 17443, 307, 264, 881, 1845, 1196, 551, 300, 727, 1562, 4204, 294, 264, 3256, 13, 51114, 51114, 1133, 309, 311, 3353, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 1614, 13, 51314, 51314, 1133, 309, 311, 3671, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 805, 13, 51464, 51464, 400, 370, 264, 2316, 1669, 1080, 3537, 1920, 7696, 2361, 322, 341, 40468, 399, 300, 321, 3869, 281, 264, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07950130559630313, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.2002068615402095e-05}, {"id": 374, "seek": 264900, "start": 2664.0, "end": 2668.0, "text": " When it's positive, it looks like the world's most quintessential 7.", "tokens": [50364, 400, 544, 420, 1570, 6608, 309, 484, 13, 467, 1177, 380, 574, 4098, 1880, 281, 505, 13, 467, 1177, 380, 4444, 527, 3202, 13, 50714, 50714, 1407, 264, 3565, 3142, 24590, 2316, 11, 341, 3256, 295, 264, 1465, 295, 264, 17443, 307, 264, 881, 1845, 1196, 551, 300, 727, 1562, 4204, 294, 264, 3256, 13, 51114, 51114, 1133, 309, 311, 3353, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 1614, 13, 51314, 51314, 1133, 309, 311, 3671, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 805, 13, 51464, 51464, 400, 370, 264, 2316, 1669, 1080, 3537, 1920, 7696, 2361, 322, 341, 40468, 399, 300, 321, 3869, 281, 264, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07950130559630313, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.2002068615402095e-05}, {"id": 375, "seek": 264900, "start": 2668.0, "end": 2671.0, "text": " When it's negative, it looks like the world's most quintessential 3.", "tokens": [50364, 400, 544, 420, 1570, 6608, 309, 484, 13, 467, 1177, 380, 574, 4098, 1880, 281, 505, 13, 467, 1177, 380, 4444, 527, 3202, 13, 50714, 50714, 1407, 264, 3565, 3142, 24590, 2316, 11, 341, 3256, 295, 264, 1465, 295, 264, 17443, 307, 264, 881, 1845, 1196, 551, 300, 727, 1562, 4204, 294, 264, 3256, 13, 51114, 51114, 1133, 309, 311, 3353, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 1614, 13, 51314, 51314, 1133, 309, 311, 3671, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 805, 13, 51464, 51464, 400, 370, 264, 2316, 1669, 1080, 3537, 1920, 7696, 2361, 322, 341, 40468, 399, 300, 321, 3869, 281, 264, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07950130559630313, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.2002068615402095e-05}, {"id": 376, "seek": 264900, "start": 2671.0, "end": 2676.0, "text": " And so the model makes its decision almost entirely based on this perturbation that we added to the image.", "tokens": [50364, 400, 544, 420, 1570, 6608, 309, 484, 13, 467, 1177, 380, 574, 4098, 1880, 281, 505, 13, 467, 1177, 380, 4444, 527, 3202, 13, 50714, 50714, 1407, 264, 3565, 3142, 24590, 2316, 11, 341, 3256, 295, 264, 1465, 295, 264, 17443, 307, 264, 881, 1845, 1196, 551, 300, 727, 1562, 4204, 294, 264, 3256, 13, 51114, 51114, 1133, 309, 311, 3353, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 1614, 13, 51314, 51314, 1133, 309, 311, 3671, 11, 309, 1542, 411, 264, 1002, 311, 881, 40006, 48143, 805, 13, 51464, 51464, 400, 370, 264, 2316, 1669, 1080, 3537, 1920, 7696, 2361, 322, 341, 40468, 399, 300, 321, 3869, 281, 264, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07950130559630313, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.2002068615402095e-05}, {"id": 377, "seek": 267600, "start": 2676.0, "end": 2679.0, "text": " But then on the backrest.", "tokens": [50364, 583, 550, 322, 264, 646, 4149, 13, 50514, 50514, 509, 393, 611, 747, 264, 912, 10747, 293, 452, 13532, 20667, 412, 7238, 48698, 4712, 577, 291, 393, 16927, 264, 3256, 322, 3256, 2533, 1228, 264, 912, 3109, 293, 1261, 341, 3821, 11608, 666, 257, 1120, 14169, 13, 51164, 51164, 1436, 3256, 2533, 307, 709, 2946, 18795, 11, 291, 500, 380, 643, 281, 764, 1596, 382, 2416, 295, 257, 17619, 322, 264, 3256, 295, 264, 17443, 13, 51514, 51514, 407, 321, 393, 652, 257, 544, 16336, 23686, 11, 1577, 12, 45390, 2690, 13, 51714, 51714], "temperature": 0.4, "avg_logprob": -0.23514382044474283, "compression_ratio": 1.5967078189300412, "no_speech_prob": 2.7785481506725773e-05}, {"id": 378, "seek": 267600, "start": 2679.0, "end": 2692.0, "text": " You can also take the same procedure and my colleague Andre at OpenAI showed how you can modify the image on image net using the same approach and turn this goldfish into a daisy.", "tokens": [50364, 583, 550, 322, 264, 646, 4149, 13, 50514, 50514, 509, 393, 611, 747, 264, 912, 10747, 293, 452, 13532, 20667, 412, 7238, 48698, 4712, 577, 291, 393, 16927, 264, 3256, 322, 3256, 2533, 1228, 264, 912, 3109, 293, 1261, 341, 3821, 11608, 666, 257, 1120, 14169, 13, 51164, 51164, 1436, 3256, 2533, 307, 709, 2946, 18795, 11, 291, 500, 380, 643, 281, 764, 1596, 382, 2416, 295, 257, 17619, 322, 264, 3256, 295, 264, 17443, 13, 51514, 51514, 407, 321, 393, 652, 257, 544, 16336, 23686, 11, 1577, 12, 45390, 2690, 13, 51714, 51714], "temperature": 0.4, "avg_logprob": -0.23514382044474283, "compression_ratio": 1.5967078189300412, "no_speech_prob": 2.7785481506725773e-05}, {"id": 379, "seek": 267600, "start": 2692.0, "end": 2699.0, "text": " Because image net is much higher dimensional, you don't need to use quite as large of a coefficient on the image of the weights.", "tokens": [50364, 583, 550, 322, 264, 646, 4149, 13, 50514, 50514, 509, 393, 611, 747, 264, 912, 10747, 293, 452, 13532, 20667, 412, 7238, 48698, 4712, 577, 291, 393, 16927, 264, 3256, 322, 3256, 2533, 1228, 264, 912, 3109, 293, 1261, 341, 3821, 11608, 666, 257, 1120, 14169, 13, 51164, 51164, 1436, 3256, 2533, 307, 709, 2946, 18795, 11, 291, 500, 380, 643, 281, 764, 1596, 382, 2416, 295, 257, 17619, 322, 264, 3256, 295, 264, 17443, 13, 51514, 51514, 407, 321, 393, 652, 257, 544, 16336, 23686, 11, 1577, 12, 45390, 2690, 13, 51714, 51714], "temperature": 0.4, "avg_logprob": -0.23514382044474283, "compression_ratio": 1.5967078189300412, "no_speech_prob": 2.7785481506725773e-05}, {"id": 380, "seek": 267600, "start": 2699.0, "end": 2703.0, "text": " So we can make a more persuasive, full-length attack.", "tokens": [50364, 583, 550, 322, 264, 646, 4149, 13, 50514, 50514, 509, 393, 611, 747, 264, 912, 10747, 293, 452, 13532, 20667, 412, 7238, 48698, 4712, 577, 291, 393, 16927, 264, 3256, 322, 3256, 2533, 1228, 264, 912, 3109, 293, 1261, 341, 3821, 11608, 666, 257, 1120, 14169, 13, 51164, 51164, 1436, 3256, 2533, 307, 709, 2946, 18795, 11, 291, 500, 380, 643, 281, 764, 1596, 382, 2416, 295, 257, 17619, 322, 264, 3256, 295, 264, 17443, 13, 51514, 51514, 407, 321, 393, 652, 257, 544, 16336, 23686, 11, 1577, 12, 45390, 2690, 13, 51714, 51714], "temperature": 0.4, "avg_logprob": -0.23514382044474283, "compression_ratio": 1.5967078189300412, "no_speech_prob": 2.7785481506725773e-05}, {"id": 381, "seek": 270300, "start": 2703.0, "end": 2712.0, "text": " You can see that the same image of the weights when applied to any different input image will actually reliably cause a misclassification.", "tokens": [50364, 509, 393, 536, 300, 264, 912, 3256, 295, 264, 17443, 562, 6456, 281, 604, 819, 4846, 3256, 486, 767, 49927, 3082, 257, 3346, 11665, 3774, 13, 50814, 50814, 708, 311, 516, 322, 307, 300, 456, 366, 867, 819, 5359, 13, 51014, 51014, 400, 309, 1355, 300, 498, 291, 2826, 264, 17443, 337, 604, 1729, 1508, 11, 309, 311, 588, 17518, 300, 257, 777, 1500, 3256, 486, 5784, 281, 300, 1508, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07113803227742513, "compression_ratio": 1.67, "no_speech_prob": 1.8110456949216314e-05}, {"id": 382, "seek": 270300, "start": 2712.0, "end": 2716.0, "text": " What's going on is that there are many different classes.", "tokens": [50364, 509, 393, 536, 300, 264, 912, 3256, 295, 264, 17443, 562, 6456, 281, 604, 819, 4846, 3256, 486, 767, 49927, 3082, 257, 3346, 11665, 3774, 13, 50814, 50814, 708, 311, 516, 322, 307, 300, 456, 366, 867, 819, 5359, 13, 51014, 51014, 400, 309, 1355, 300, 498, 291, 2826, 264, 17443, 337, 604, 1729, 1508, 11, 309, 311, 588, 17518, 300, 257, 777, 1500, 3256, 486, 5784, 281, 300, 1508, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07113803227742513, "compression_ratio": 1.67, "no_speech_prob": 1.8110456949216314e-05}, {"id": 383, "seek": 270300, "start": 2716.0, "end": 2726.0, "text": " And it means that if you choose the weights for any particular class, it's very unlikely that a new test image will belong to that class.", "tokens": [50364, 509, 393, 536, 300, 264, 912, 3256, 295, 264, 17443, 562, 6456, 281, 604, 819, 4846, 3256, 486, 767, 49927, 3082, 257, 3346, 11665, 3774, 13, 50814, 50814, 708, 311, 516, 322, 307, 300, 456, 366, 867, 819, 5359, 13, 51014, 51014, 400, 309, 1355, 300, 498, 291, 2826, 264, 17443, 337, 604, 1729, 1508, 11, 309, 311, 588, 17518, 300, 257, 777, 1500, 3256, 486, 5784, 281, 300, 1508, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07113803227742513, "compression_ratio": 1.67, "no_speech_prob": 1.8110456949216314e-05}, {"id": 384, "seek": 272600, "start": 2726.0, "end": 2737.0, "text": " So an image net, if we're using the weights for the daisy class, and there are a thousand different classes, then we have about a 99.9% chance that a test image will not be a daisy.", "tokens": [50364, 407, 364, 3256, 2533, 11, 498, 321, 434, 1228, 264, 17443, 337, 264, 1120, 14169, 1508, 11, 293, 456, 366, 257, 4714, 819, 5359, 11, 550, 321, 362, 466, 257, 11803, 13, 24, 4, 2931, 300, 257, 1500, 3256, 486, 406, 312, 257, 1120, 14169, 13, 50914, 50914, 759, 321, 550, 352, 2286, 293, 909, 264, 17443, 337, 264, 1120, 14169, 1508, 281, 300, 3256, 11, 550, 321, 483, 257, 1120, 14169, 11, 293, 570, 300, 311, 406, 264, 3006, 1508, 11, 309, 311, 257, 3346, 11665, 3774, 13, 51364, 51364, 407, 456, 311, 257, 3035, 412, 22995, 15958, 341, 1064, 1219, 22617, 1999, 840, 44745, 3026, 391, 65, 763, 300, 33706, 257, 688, 544, 322, 341, 14816, 300, 321, 632, 516, 646, 294, 8227, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1032499386714055, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.585913070011884e-06}, {"id": 385, "seek": 272600, "start": 2737.0, "end": 2746.0, "text": " If we then go ahead and add the weights for the daisy class to that image, then we get a daisy, and because that's not the correct class, it's a misclassification.", "tokens": [50364, 407, 364, 3256, 2533, 11, 498, 321, 434, 1228, 264, 17443, 337, 264, 1120, 14169, 1508, 11, 293, 456, 366, 257, 4714, 819, 5359, 11, 550, 321, 362, 466, 257, 11803, 13, 24, 4, 2931, 300, 257, 1500, 3256, 486, 406, 312, 257, 1120, 14169, 13, 50914, 50914, 759, 321, 550, 352, 2286, 293, 909, 264, 17443, 337, 264, 1120, 14169, 1508, 281, 300, 3256, 11, 550, 321, 483, 257, 1120, 14169, 11, 293, 570, 300, 311, 406, 264, 3006, 1508, 11, 309, 311, 257, 3346, 11665, 3774, 13, 51364, 51364, 407, 456, 311, 257, 3035, 412, 22995, 15958, 341, 1064, 1219, 22617, 1999, 840, 44745, 3026, 391, 65, 763, 300, 33706, 257, 688, 544, 322, 341, 14816, 300, 321, 632, 516, 646, 294, 8227, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1032499386714055, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.585913070011884e-06}, {"id": 386, "seek": 272600, "start": 2746.0, "end": 2754.0, "text": " So there's a paper at CVPR this year called Universal Adversarial Perterbations that expands a lot more on this observation that we had going back in 2014.", "tokens": [50364, 407, 364, 3256, 2533, 11, 498, 321, 434, 1228, 264, 17443, 337, 264, 1120, 14169, 1508, 11, 293, 456, 366, 257, 4714, 819, 5359, 11, 550, 321, 362, 466, 257, 11803, 13, 24, 4, 2931, 300, 257, 1500, 3256, 486, 406, 312, 257, 1120, 14169, 13, 50914, 50914, 759, 321, 550, 352, 2286, 293, 909, 264, 17443, 337, 264, 1120, 14169, 1508, 281, 300, 3256, 11, 550, 321, 483, 257, 1120, 14169, 11, 293, 570, 300, 311, 406, 264, 3006, 1508, 11, 309, 311, 257, 3346, 11665, 3774, 13, 51364, 51364, 407, 456, 311, 257, 3035, 412, 22995, 15958, 341, 1064, 1219, 22617, 1999, 840, 44745, 3026, 391, 65, 763, 300, 33706, 257, 688, 544, 322, 341, 14816, 300, 321, 632, 516, 646, 294, 8227, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1032499386714055, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.585913070011884e-06}, {"id": 387, "seek": 275400, "start": 2754.0, "end": 2765.0, "text": " But basically these weight vectors, when applied to many different images, can cause misclassification in all of them.", "tokens": [50364, 583, 1936, 613, 3364, 18875, 11, 562, 6456, 281, 867, 819, 5267, 11, 393, 3082, 3346, 11665, 3774, 294, 439, 295, 552, 13, 50914, 50914, 286, 600, 4418, 257, 688, 295, 565, 3585, 291, 300, 613, 8213, 5245, 366, 445, 6237, 11, 293, 412, 512, 935, 291, 600, 1391, 668, 7159, 286, 603, 976, 291, 512, 1333, 295, 257, 1969, 5120, 281, 13447, 291, 300, 456, 311, 1071, 2316, 300, 311, 406, 6237, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09975877174964318, "compression_ratio": 1.59375, "no_speech_prob": 1.6057347238529474e-05}, {"id": 388, "seek": 275400, "start": 2765.0, "end": 2777.0, "text": " I've spent a lot of time telling you that these linear models are just terrible, and at some point you've probably been hoping I'll give you some sort of a control experiment to convince you that there's another model that's not terrible.", "tokens": [50364, 583, 1936, 613, 3364, 18875, 11, 562, 6456, 281, 867, 819, 5267, 11, 393, 3082, 3346, 11665, 3774, 294, 439, 295, 552, 13, 50914, 50914, 286, 600, 4418, 257, 688, 295, 565, 3585, 291, 300, 613, 8213, 5245, 366, 445, 6237, 11, 293, 412, 512, 935, 291, 600, 1391, 668, 7159, 286, 603, 976, 291, 512, 1333, 295, 257, 1969, 5120, 281, 13447, 291, 300, 456, 311, 1071, 2316, 300, 311, 406, 6237, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09975877174964318, "compression_ratio": 1.59375, "no_speech_prob": 1.6057347238529474e-05}, {"id": 389, "seek": 277700, "start": 2777.0, "end": 2788.0, "text": " So it turns out that some quadratic models actually perform really well. In particular, a shallow RBF network is able to resist adversarial perturbations very well.", "tokens": [50364, 407, 309, 4523, 484, 300, 512, 37262, 5245, 767, 2042, 534, 731, 13, 682, 1729, 11, 257, 20488, 40302, 37, 3209, 307, 1075, 281, 4597, 17641, 44745, 40468, 763, 588, 731, 13, 50914, 50914, 24552, 286, 4712, 291, 364, 9603, 689, 286, 1890, 257, 1722, 293, 286, 3574, 309, 666, 257, 1958, 11, 502, 11, 568, 11, 293, 370, 322, 11, 1553, 534, 4473, 1080, 8967, 412, 439, 13, 51264, 51264, 400, 286, 390, 1075, 281, 7979, 257, 8213, 2787, 41167, 24590, 1508, 9902, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11506845709982882, "compression_ratio": 1.5206611570247934, "no_speech_prob": 2.4792143449303694e-05}, {"id": 390, "seek": 277700, "start": 2788.0, "end": 2795.0, "text": " Earlier I showed you an animation where I took a 9 and I turned it into a 0, 1, 2, and so on, without really changing its appearance at all.", "tokens": [50364, 407, 309, 4523, 484, 300, 512, 37262, 5245, 767, 2042, 534, 731, 13, 682, 1729, 11, 257, 20488, 40302, 37, 3209, 307, 1075, 281, 4597, 17641, 44745, 40468, 763, 588, 731, 13, 50914, 50914, 24552, 286, 4712, 291, 364, 9603, 689, 286, 1890, 257, 1722, 293, 286, 3574, 309, 666, 257, 1958, 11, 502, 11, 568, 11, 293, 370, 322, 11, 1553, 534, 4473, 1080, 8967, 412, 439, 13, 51264, 51264, 400, 286, 390, 1075, 281, 7979, 257, 8213, 2787, 41167, 24590, 1508, 9902, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11506845709982882, "compression_ratio": 1.5206611570247934, "no_speech_prob": 2.4792143449303694e-05}, {"id": 391, "seek": 277700, "start": 2795.0, "end": 2800.0, "text": " And I was able to fool a linear softmax regression classifier.", "tokens": [50364, 407, 309, 4523, 484, 300, 512, 37262, 5245, 767, 2042, 534, 731, 13, 682, 1729, 11, 257, 20488, 40302, 37, 3209, 307, 1075, 281, 4597, 17641, 44745, 40468, 763, 588, 731, 13, 50914, 50914, 24552, 286, 4712, 291, 364, 9603, 689, 286, 1890, 257, 1722, 293, 286, 3574, 309, 666, 257, 1958, 11, 502, 11, 568, 11, 293, 370, 322, 11, 1553, 534, 4473, 1080, 8967, 412, 439, 13, 51264, 51264, 400, 286, 390, 1075, 281, 7979, 257, 8213, 2787, 41167, 24590, 1508, 9902, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11506845709982882, "compression_ratio": 1.5206611570247934, "no_speech_prob": 2.4792143449303694e-05}, {"id": 392, "seek": 280000, "start": 2800.0, "end": 2816.0, "text": " Here I've got an RBF network where it outputs a separate probability of each class being absent or present. And that probability is given by e to the negative square of the difference between a template image and the input image.", "tokens": [50364, 1692, 286, 600, 658, 364, 40302, 37, 3209, 689, 309, 23930, 257, 4994, 8482, 295, 1184, 1508, 885, 25185, 420, 1974, 13, 400, 300, 8482, 307, 2212, 538, 308, 281, 264, 3671, 3732, 295, 264, 2649, 1296, 257, 12379, 3256, 293, 264, 4846, 3256, 13, 51164, 51164, 400, 498, 321, 767, 1524, 264, 16235, 295, 341, 1508, 9902, 11, 309, 775, 767, 1261, 264, 3256, 666, 257, 1958, 11, 257, 502, 11, 257, 568, 11, 257, 805, 11, 293, 370, 322, 13, 400, 321, 393, 767, 5521, 729, 2962, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0776422733956195, "compression_ratio": 1.6104417670682731, "no_speech_prob": 1.6047559256548993e-05}, {"id": 393, "seek": 280000, "start": 2816.0, "end": 2828.0, "text": " And if we actually follow the gradient of this classifier, it does actually turn the image into a 0, a 1, a 2, a 3, and so on. And we can actually recognize those changes.", "tokens": [50364, 1692, 286, 600, 658, 364, 40302, 37, 3209, 689, 309, 23930, 257, 4994, 8482, 295, 1184, 1508, 885, 25185, 420, 1974, 13, 400, 300, 8482, 307, 2212, 538, 308, 281, 264, 3671, 3732, 295, 264, 2649, 1296, 257, 12379, 3256, 293, 264, 4846, 3256, 13, 51164, 51164, 400, 498, 321, 767, 1524, 264, 16235, 295, 341, 1508, 9902, 11, 309, 775, 767, 1261, 264, 3256, 666, 257, 1958, 11, 257, 502, 11, 257, 568, 11, 257, 805, 11, 293, 370, 322, 13, 400, 321, 393, 767, 5521, 729, 2962, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0776422733956195, "compression_ratio": 1.6104417670682731, "no_speech_prob": 1.6047559256548993e-05}, {"id": 394, "seek": 282800, "start": 2828.0, "end": 2838.0, "text": " The problem is this classifier does not get very good accuracy on the training set. It's a shallow model. It's basically just a template matcher. It is literally a template matcher.", "tokens": [50364, 440, 1154, 307, 341, 1508, 9902, 775, 406, 483, 588, 665, 14170, 322, 264, 3097, 992, 13, 467, 311, 257, 20488, 2316, 13, 467, 311, 1936, 445, 257, 12379, 2995, 260, 13, 467, 307, 3736, 257, 12379, 2995, 260, 13, 50864, 50864, 400, 498, 291, 853, 281, 652, 309, 544, 16950, 538, 1455, 309, 7731, 11, 309, 4523, 484, 300, 264, 16235, 295, 613, 40302, 37, 6815, 307, 1958, 11, 420, 588, 2651, 1958, 11, 3710, 881, 295, 527, 917, 13, 407, 436, 434, 4664, 2252, 281, 3847, 11, 754, 365, 15245, 2710, 2144, 293, 7150, 411, 300, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07466378721218665, "compression_ratio": 1.5978260869565217, "no_speech_prob": 5.5940035963431e-06}, {"id": 395, "seek": 282800, "start": 2838.0, "end": 2857.0, "text": " And if you try to make it more sophisticated by making it deeper, it turns out that the gradient of these RBF units is 0, or very near 0, throughout most of our end. So they're extremely difficult to train, even with batch normalization and methods like that.", "tokens": [50364, 440, 1154, 307, 341, 1508, 9902, 775, 406, 483, 588, 665, 14170, 322, 264, 3097, 992, 13, 467, 311, 257, 20488, 2316, 13, 467, 311, 1936, 445, 257, 12379, 2995, 260, 13, 467, 307, 3736, 257, 12379, 2995, 260, 13, 50864, 50864, 400, 498, 291, 853, 281, 652, 309, 544, 16950, 538, 1455, 309, 7731, 11, 309, 4523, 484, 300, 264, 16235, 295, 613, 40302, 37, 6815, 307, 1958, 11, 420, 588, 2651, 1958, 11, 3710, 881, 295, 527, 917, 13, 407, 436, 434, 4664, 2252, 281, 3847, 11, 754, 365, 15245, 2710, 2144, 293, 7150, 411, 300, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07466378721218665, "compression_ratio": 1.5978260869565217, "no_speech_prob": 5.5940035963431e-06}, {"id": 396, "seek": 285700, "start": 2857.0, "end": 2872.0, "text": " I haven't managed to train a deep RBF network yet. But I think if somebody comes up with better hyper parameters or a new more powerful optimization algorithm, it might be possible to solve the adversarial example problem by training a deep RBF network.", "tokens": [50364, 286, 2378, 380, 6453, 281, 3847, 257, 2452, 40302, 37, 3209, 1939, 13, 583, 286, 519, 498, 2618, 1487, 493, 365, 1101, 9848, 9834, 420, 257, 777, 544, 4005, 19618, 9284, 11, 309, 1062, 312, 1944, 281, 5039, 264, 17641, 44745, 1365, 1154, 538, 3097, 257, 2452, 40302, 37, 3209, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.10309463847767222, "compression_ratio": 1.4375, "no_speech_prob": 2.741433672781568e-05}, {"id": 397, "seek": 287200, "start": 2872.0, "end": 2886.0, "text": " Where the model is so nonlinear and has such wide flat areas that the adversary is not able to push the cost up hill just by making small changes to the models input.", "tokens": [50364, 2305, 264, 2316, 307, 370, 2107, 28263, 293, 575, 1270, 4874, 4962, 3179, 300, 264, 48222, 307, 406, 1075, 281, 2944, 264, 2063, 493, 10997, 445, 538, 1455, 1359, 2962, 281, 264, 5245, 4846, 13, 51064, 51064, 1485, 295, 264, 721, 300, 311, 264, 881, 44043, 466, 17641, 44745, 5110, 307, 300, 436, 2674, 1125, 490, 472, 1412, 992, 281, 1071, 412, 472, 2316, 281, 1071, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09257395166746328, "compression_ratio": 1.6354166666666667, "no_speech_prob": 2.0525360014289618e-05}, {"id": 398, "seek": 287200, "start": 2886.0, "end": 2894.0, "text": " One of the things that's the most alarming about adversarial examples is that they generalize from one data set to another at one model to another.", "tokens": [50364, 2305, 264, 2316, 307, 370, 2107, 28263, 293, 575, 1270, 4874, 4962, 3179, 300, 264, 48222, 307, 406, 1075, 281, 2944, 264, 2063, 493, 10997, 445, 538, 1455, 1359, 2962, 281, 264, 5245, 4846, 13, 51064, 51064, 1485, 295, 264, 721, 300, 311, 264, 881, 44043, 466, 17641, 44745, 5110, 307, 300, 436, 2674, 1125, 490, 472, 1412, 992, 281, 1071, 412, 472, 2316, 281, 1071, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09257395166746328, "compression_ratio": 1.6354166666666667, "no_speech_prob": 2.0525360014289618e-05}, {"id": 399, "seek": 289400, "start": 2894.0, "end": 2908.0, "text": " Here I've trained two different models on two different training sets. The training sets are tiny in both cases. It's just Mness 3 versus 7 classification. And this is really just for the purpose of making a slide.", "tokens": [50364, 1692, 286, 600, 8895, 732, 819, 5245, 322, 732, 819, 3097, 6352, 13, 440, 3097, 6352, 366, 5870, 294, 1293, 3331, 13, 467, 311, 445, 376, 1287, 805, 5717, 1614, 21538, 13, 400, 341, 307, 534, 445, 337, 264, 4334, 295, 1455, 257, 4137, 13, 51064, 51064, 759, 291, 3847, 257, 3565, 3142, 24590, 2316, 322, 264, 27011, 4898, 294, 264, 1411, 4831, 11, 291, 483, 264, 17443, 4898, 294, 264, 1411, 322, 264, 3126, 4831, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14089123996687525, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.735180457122624e-05}, {"id": 400, "seek": 289400, "start": 2908.0, "end": 2917.0, "text": " If you train a logistic regression model on the digits shown in the left panel, you get the weights shown in the left on the lower panel.", "tokens": [50364, 1692, 286, 600, 8895, 732, 819, 5245, 322, 732, 819, 3097, 6352, 13, 440, 3097, 6352, 366, 5870, 294, 1293, 3331, 13, 467, 311, 445, 376, 1287, 805, 5717, 1614, 21538, 13, 400, 341, 307, 534, 445, 337, 264, 4334, 295, 1455, 257, 4137, 13, 51064, 51064, 759, 291, 3847, 257, 3565, 3142, 24590, 2316, 322, 264, 27011, 4898, 294, 264, 1411, 4831, 11, 291, 483, 264, 17443, 4898, 294, 264, 1411, 322, 264, 3126, 4831, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14089123996687525, "compression_ratio": 1.6682464454976302, "no_speech_prob": 5.735180457122624e-05}, {"id": 401, "seek": 291700, "start": 2917.0, "end": 2932.0, "text": " So you've got two different training sets and we learn weight vectors that look very similar to each other. That's just because Mness 3 learning algorithms generalize.", "tokens": [50364, 407, 291, 600, 658, 732, 819, 3097, 6352, 293, 321, 1466, 3364, 18875, 300, 574, 588, 2531, 281, 1184, 661, 13, 663, 311, 445, 570, 376, 1287, 805, 2539, 14642, 2674, 1125, 13, 51114, 51114, 509, 528, 552, 281, 1466, 257, 2445, 300, 311, 8344, 6695, 295, 264, 1412, 300, 291, 3847, 552, 322, 13, 467, 4659, 380, 1871, 597, 1729, 3097, 5110, 291, 2826, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15373304912022182, "compression_ratio": 1.5159817351598173, "no_speech_prob": 1.351534592686221e-05}, {"id": 402, "seek": 291700, "start": 2932.0, "end": 2940.0, "text": " You want them to learn a function that's somewhat independent of the data that you train them on. It shouldn't matter which particular training examples you choose.", "tokens": [50364, 407, 291, 600, 658, 732, 819, 3097, 6352, 293, 321, 1466, 3364, 18875, 300, 574, 588, 2531, 281, 1184, 661, 13, 663, 311, 445, 570, 376, 1287, 805, 2539, 14642, 2674, 1125, 13, 51114, 51114, 509, 528, 552, 281, 1466, 257, 2445, 300, 311, 8344, 6695, 295, 264, 1412, 300, 291, 3847, 552, 322, 13, 467, 4659, 380, 1871, 597, 1729, 3097, 5110, 291, 2826, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15373304912022182, "compression_ratio": 1.5159817351598173, "no_speech_prob": 1.351534592686221e-05}, {"id": 403, "seek": 294000, "start": 2940.0, "end": 2953.0, "text": " The training set to the test set, you've also got to expect the different training sets will give you more or less the same result. And that means that because they've learned more or less similar functions, they're vulnerable to similar adversarial examples.", "tokens": [50364, 440, 3097, 992, 281, 264, 1500, 992, 11, 291, 600, 611, 658, 281, 2066, 264, 819, 3097, 6352, 486, 976, 291, 544, 420, 1570, 264, 912, 1874, 13, 400, 300, 1355, 300, 570, 436, 600, 3264, 544, 420, 1570, 2531, 6828, 11, 436, 434, 10955, 281, 2531, 17641, 44745, 5110, 13, 51014, 51014, 440, 912, 48222, 11, 364, 48222, 393, 14722, 364, 3256, 300, 38625, 472, 293, 764, 309, 281, 7979, 264, 661, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13544542361528444, "compression_ratio": 1.7378640776699028, "no_speech_prob": 5.325574966263957e-05}, {"id": 404, "seek": 294000, "start": 2953.0, "end": 2960.0, "text": " The same adversary, an adversary can compute an image that fools one and use it to fool the other.", "tokens": [50364, 440, 3097, 992, 281, 264, 1500, 992, 11, 291, 600, 611, 658, 281, 2066, 264, 819, 3097, 6352, 486, 976, 291, 544, 420, 1570, 264, 912, 1874, 13, 400, 300, 1355, 300, 570, 436, 600, 3264, 544, 420, 1570, 2531, 6828, 11, 436, 434, 10955, 281, 2531, 17641, 44745, 5110, 13, 51014, 51014, 440, 912, 48222, 11, 364, 48222, 393, 14722, 364, 3256, 300, 38625, 472, 293, 764, 309, 281, 7979, 264, 661, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13544542361528444, "compression_ratio": 1.7378640776699028, "no_speech_prob": 5.325574966263957e-05}, {"id": 405, "seek": 296000, "start": 2960.0, "end": 2972.0, "text": " We can actually go ahead and measure the transfer rate between several different machine learning techniques, not just different data sets.", "tokens": [50364, 492, 393, 767, 352, 2286, 293, 3481, 264, 5003, 3314, 1296, 2940, 819, 3479, 2539, 7512, 11, 406, 445, 819, 1412, 6352, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.18898776725486474, "compression_ratio": 1.3113207547169812, "no_speech_prob": 5.6363373005297035e-05}, {"id": 406, "seek": 297200, "start": 2972.0, "end": 2992.0, "text": " And they found that for example, logistic regression makes adversarial examples that transfer to decision trees with 87.4% probability. Wherever you see dark squares in this matrix, that shows that there's a high amount of transfer.", "tokens": [50364, 400, 436, 1352, 300, 337, 1365, 11, 3565, 3142, 24590, 1669, 17641, 44745, 5110, 300, 5003, 281, 3537, 5852, 365, 27990, 13, 19, 4, 8482, 13, 30903, 291, 536, 2877, 19368, 294, 341, 8141, 11, 300, 3110, 300, 456, 311, 257, 1090, 2372, 295, 5003, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.18945377349853515, "compression_ratio": 1.4320987654320987, "no_speech_prob": 1.8003693185164593e-05}, {"id": 407, "seek": 299200, "start": 2992.0, "end": 3003.0, "text": " And it's very possible for an attacker using the model on the left to create adversarial examples for the model on the right.", "tokens": [50364, 400, 309, 311, 588, 1944, 337, 364, 35871, 1228, 264, 2316, 322, 264, 1411, 281, 1884, 17641, 44745, 5110, 337, 264, 2316, 322, 264, 558, 13, 50914, 50914, 440, 10747, 4787, 307, 300, 7297, 264, 35871, 2738, 281, 7979, 257, 2316, 300, 436, 500, 380, 767, 362, 2105, 281, 13, 51214, 51214, 814, 500, 380, 458, 264, 9482, 300, 311, 1143, 281, 3847, 264, 2316, 13, 814, 815, 406, 754, 458, 597, 9284, 307, 885, 1143, 13, 814, 815, 406, 458, 1968, 436, 434, 15010, 257, 3537, 4230, 420, 257, 2452, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11762628749925264, "compression_ratio": 1.8032786885245902, "no_speech_prob": 8.454864291707054e-05}, {"id": 408, "seek": 299200, "start": 3003.0, "end": 3009.0, "text": " The procedure overall is that suppose the attacker wants to fool a model that they don't actually have access to.", "tokens": [50364, 400, 309, 311, 588, 1944, 337, 364, 35871, 1228, 264, 2316, 322, 264, 1411, 281, 1884, 17641, 44745, 5110, 337, 264, 2316, 322, 264, 558, 13, 50914, 50914, 440, 10747, 4787, 307, 300, 7297, 264, 35871, 2738, 281, 7979, 257, 2316, 300, 436, 500, 380, 767, 362, 2105, 281, 13, 51214, 51214, 814, 500, 380, 458, 264, 9482, 300, 311, 1143, 281, 3847, 264, 2316, 13, 814, 815, 406, 754, 458, 597, 9284, 307, 885, 1143, 13, 814, 815, 406, 458, 1968, 436, 434, 15010, 257, 3537, 4230, 420, 257, 2452, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11762628749925264, "compression_ratio": 1.8032786885245902, "no_speech_prob": 8.454864291707054e-05}, {"id": 409, "seek": 299200, "start": 3009.0, "end": 3019.0, "text": " They don't know the architecture that's used to train the model. They may not even know which algorithm is being used. They may not know whether they're attacking a decision tree or a deep neural net.", "tokens": [50364, 400, 309, 311, 588, 1944, 337, 364, 35871, 1228, 264, 2316, 322, 264, 1411, 281, 1884, 17641, 44745, 5110, 337, 264, 2316, 322, 264, 558, 13, 50914, 50914, 440, 10747, 4787, 307, 300, 7297, 264, 35871, 2738, 281, 7979, 257, 2316, 300, 436, 500, 380, 767, 362, 2105, 281, 13, 51214, 51214, 814, 500, 380, 458, 264, 9482, 300, 311, 1143, 281, 3847, 264, 2316, 13, 814, 815, 406, 754, 458, 597, 9284, 307, 885, 1143, 13, 814, 815, 406, 458, 1968, 436, 434, 15010, 257, 3537, 4230, 420, 257, 2452, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11762628749925264, "compression_ratio": 1.8032786885245902, "no_speech_prob": 8.454864291707054e-05}, {"id": 410, "seek": 301900, "start": 3019.0, "end": 3024.0, "text": " They also don't know the parameters of the model that they're going to attack.", "tokens": [50364, 814, 611, 500, 380, 458, 264, 9834, 295, 264, 2316, 300, 436, 434, 516, 281, 2690, 13, 50614, 50614, 407, 437, 436, 393, 360, 307, 436, 393, 3847, 641, 1065, 2316, 300, 436, 528, 281, 11, 300, 436, 603, 764, 281, 1322, 264, 2690, 13, 50964, 50964, 821, 311, 732, 819, 2098, 291, 393, 3847, 428, 1065, 2316, 13, 708, 307, 291, 393, 7645, 428, 1065, 3097, 992, 337, 264, 912, 5633, 300, 291, 528, 281, 2690, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07189523301473479, "compression_ratio": 1.819672131147541, "no_speech_prob": 3.5839540942106396e-05}, {"id": 411, "seek": 301900, "start": 3024.0, "end": 3031.0, "text": " So what they can do is they can train their own model that they want to, that they'll use to build the attack.", "tokens": [50364, 814, 611, 500, 380, 458, 264, 9834, 295, 264, 2316, 300, 436, 434, 516, 281, 2690, 13, 50614, 50614, 407, 437, 436, 393, 360, 307, 436, 393, 3847, 641, 1065, 2316, 300, 436, 528, 281, 11, 300, 436, 603, 764, 281, 1322, 264, 2690, 13, 50964, 50964, 821, 311, 732, 819, 2098, 291, 393, 3847, 428, 1065, 2316, 13, 708, 307, 291, 393, 7645, 428, 1065, 3097, 992, 337, 264, 912, 5633, 300, 291, 528, 281, 2690, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07189523301473479, "compression_ratio": 1.819672131147541, "no_speech_prob": 3.5839540942106396e-05}, {"id": 412, "seek": 301900, "start": 3031.0, "end": 3037.0, "text": " There's two different ways you can train your own model. What is you can label your own training set for the same task that you want to attack?", "tokens": [50364, 814, 611, 500, 380, 458, 264, 9834, 295, 264, 2316, 300, 436, 434, 516, 281, 2690, 13, 50614, 50614, 407, 437, 436, 393, 360, 307, 436, 393, 3847, 641, 1065, 2316, 300, 436, 528, 281, 11, 300, 436, 603, 764, 281, 1322, 264, 2690, 13, 50964, 50964, 821, 311, 732, 819, 2098, 291, 393, 3847, 428, 1065, 2316, 13, 708, 307, 291, 393, 7645, 428, 1065, 3097, 992, 337, 264, 912, 5633, 300, 291, 528, 281, 2690, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07189523301473479, "compression_ratio": 1.819672131147541, "no_speech_prob": 3.5839540942106396e-05}, {"id": 413, "seek": 303700, "start": 3037.0, "end": 3051.0, "text": " Say that somebody is using an image net classifier and for whatever reason you don't have access to image net, you can take your own photos and label them train your own object recognizer. It's going to share adversarial examples with an image net model.", "tokens": [50364, 6463, 300, 2618, 307, 1228, 364, 3256, 2533, 1508, 9902, 293, 337, 2035, 1778, 291, 500, 380, 362, 2105, 281, 3256, 2533, 11, 291, 393, 747, 428, 1065, 5787, 293, 7645, 552, 3847, 428, 1065, 2657, 3068, 6545, 13, 467, 311, 516, 281, 2073, 17641, 44745, 5110, 365, 364, 3256, 2533, 2316, 13, 51064, 51064, 440, 661, 551, 291, 393, 360, 307, 584, 300, 291, 393, 380, 6157, 281, 5448, 428, 1065, 3097, 992, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1014511253260359, "compression_ratio": 1.6490384615384615, "no_speech_prob": 6.320379907265306e-05}, {"id": 414, "seek": 303700, "start": 3051.0, "end": 3056.0, "text": " The other thing you can do is say that you can't afford to gather your own training set.", "tokens": [50364, 6463, 300, 2618, 307, 1228, 364, 3256, 2533, 1508, 9902, 293, 337, 2035, 1778, 291, 500, 380, 362, 2105, 281, 3256, 2533, 11, 291, 393, 747, 428, 1065, 5787, 293, 7645, 552, 3847, 428, 1065, 2657, 3068, 6545, 13, 467, 311, 516, 281, 2073, 17641, 44745, 5110, 365, 364, 3256, 2533, 2316, 13, 51064, 51064, 440, 661, 551, 291, 393, 360, 307, 584, 300, 291, 393, 380, 6157, 281, 5448, 428, 1065, 3097, 992, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1014511253260359, "compression_ratio": 1.6490384615384615, "no_speech_prob": 6.320379907265306e-05}, {"id": 415, "seek": 305600, "start": 3056.0, "end": 3070.0, "text": " You can do instead is if you can get limited access to the model where you just have the ability to send inputs to the model to observe its outputs, then you can send those inputs, observe the outputs, and use those as your training set.", "tokens": [50364, 509, 393, 360, 2602, 307, 498, 291, 393, 483, 5567, 2105, 281, 264, 2316, 689, 291, 445, 362, 264, 3485, 281, 2845, 15743, 281, 264, 2316, 281, 11441, 1080, 23930, 11, 550, 291, 393, 2845, 729, 15743, 11, 11441, 264, 23930, 11, 293, 764, 729, 382, 428, 3097, 992, 13, 51064, 51064, 639, 486, 589, 754, 498, 264, 5598, 300, 291, 483, 490, 264, 3779, 2316, 307, 787, 264, 1508, 7645, 300, 309, 25963, 13, 51414, 51414, 316, 688, 295, 561, 1401, 341, 294, 257, 6552, 300, 291, 643, 281, 362, 2105, 281, 439, 264, 8482, 4190, 309, 23930, 11, 457, 754, 445, 264, 1508, 16949, 366, 11563, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09211268467185772, "compression_ratio": 1.9053030303030303, "no_speech_prob": 2.143249184882734e-05}, {"id": 416, "seek": 305600, "start": 3070.0, "end": 3077.0, "text": " This will work even if the output that you get from the target model is only the class label that it chooses.", "tokens": [50364, 509, 393, 360, 2602, 307, 498, 291, 393, 483, 5567, 2105, 281, 264, 2316, 689, 291, 445, 362, 264, 3485, 281, 2845, 15743, 281, 264, 2316, 281, 11441, 1080, 23930, 11, 550, 291, 393, 2845, 729, 15743, 11, 11441, 264, 23930, 11, 293, 764, 729, 382, 428, 3097, 992, 13, 51064, 51064, 639, 486, 589, 754, 498, 264, 5598, 300, 291, 483, 490, 264, 3779, 2316, 307, 787, 264, 1508, 7645, 300, 309, 25963, 13, 51414, 51414, 316, 688, 295, 561, 1401, 341, 294, 257, 6552, 300, 291, 643, 281, 362, 2105, 281, 439, 264, 8482, 4190, 309, 23930, 11, 457, 754, 445, 264, 1508, 16949, 366, 11563, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09211268467185772, "compression_ratio": 1.9053030303030303, "no_speech_prob": 2.143249184882734e-05}, {"id": 417, "seek": 305600, "start": 3077.0, "end": 3085.0, "text": " A lot of people read this in a assume that you need to have access to all the probability values it outputs, but even just the class labels are sufficient.", "tokens": [50364, 509, 393, 360, 2602, 307, 498, 291, 393, 483, 5567, 2105, 281, 264, 2316, 689, 291, 445, 362, 264, 3485, 281, 2845, 15743, 281, 264, 2316, 281, 11441, 1080, 23930, 11, 550, 291, 393, 2845, 729, 15743, 11, 11441, 264, 23930, 11, 293, 764, 729, 382, 428, 3097, 992, 13, 51064, 51064, 639, 486, 589, 754, 498, 264, 5598, 300, 291, 483, 490, 264, 3779, 2316, 307, 787, 264, 1508, 7645, 300, 309, 25963, 13, 51414, 51414, 316, 688, 295, 561, 1401, 341, 294, 257, 6552, 300, 291, 643, 281, 362, 2105, 281, 439, 264, 8482, 4190, 309, 23930, 11, 457, 754, 445, 264, 1508, 16949, 366, 11563, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09211268467185772, "compression_ratio": 1.9053030303030303, "no_speech_prob": 2.143249184882734e-05}, {"id": 418, "seek": 308500, "start": 3085.0, "end": 3097.0, "text": " Once you've used one of these two methods, either gather your own training set or observing the outputs of a target model, you can train your own model and then make adversarial examples for your model.", "tokens": [50364, 3443, 291, 600, 1143, 472, 295, 613, 732, 7150, 11, 2139, 5448, 428, 1065, 3097, 992, 420, 22107, 264, 23930, 295, 257, 3779, 2316, 11, 291, 393, 3847, 428, 1065, 2316, 293, 550, 652, 17641, 44745, 5110, 337, 428, 2316, 13, 50964, 50964, 3950, 17641, 44745, 5110, 366, 588, 3700, 281, 5003, 293, 3345, 264, 3779, 2316, 11, 370, 291, 393, 550, 352, 293, 2845, 729, 484, 293, 7979, 309, 11, 754, 498, 291, 994, 380, 362, 2105, 281, 309, 3838, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06243300437927246, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.265999501105398e-05}, {"id": 419, "seek": 308500, "start": 3097.0, "end": 3108.0, "text": " Those adversarial examples are very likely to transfer and affect the target model, so you can then go and send those out and fool it, even if you didn't have access to it directly.", "tokens": [50364, 3443, 291, 600, 1143, 472, 295, 613, 732, 7150, 11, 2139, 5448, 428, 1065, 3097, 992, 420, 22107, 264, 23930, 295, 257, 3779, 2316, 11, 291, 393, 3847, 428, 1065, 2316, 293, 550, 652, 17641, 44745, 5110, 337, 428, 2316, 13, 50964, 50964, 3950, 17641, 44745, 5110, 366, 588, 3700, 281, 5003, 293, 3345, 264, 3779, 2316, 11, 370, 291, 393, 550, 352, 293, 2845, 729, 484, 293, 7979, 309, 11, 754, 498, 291, 994, 380, 362, 2105, 281, 309, 3838, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06243300437927246, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.265999501105398e-05}, {"id": 420, "seek": 310800, "start": 3108.0, "end": 3122.0, "text": " We've also measured the transferability across different data sets, and for most models we find that there are kind of an intermediate zone where different data sets will result in a transfer rate of like 60 to 80%.", "tokens": [50364, 492, 600, 611, 12690, 264, 5003, 2310, 2108, 819, 1412, 6352, 11, 293, 337, 881, 5245, 321, 915, 300, 456, 366, 733, 295, 364, 19376, 6668, 689, 819, 1412, 6352, 486, 1874, 294, 257, 5003, 3314, 295, 411, 4060, 281, 4688, 6856, 51064, 51064, 821, 311, 257, 1326, 5245, 411, 31910, 26386, 300, 366, 588, 1412, 12334, 570, 31910, 26386, 300, 366, 8416, 322, 257, 588, 1359, 25993, 295, 264, 3097, 1412, 1254, 641, 2572, 3537, 12866, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12454136406503073, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.6393361875088885e-05}, {"id": 421, "seek": 310800, "start": 3122.0, "end": 3131.0, "text": " There's a few models like SVMs that are very data dependent because SVMs that are focusing on a very small subset of the training data form their final decision boundary.", "tokens": [50364, 492, 600, 611, 12690, 264, 5003, 2310, 2108, 819, 1412, 6352, 11, 293, 337, 881, 5245, 321, 915, 300, 456, 366, 733, 295, 364, 19376, 6668, 689, 819, 1412, 6352, 486, 1874, 294, 257, 5003, 3314, 295, 411, 4060, 281, 4688, 6856, 51064, 51064, 821, 311, 257, 1326, 5245, 411, 31910, 26386, 300, 366, 588, 1412, 12334, 570, 31910, 26386, 300, 366, 8416, 322, 257, 588, 1359, 25993, 295, 264, 3097, 1412, 1254, 641, 2572, 3537, 12866, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12454136406503073, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.6393361875088885e-05}, {"id": 422, "seek": 313100, "start": 3131.0, "end": 3139.0, "text": " But most models that we care about are somewhere in the intermediate zone.", "tokens": [50364, 583, 881, 5245, 300, 321, 1127, 466, 366, 4079, 294, 264, 19376, 6668, 13, 50764, 50764, 823, 300, 311, 445, 11926, 300, 291, 10687, 322, 264, 5003, 2737, 8195, 13, 50964, 50964, 509, 652, 364, 17641, 44745, 1365, 293, 291, 1454, 300, 309, 486, 5003, 281, 428, 3779, 13, 51214, 51214, 708, 498, 291, 360, 746, 281, 8630, 264, 9341, 294, 428, 2294, 293, 3470, 264, 17439, 300, 291, 603, 483, 428, 17641, 44745, 5110, 281, 5003, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08294951624986602, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.6706191420089453e-05}, {"id": 423, "seek": 313100, "start": 3139.0, "end": 3143.0, "text": " Now that's just assuming that you rely on the transfer happening naturally.", "tokens": [50364, 583, 881, 5245, 300, 321, 1127, 466, 366, 4079, 294, 264, 19376, 6668, 13, 50764, 50764, 823, 300, 311, 445, 11926, 300, 291, 10687, 322, 264, 5003, 2737, 8195, 13, 50964, 50964, 509, 652, 364, 17641, 44745, 1365, 293, 291, 1454, 300, 309, 486, 5003, 281, 428, 3779, 13, 51214, 51214, 708, 498, 291, 360, 746, 281, 8630, 264, 9341, 294, 428, 2294, 293, 3470, 264, 17439, 300, 291, 603, 483, 428, 17641, 44745, 5110, 281, 5003, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08294951624986602, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.6706191420089453e-05}, {"id": 424, "seek": 313100, "start": 3143.0, "end": 3148.0, "text": " You make an adversarial example and you hope that it will transfer to your target.", "tokens": [50364, 583, 881, 5245, 300, 321, 1127, 466, 366, 4079, 294, 264, 19376, 6668, 13, 50764, 50764, 823, 300, 311, 445, 11926, 300, 291, 10687, 322, 264, 5003, 2737, 8195, 13, 50964, 50964, 509, 652, 364, 17641, 44745, 1365, 293, 291, 1454, 300, 309, 486, 5003, 281, 428, 3779, 13, 51214, 51214, 708, 498, 291, 360, 746, 281, 8630, 264, 9341, 294, 428, 2294, 293, 3470, 264, 17439, 300, 291, 603, 483, 428, 17641, 44745, 5110, 281, 5003, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08294951624986602, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.6706191420089453e-05}, {"id": 425, "seek": 313100, "start": 3148.0, "end": 3157.0, "text": " What if you do something to stack the deck in your favor and improve the odds that you'll get your adversarial examples to transfer?", "tokens": [50364, 583, 881, 5245, 300, 321, 1127, 466, 366, 4079, 294, 264, 19376, 6668, 13, 50764, 50764, 823, 300, 311, 445, 11926, 300, 291, 10687, 322, 264, 5003, 2737, 8195, 13, 50964, 50964, 509, 652, 364, 17641, 44745, 1365, 293, 291, 1454, 300, 309, 486, 5003, 281, 428, 3779, 13, 51214, 51214, 708, 498, 291, 360, 746, 281, 8630, 264, 9341, 294, 428, 2294, 293, 3470, 264, 17439, 300, 291, 603, 483, 428, 17641, 44745, 5110, 281, 5003, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08294951624986602, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.6706191420089453e-05}, {"id": 426, "seek": 315700, "start": 3157.0, "end": 3172.0, "text": " Don songs group that you see Berkeley studied this. They found that if they take an ensemble of different models and they use gradient descent to search for an adversarial example that will fool every member of their ensemble.", "tokens": [50364, 1468, 5781, 1594, 300, 291, 536, 23684, 9454, 341, 13, 814, 1352, 300, 498, 436, 747, 364, 19492, 295, 819, 5245, 293, 436, 764, 16235, 23475, 281, 3164, 337, 364, 17641, 44745, 1365, 300, 486, 7979, 633, 4006, 295, 641, 19492, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.17128322435461957, "compression_ratio": 1.4675324675324675, "no_speech_prob": 2.238306842627935e-05}, {"id": 427, "seek": 317200, "start": 3172.0, "end": 3188.0, "text": " It's extremely likely that it will transfer and fool a new machine learning model. So if you have an ensemble of five models, you can get it to the point where there's essentially a 100% chance that you'll fool a sixth model out of the set of the models that they compared.", "tokens": [50364, 467, 311, 4664, 3700, 300, 309, 486, 5003, 293, 7979, 257, 777, 3479, 2539, 2316, 13, 407, 498, 291, 362, 364, 19492, 295, 1732, 5245, 11, 291, 393, 483, 309, 281, 264, 935, 689, 456, 311, 4476, 257, 2319, 4, 2931, 300, 291, 603, 7979, 257, 15102, 2316, 484, 295, 264, 992, 295, 264, 5245, 300, 436, 5347, 13, 51164, 51164, 814, 2956, 412, 721, 411, 30944, 295, 819, 28439, 11, 691, 38, 38, 293, 3329, 31890, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.137991858691704, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.6673758081160486e-05}, {"id": 428, "seek": 317200, "start": 3188.0, "end": 3194.0, "text": " They looked at things like resonance of different depths, VGG and GoogleNet.", "tokens": [50364, 467, 311, 4664, 3700, 300, 309, 486, 5003, 293, 7979, 257, 777, 3479, 2539, 2316, 13, 407, 498, 291, 362, 364, 19492, 295, 1732, 5245, 11, 291, 393, 483, 309, 281, 264, 935, 689, 456, 311, 4476, 257, 2319, 4, 2931, 300, 291, 603, 7979, 257, 15102, 2316, 484, 295, 264, 992, 295, 264, 5245, 300, 436, 5347, 13, 51164, 51164, 814, 2956, 412, 721, 411, 30944, 295, 819, 28439, 11, 691, 38, 38, 293, 3329, 31890, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.137991858691704, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.6673758081160486e-05}, {"id": 429, "seek": 319400, "start": 3194.0, "end": 3204.0, "text": " So in the labels for each of the different rows, you can see that they made ensembles, that lacked each of these different models and then they would test it on the different target models.", "tokens": [50364, 407, 294, 264, 16949, 337, 1184, 295, 264, 819, 13241, 11, 291, 393, 536, 300, 436, 1027, 12567, 2504, 904, 11, 300, 41481, 1184, 295, 613, 819, 5245, 293, 550, 436, 576, 1500, 309, 322, 264, 819, 3779, 5245, 13, 50864, 50864, 407, 411, 498, 291, 652, 364, 19492, 300, 3406, 1208, 3329, 31890, 11, 291, 362, 787, 466, 257, 1025, 4, 2931, 295, 3329, 31890, 8944, 1508, 5489, 264, 17641, 44745, 1365, 291, 652, 337, 300, 19492, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13494267521134343, "compression_ratio": 1.7320574162679425, "no_speech_prob": 2.947448410850484e-05}, {"id": 430, "seek": 319400, "start": 3204.0, "end": 3217.0, "text": " So like if you make an ensemble that omits GoogleNet, you have only about a 5% chance of GoogleNet correctly classifying the adversarial example you make for that ensemble.", "tokens": [50364, 407, 294, 264, 16949, 337, 1184, 295, 264, 819, 13241, 11, 291, 393, 536, 300, 436, 1027, 12567, 2504, 904, 11, 300, 41481, 1184, 295, 613, 819, 5245, 293, 550, 436, 576, 1500, 309, 322, 264, 819, 3779, 5245, 13, 50864, 50864, 407, 411, 498, 291, 652, 364, 19492, 300, 3406, 1208, 3329, 31890, 11, 291, 362, 787, 466, 257, 1025, 4, 2931, 295, 3329, 31890, 8944, 1508, 5489, 264, 17641, 44745, 1365, 291, 652, 337, 300, 19492, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13494267521134343, "compression_ratio": 1.7320574162679425, "no_speech_prob": 2.947448410850484e-05}, {"id": 431, "seek": 321700, "start": 3217.0, "end": 3229.0, "text": " If you make an ensemble that omits, resonate 152 in their experiments, they found that there is a 0% chance of resonate 152 resisting that attack.", "tokens": [50364, 759, 291, 652, 364, 19492, 300, 3406, 1208, 11, 34285, 2119, 17, 294, 641, 12050, 11, 436, 1352, 300, 456, 307, 257, 1958, 4, 2931, 295, 34285, 2119, 17, 43940, 300, 2690, 13, 50964, 50964, 663, 1391, 294, 264, 1389, 436, 820, 362, 1190, 512, 544, 17641, 44745, 5110, 1826, 436, 1352, 257, 2107, 12, 32226, 2245, 3314, 13, 51264, 51264, 583, 309, 775, 855, 300, 264, 2690, 307, 588, 4005, 293, 300, 562, 291, 352, 1237, 281, 22062, 3082, 264, 5003, 1802, 11, 291, 393, 534, 652, 309, 1596, 2068, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15563368797302246, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8025577446678653e-05}, {"id": 432, "seek": 321700, "start": 3229.0, "end": 3235.0, "text": " That probably in the case they should have run some more adversarial examples until they found a non-zero success rate.", "tokens": [50364, 759, 291, 652, 364, 19492, 300, 3406, 1208, 11, 34285, 2119, 17, 294, 641, 12050, 11, 436, 1352, 300, 456, 307, 257, 1958, 4, 2931, 295, 34285, 2119, 17, 43940, 300, 2690, 13, 50964, 50964, 663, 1391, 294, 264, 1389, 436, 820, 362, 1190, 512, 544, 17641, 44745, 5110, 1826, 436, 1352, 257, 2107, 12, 32226, 2245, 3314, 13, 51264, 51264, 583, 309, 775, 855, 300, 264, 2690, 307, 588, 4005, 293, 300, 562, 291, 352, 1237, 281, 22062, 3082, 264, 5003, 1802, 11, 291, 393, 534, 652, 309, 1596, 2068, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15563368797302246, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8025577446678653e-05}, {"id": 433, "seek": 321700, "start": 3235.0, "end": 3246.0, "text": " But it does show that the attack is very powerful and that when you go looking to intentionally cause the transfer effect, you can really make it quite strong.", "tokens": [50364, 759, 291, 652, 364, 19492, 300, 3406, 1208, 11, 34285, 2119, 17, 294, 641, 12050, 11, 436, 1352, 300, 456, 307, 257, 1958, 4, 2931, 295, 34285, 2119, 17, 43940, 300, 2690, 13, 50964, 50964, 663, 1391, 294, 264, 1389, 436, 820, 362, 1190, 512, 544, 17641, 44745, 5110, 1826, 436, 1352, 257, 2107, 12, 32226, 2245, 3314, 13, 51264, 51264, 583, 309, 775, 855, 300, 264, 2690, 307, 588, 4005, 293, 300, 562, 291, 352, 1237, 281, 22062, 3082, 264, 5003, 1802, 11, 291, 393, 534, 652, 309, 1596, 2068, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15563368797302246, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8025577446678653e-05}, {"id": 434, "seek": 324600, "start": 3246.0, "end": 3252.0, "text": " A lot of people often ask me if the human brain is vulnerable to adversarial examples.", "tokens": [50364, 316, 688, 295, 561, 2049, 1029, 385, 498, 264, 1952, 3567, 307, 10955, 281, 17641, 44745, 5110, 13, 50664, 50664, 400, 337, 341, 7991, 11, 286, 393, 380, 764, 17996, 292, 2527, 11, 457, 456, 311, 512, 534, 19796, 721, 322, 264, 4705, 13, 50964, 50964, 759, 291, 352, 1237, 337, 411, 264, 7592, 7983, 365, 5267, 295, 3934, 8234, 373, 11, 291, 603, 915, 746, 300, 11, 291, 458, 11, 452, 12860, 1185, 2138, 393, 380, 4813, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11615208545363093, "compression_ratio": 1.5148936170212766, "no_speech_prob": 2.1748895960627124e-05}, {"id": 435, "seek": 324600, "start": 3252.0, "end": 3258.0, "text": " And for this lecture, I can't use copyrighted material, but there's some really hilarious things on the internet.", "tokens": [50364, 316, 688, 295, 561, 2049, 1029, 385, 498, 264, 1952, 3567, 307, 10955, 281, 17641, 44745, 5110, 13, 50664, 50664, 400, 337, 341, 7991, 11, 286, 393, 380, 764, 17996, 292, 2527, 11, 457, 456, 311, 512, 534, 19796, 721, 322, 264, 4705, 13, 50964, 50964, 759, 291, 352, 1237, 337, 411, 264, 7592, 7983, 365, 5267, 295, 3934, 8234, 373, 11, 291, 603, 915, 746, 300, 11, 291, 458, 11, 452, 12860, 1185, 2138, 393, 380, 4813, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11615208545363093, "compression_ratio": 1.5148936170212766, "no_speech_prob": 2.1748895960627124e-05}, {"id": 436, "seek": 324600, "start": 3258.0, "end": 3269.0, "text": " If you go looking for like the fake capture with images of Mark Hamill, you'll find something that, you know, my perception system definitely can't handle.", "tokens": [50364, 316, 688, 295, 561, 2049, 1029, 385, 498, 264, 1952, 3567, 307, 10955, 281, 17641, 44745, 5110, 13, 50664, 50664, 400, 337, 341, 7991, 11, 286, 393, 380, 764, 17996, 292, 2527, 11, 457, 456, 311, 512, 534, 19796, 721, 322, 264, 4705, 13, 50964, 50964, 759, 291, 352, 1237, 337, 411, 264, 7592, 7983, 365, 5267, 295, 3934, 8234, 373, 11, 291, 603, 915, 746, 300, 11, 291, 458, 11, 452, 12860, 1185, 2138, 393, 380, 4813, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11615208545363093, "compression_ratio": 1.5148936170212766, "no_speech_prob": 2.1748895960627124e-05}, {"id": 437, "seek": 326900, "start": 3269.0, "end": 3276.0, "text": " So here's another one that's actually published with a license where I was confident I'm allowed to use it.", "tokens": [50364, 407, 510, 311, 1071, 472, 300, 311, 767, 6572, 365, 257, 10476, 689, 286, 390, 6679, 286, 478, 4350, 281, 764, 309, 13, 50714, 50714, 509, 393, 574, 412, 341, 3256, 295, 819, 13040, 510, 293, 264, 4204, 281, 312, 44400, 2001, 10733, 1124, 11, 457, 294, 1186, 436, 366, 5512, 1341, 13040, 13, 51214, 51214, 440, 14764, 295, 264, 8819, 295, 264, 19368, 307, 48721, 365, 264, 4691, 46866, 294, 428, 3567, 293, 1455, 309, 574, 411, 264, 13040, 366, 10733, 4270, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09271284666928378, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.0746137528913096e-05}, {"id": 438, "seek": 326900, "start": 3276.0, "end": 3286.0, "text": " You can look at this image of different circles here and the appear to be intertwined spirals, but in fact they are concentric circles.", "tokens": [50364, 407, 510, 311, 1071, 472, 300, 311, 767, 6572, 365, 257, 10476, 689, 286, 390, 6679, 286, 478, 4350, 281, 764, 309, 13, 50714, 50714, 509, 393, 574, 412, 341, 3256, 295, 819, 13040, 510, 293, 264, 4204, 281, 312, 44400, 2001, 10733, 1124, 11, 457, 294, 1186, 436, 366, 5512, 1341, 13040, 13, 51214, 51214, 440, 14764, 295, 264, 8819, 295, 264, 19368, 307, 48721, 365, 264, 4691, 46866, 294, 428, 3567, 293, 1455, 309, 574, 411, 264, 13040, 366, 10733, 4270, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09271284666928378, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.0746137528913096e-05}, {"id": 439, "seek": 326900, "start": 3286.0, "end": 3296.0, "text": " The orientation of the edges of the squares is interfering with the edge detectors in your brain and making it look like the circles are spiraling.", "tokens": [50364, 407, 510, 311, 1071, 472, 300, 311, 767, 6572, 365, 257, 10476, 689, 286, 390, 6679, 286, 478, 4350, 281, 764, 309, 13, 50714, 50714, 509, 393, 574, 412, 341, 3256, 295, 819, 13040, 510, 293, 264, 4204, 281, 312, 44400, 2001, 10733, 1124, 11, 457, 294, 1186, 436, 366, 5512, 1341, 13040, 13, 51214, 51214, 440, 14764, 295, 264, 8819, 295, 264, 19368, 307, 48721, 365, 264, 4691, 46866, 294, 428, 3567, 293, 1455, 309, 574, 411, 264, 13040, 366, 10733, 4270, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09271284666928378, "compression_ratio": 1.649789029535865, "no_speech_prob": 4.0746137528913096e-05}, {"id": 440, "seek": 329600, "start": 3296.0, "end": 3300.0, "text": " So you can think of these optical illusions as being adversarial examples for the human brain.", "tokens": [50364, 407, 291, 393, 519, 295, 613, 20674, 49836, 382, 885, 17641, 44745, 5110, 337, 264, 1952, 3567, 13, 50564, 50564, 708, 311, 1880, 307, 300, 321, 500, 380, 1643, 281, 2073, 867, 17641, 44745, 5110, 294, 2689, 365, 3479, 2539, 5245, 13, 50864, 50864, 1999, 840, 44745, 5110, 5003, 4664, 49927, 1296, 819, 3479, 2539, 5245, 11, 2318, 498, 291, 764, 300, 19492, 4282, 300, 390, 4743, 294, 257, 14079, 23684, 13, 51364, 51364, 583, 729, 17641, 44745, 5110, 500, 380, 7979, 505, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05959156968376853, "compression_ratio": 1.786610878661088, "no_speech_prob": 1.3646097613673192e-05}, {"id": 441, "seek": 329600, "start": 3300.0, "end": 3306.0, "text": " What's interesting is that we don't seem to share many adversarial examples in common with machine learning models.", "tokens": [50364, 407, 291, 393, 519, 295, 613, 20674, 49836, 382, 885, 17641, 44745, 5110, 337, 264, 1952, 3567, 13, 50564, 50564, 708, 311, 1880, 307, 300, 321, 500, 380, 1643, 281, 2073, 867, 17641, 44745, 5110, 294, 2689, 365, 3479, 2539, 5245, 13, 50864, 50864, 1999, 840, 44745, 5110, 5003, 4664, 49927, 1296, 819, 3479, 2539, 5245, 11, 2318, 498, 291, 764, 300, 19492, 4282, 300, 390, 4743, 294, 257, 14079, 23684, 13, 51364, 51364, 583, 729, 17641, 44745, 5110, 500, 380, 7979, 505, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05959156968376853, "compression_ratio": 1.786610878661088, "no_speech_prob": 1.3646097613673192e-05}, {"id": 442, "seek": 329600, "start": 3306.0, "end": 3316.0, "text": " Adversarial examples transfer extremely reliably between different machine learning models, especially if you use that ensemble trick that was developed in a UC Berkeley.", "tokens": [50364, 407, 291, 393, 519, 295, 613, 20674, 49836, 382, 885, 17641, 44745, 5110, 337, 264, 1952, 3567, 13, 50564, 50564, 708, 311, 1880, 307, 300, 321, 500, 380, 1643, 281, 2073, 867, 17641, 44745, 5110, 294, 2689, 365, 3479, 2539, 5245, 13, 50864, 50864, 1999, 840, 44745, 5110, 5003, 4664, 49927, 1296, 819, 3479, 2539, 5245, 11, 2318, 498, 291, 764, 300, 19492, 4282, 300, 390, 4743, 294, 257, 14079, 23684, 13, 51364, 51364, 583, 729, 17641, 44745, 5110, 500, 380, 7979, 505, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05959156968376853, "compression_ratio": 1.786610878661088, "no_speech_prob": 1.3646097613673192e-05}, {"id": 443, "seek": 329600, "start": 3316.0, "end": 3319.0, "text": " But those adversarial examples don't fool us.", "tokens": [50364, 407, 291, 393, 519, 295, 613, 20674, 49836, 382, 885, 17641, 44745, 5110, 337, 264, 1952, 3567, 13, 50564, 50564, 708, 311, 1880, 307, 300, 321, 500, 380, 1643, 281, 2073, 867, 17641, 44745, 5110, 294, 2689, 365, 3479, 2539, 5245, 13, 50864, 50864, 1999, 840, 44745, 5110, 5003, 4664, 49927, 1296, 819, 3479, 2539, 5245, 11, 2318, 498, 291, 764, 300, 19492, 4282, 300, 390, 4743, 294, 257, 14079, 23684, 13, 51364, 51364, 583, 729, 17641, 44745, 5110, 500, 380, 7979, 505, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05959156968376853, "compression_ratio": 1.786610878661088, "no_speech_prob": 1.3646097613673192e-05}, {"id": 444, "seek": 331900, "start": 3319.0, "end": 3326.0, "text": " It tells us that we must be using a very different algorithm or model family than current convolutional networks.", "tokens": [50364, 467, 5112, 505, 300, 321, 1633, 312, 1228, 257, 588, 819, 9284, 420, 2316, 1605, 813, 2190, 45216, 304, 9590, 13, 50714, 50714, 400, 321, 500, 380, 534, 458, 437, 264, 2649, 307, 1939, 11, 457, 309, 576, 312, 588, 1880, 281, 2573, 300, 484, 13, 50964, 50964, 467, 2544, 281, 3402, 300, 7601, 17641, 44745, 5110, 727, 980, 505, 577, 281, 10591, 3470, 527, 6741, 3479, 2539, 5245, 13, 51314, 51314, 2754, 498, 291, 500, 380, 1127, 466, 1419, 364, 48222, 11, 321, 1062, 2573, 484, 746, 420, 661, 466, 577, 281, 652, 3479, 2539, 14642, 2028, 365, 46519, 293, 13106, 15743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05013132095336914, "compression_ratio": 1.7305194805194806, "no_speech_prob": 2.8294974981690757e-05}, {"id": 445, "seek": 331900, "start": 3326.0, "end": 3331.0, "text": " And we don't really know what the difference is yet, but it would be very interesting to figure that out.", "tokens": [50364, 467, 5112, 505, 300, 321, 1633, 312, 1228, 257, 588, 819, 9284, 420, 2316, 1605, 813, 2190, 45216, 304, 9590, 13, 50714, 50714, 400, 321, 500, 380, 534, 458, 437, 264, 2649, 307, 1939, 11, 457, 309, 576, 312, 588, 1880, 281, 2573, 300, 484, 13, 50964, 50964, 467, 2544, 281, 3402, 300, 7601, 17641, 44745, 5110, 727, 980, 505, 577, 281, 10591, 3470, 527, 6741, 3479, 2539, 5245, 13, 51314, 51314, 2754, 498, 291, 500, 380, 1127, 466, 1419, 364, 48222, 11, 321, 1062, 2573, 484, 746, 420, 661, 466, 577, 281, 652, 3479, 2539, 14642, 2028, 365, 46519, 293, 13106, 15743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05013132095336914, "compression_ratio": 1.7305194805194806, "no_speech_prob": 2.8294974981690757e-05}, {"id": 446, "seek": 331900, "start": 3331.0, "end": 3338.0, "text": " It seems to suggest that studying adversarial examples could tell us how to significantly improve our existing machine learning models.", "tokens": [50364, 467, 5112, 505, 300, 321, 1633, 312, 1228, 257, 588, 819, 9284, 420, 2316, 1605, 813, 2190, 45216, 304, 9590, 13, 50714, 50714, 400, 321, 500, 380, 534, 458, 437, 264, 2649, 307, 1939, 11, 457, 309, 576, 312, 588, 1880, 281, 2573, 300, 484, 13, 50964, 50964, 467, 2544, 281, 3402, 300, 7601, 17641, 44745, 5110, 727, 980, 505, 577, 281, 10591, 3470, 527, 6741, 3479, 2539, 5245, 13, 51314, 51314, 2754, 498, 291, 500, 380, 1127, 466, 1419, 364, 48222, 11, 321, 1062, 2573, 484, 746, 420, 661, 466, 577, 281, 652, 3479, 2539, 14642, 2028, 365, 46519, 293, 13106, 15743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05013132095336914, "compression_ratio": 1.7305194805194806, "no_speech_prob": 2.8294974981690757e-05}, {"id": 447, "seek": 331900, "start": 3338.0, "end": 3348.0, "text": " Even if you don't care about having an adversary, we might figure out something or other about how to make machine learning algorithms deal with ambiguity and unexpected inputs.", "tokens": [50364, 467, 5112, 505, 300, 321, 1633, 312, 1228, 257, 588, 819, 9284, 420, 2316, 1605, 813, 2190, 45216, 304, 9590, 13, 50714, 50714, 400, 321, 500, 380, 534, 458, 437, 264, 2649, 307, 1939, 11, 457, 309, 576, 312, 588, 1880, 281, 2573, 300, 484, 13, 50964, 50964, 467, 2544, 281, 3402, 300, 7601, 17641, 44745, 5110, 727, 980, 505, 577, 281, 10591, 3470, 527, 6741, 3479, 2539, 5245, 13, 51314, 51314, 2754, 498, 291, 500, 380, 1127, 466, 1419, 364, 48222, 11, 321, 1062, 2573, 484, 746, 420, 661, 466, 577, 281, 652, 3479, 2539, 14642, 2028, 365, 46519, 293, 13106, 15743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05013132095336914, "compression_ratio": 1.7305194805194806, "no_speech_prob": 2.8294974981690757e-05}, {"id": 448, "seek": 334800, "start": 3348.0, "end": 3352.0, "text": " More like a human does.", "tokens": [50364, 5048, 411, 257, 1952, 775, 13, 50564, 50564, 759, 321, 767, 528, 281, 352, 484, 293, 360, 8122, 294, 3124, 11, 456, 311, 1409, 281, 312, 257, 1772, 295, 2132, 322, 341, 3983, 13, 50964, 50964, 13969, 4711, 15919, 1248, 329, 4712, 300, 415, 727, 764, 264, 5003, 1802, 281, 7979, 1508, 23463, 19204, 538, 1131, 335, 533, 11, 6795, 293, 3329, 13, 51414, 51414, 407, 613, 366, 439, 445, 819, 3479, 2539, 21445, 689, 291, 393, 6580, 257, 28872, 293, 264, 9362, 486, 3847, 257, 2316, 337, 291, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15367369956158577, "compression_ratio": 1.5, "no_speech_prob": 7.24752462701872e-05}, {"id": 449, "seek": 334800, "start": 3352.0, "end": 3360.0, "text": " If we actually want to go out and do attacks in practice, there's started to be a body of research on this subject.", "tokens": [50364, 5048, 411, 257, 1952, 775, 13, 50564, 50564, 759, 321, 767, 528, 281, 352, 484, 293, 360, 8122, 294, 3124, 11, 456, 311, 1409, 281, 312, 257, 1772, 295, 2132, 322, 341, 3983, 13, 50964, 50964, 13969, 4711, 15919, 1248, 329, 4712, 300, 415, 727, 764, 264, 5003, 1802, 281, 7979, 1508, 23463, 19204, 538, 1131, 335, 533, 11, 6795, 293, 3329, 13, 51414, 51414, 407, 613, 366, 439, 445, 819, 3479, 2539, 21445, 689, 291, 393, 6580, 257, 28872, 293, 264, 9362, 486, 3847, 257, 2316, 337, 291, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15367369956158577, "compression_ratio": 1.5, "no_speech_prob": 7.24752462701872e-05}, {"id": 450, "seek": 334800, "start": 3360.0, "end": 3369.0, "text": " Nikola Papernos showed that he could use the transfer effect to fool classifiers hosted by metamine, Amazon and Google.", "tokens": [50364, 5048, 411, 257, 1952, 775, 13, 50564, 50564, 759, 321, 767, 528, 281, 352, 484, 293, 360, 8122, 294, 3124, 11, 456, 311, 1409, 281, 312, 257, 1772, 295, 2132, 322, 341, 3983, 13, 50964, 50964, 13969, 4711, 15919, 1248, 329, 4712, 300, 415, 727, 764, 264, 5003, 1802, 281, 7979, 1508, 23463, 19204, 538, 1131, 335, 533, 11, 6795, 293, 3329, 13, 51414, 51414, 407, 613, 366, 439, 445, 819, 3479, 2539, 21445, 689, 291, 393, 6580, 257, 28872, 293, 264, 9362, 486, 3847, 257, 2316, 337, 291, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15367369956158577, "compression_ratio": 1.5, "no_speech_prob": 7.24752462701872e-05}, {"id": 451, "seek": 334800, "start": 3369.0, "end": 3376.0, "text": " So these are all just different machine learning APIs where you can upload a dataset and the API will train a model for you.", "tokens": [50364, 5048, 411, 257, 1952, 775, 13, 50564, 50564, 759, 321, 767, 528, 281, 352, 484, 293, 360, 8122, 294, 3124, 11, 456, 311, 1409, 281, 312, 257, 1772, 295, 2132, 322, 341, 3983, 13, 50964, 50964, 13969, 4711, 15919, 1248, 329, 4712, 300, 415, 727, 764, 264, 5003, 1802, 281, 7979, 1508, 23463, 19204, 538, 1131, 335, 533, 11, 6795, 293, 3329, 13, 51414, 51414, 407, 613, 366, 439, 445, 819, 3479, 2539, 21445, 689, 291, 393, 6580, 257, 28872, 293, 264, 9362, 486, 3847, 257, 2316, 337, 291, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15367369956158577, "compression_ratio": 1.5, "no_speech_prob": 7.24752462701872e-05}, {"id": 452, "seek": 337600, "start": 3376.0, "end": 3381.0, "text": " And then you don't actually know in most cases which model has trained for you.", "tokens": [50364, 400, 550, 291, 500, 380, 767, 458, 294, 881, 3331, 597, 2316, 575, 8895, 337, 291, 13, 50614, 50614, 509, 500, 380, 362, 2105, 281, 1080, 17443, 420, 1340, 411, 300, 13, 50764, 50764, 407, 13969, 4711, 576, 3847, 702, 1065, 5055, 295, 257, 2316, 1228, 264, 9362, 293, 550, 1322, 257, 2316, 322, 702, 1065, 2973, 14502, 689, 415, 727, 7979, 264, 9362, 19204, 2316, 13, 51264, 51264, 11965, 23684, 4712, 291, 727, 7979, 6093, 2505, 294, 341, 636, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10671043395996094, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.3687512364413124e-05}, {"id": 453, "seek": 337600, "start": 3381.0, "end": 3384.0, "text": " You don't have access to its weights or anything like that.", "tokens": [50364, 400, 550, 291, 500, 380, 767, 458, 294, 881, 3331, 597, 2316, 575, 8895, 337, 291, 13, 50614, 50614, 509, 500, 380, 362, 2105, 281, 1080, 17443, 420, 1340, 411, 300, 13, 50764, 50764, 407, 13969, 4711, 576, 3847, 702, 1065, 5055, 295, 257, 2316, 1228, 264, 9362, 293, 550, 1322, 257, 2316, 322, 702, 1065, 2973, 14502, 689, 415, 727, 7979, 264, 9362, 19204, 2316, 13, 51264, 51264, 11965, 23684, 4712, 291, 727, 7979, 6093, 2505, 294, 341, 636, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10671043395996094, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.3687512364413124e-05}, {"id": 454, "seek": 337600, "start": 3384.0, "end": 3394.0, "text": " So Nikola would train his own copy of a model using the API and then build a model on his own personal desktop where he could fool the API hosted model.", "tokens": [50364, 400, 550, 291, 500, 380, 767, 458, 294, 881, 3331, 597, 2316, 575, 8895, 337, 291, 13, 50614, 50614, 509, 500, 380, 362, 2105, 281, 1080, 17443, 420, 1340, 411, 300, 13, 50764, 50764, 407, 13969, 4711, 576, 3847, 702, 1065, 5055, 295, 257, 2316, 1228, 264, 9362, 293, 550, 1322, 257, 2316, 322, 702, 1065, 2973, 14502, 689, 415, 727, 7979, 264, 9362, 19204, 2316, 13, 51264, 51264, 11965, 23684, 4712, 291, 727, 7979, 6093, 2505, 294, 341, 636, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10671043395996094, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.3687512364413124e-05}, {"id": 455, "seek": 339400, "start": 3394.0, "end": 3409.0, "text": " Later, Berkeley showed you could fool clarify in this way.", "tokens": [50364, 11965, 11, 23684, 4712, 291, 727, 7979, 6093, 2505, 294, 341, 636, 13, 51114, 51114, 876, 11, 437, 311, 309, 411, 498, 321, 574, 412, 337, 1365, 11, 411, 341, 3036, 295, 264, 46685, 281, 505, 309, 1542, 411, 257, 46685, 281, 881, 3479, 2539, 5245, 309, 1542, 411, 257, 2212, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.24396736281258719, "compression_ratio": 1.4931506849315068, "no_speech_prob": 7.874579023336992e-05}, {"id": 456, "seek": 339400, "start": 3409.0, "end": 3420.0, "text": " Oh, what's it like if we look at for example, like this picture of the panda to us it looks like a panda to most machine learning models it looks like a given.", "tokens": [50364, 11965, 11, 23684, 4712, 291, 727, 7979, 6093, 2505, 294, 341, 636, 13, 51114, 51114, 876, 11, 437, 311, 309, 411, 498, 321, 574, 412, 337, 1365, 11, 411, 341, 3036, 295, 264, 46685, 281, 505, 309, 1542, 411, 257, 46685, 281, 881, 3479, 2539, 5245, 309, 1542, 411, 257, 2212, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.24396736281258719, "compression_ratio": 1.4931506849315068, "no_speech_prob": 7.874579023336992e-05}, {"id": 457, "seek": 342000, "start": 3420.0, "end": 3427.0, "text": " And so this this changes in interfering with our brains, but it fools reliably with lots of different machine learning models.", "tokens": [50364, 400, 370, 341, 341, 2962, 294, 48721, 365, 527, 15442, 11, 457, 309, 38625, 49927, 365, 3195, 295, 819, 3479, 2539, 5245, 13, 50714, 50714, 865, 13, 50814, 50814, 286, 1866, 2618, 767, 1890, 341, 3256, 295, 264, 40468, 399, 484, 295, 527, 3035, 293, 436, 1791, 292, 309, 322, 641, 4384, 7964, 3036, 281, 536, 498, 309, 727, 23946, 365, 4384, 18538, 552, 293, 436, 848, 300, 309, 630, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16145987192789713, "compression_ratio": 1.5936073059360731, "no_speech_prob": 5.505580338649452e-05}, {"id": 458, "seek": 342000, "start": 3427.0, "end": 3429.0, "text": " Yeah.", "tokens": [50364, 400, 370, 341, 341, 2962, 294, 48721, 365, 527, 15442, 11, 457, 309, 38625, 49927, 365, 3195, 295, 819, 3479, 2539, 5245, 13, 50714, 50714, 865, 13, 50814, 50814, 286, 1866, 2618, 767, 1890, 341, 3256, 295, 264, 40468, 399, 484, 295, 527, 3035, 293, 436, 1791, 292, 309, 322, 641, 4384, 7964, 3036, 281, 536, 498, 309, 727, 23946, 365, 4384, 18538, 552, 293, 436, 848, 300, 309, 630, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16145987192789713, "compression_ratio": 1.5936073059360731, "no_speech_prob": 5.505580338649452e-05}, {"id": 459, "seek": 342000, "start": 3429.0, "end": 3443.0, "text": " I saw somebody actually took this image of the perturbation out of our paper and they pasted it on their Facebook profile picture to see if it could interfere with Facebook recognizing them and they said that it did.", "tokens": [50364, 400, 370, 341, 341, 2962, 294, 48721, 365, 527, 15442, 11, 457, 309, 38625, 49927, 365, 3195, 295, 819, 3479, 2539, 5245, 13, 50714, 50714, 865, 13, 50814, 50814, 286, 1866, 2618, 767, 1890, 341, 3256, 295, 264, 40468, 399, 484, 295, 527, 3035, 293, 436, 1791, 292, 309, 322, 641, 4384, 7964, 3036, 281, 536, 498, 309, 727, 23946, 365, 4384, 18538, 552, 293, 436, 848, 300, 309, 630, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16145987192789713, "compression_ratio": 1.5936073059360731, "no_speech_prob": 5.505580338649452e-05}, {"id": 460, "seek": 344300, "start": 3443.0, "end": 3455.0, "text": " I don't I don't think that Facebook has a given tag though so we don't know if they managed to make it think that they were a given.", "tokens": [50364, 286, 500, 380, 286, 500, 380, 519, 300, 4384, 575, 257, 2212, 6162, 1673, 370, 321, 500, 380, 458, 498, 436, 6453, 281, 652, 309, 519, 300, 436, 645, 257, 2212, 13, 50964, 50964, 400, 472, 295, 264, 661, 721, 300, 291, 393, 360, 300, 311, 295, 6457, 1090, 8496, 17687, 307, 291, 393, 767, 7979, 40747, 46866, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13250659760974703, "compression_ratio": 1.606060606060606, "no_speech_prob": 5.199863517191261e-05}, {"id": 461, "seek": 344300, "start": 3455.0, "end": 3463.0, "text": " And one of the other things that you can do that's of fairly high practical significance is you can actually fool malware detectors.", "tokens": [50364, 286, 500, 380, 286, 500, 380, 519, 300, 4384, 575, 257, 2212, 6162, 1673, 370, 321, 500, 380, 458, 498, 436, 6453, 281, 652, 309, 519, 300, 436, 645, 257, 2212, 13, 50964, 50964, 400, 472, 295, 264, 661, 721, 300, 291, 393, 360, 300, 311, 295, 6457, 1090, 8496, 17687, 307, 291, 393, 767, 7979, 40747, 46866, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13250659760974703, "compression_ratio": 1.606060606060606, "no_speech_prob": 5.199863517191261e-05}, {"id": 462, "seek": 346300, "start": 3463.0, "end": 3476.0, "text": " And so, you can actually see that the pattern grows at the University of Starland to wrote a paper about this and there's starting to be a few others there's a model called malgan that actually uses again to generate adversarial examples for malware detectors.", "tokens": [50364, 400, 370, 11, 291, 393, 767, 536, 300, 264, 5102, 13156, 412, 264, 3535, 295, 5705, 1661, 281, 4114, 257, 3035, 466, 341, 293, 456, 311, 2891, 281, 312, 257, 1326, 2357, 456, 311, 257, 2316, 1219, 2806, 1275, 300, 767, 4960, 797, 281, 8460, 17641, 44745, 5110, 337, 40747, 46866, 13, 51014, 51014, 3996, 551, 300, 7001, 257, 688, 498, 291, 366, 3102, 294, 1228, 613, 8122, 294, 264, 957, 1002, 293, 21377, 1970, 552, 294, 264, 957, 1002, 13, 51364, 51364], "temperature": 0.2, "avg_logprob": -0.37869020349839155, "compression_ratio": 1.7179487179487178, "no_speech_prob": 8.284106297651306e-05}, {"id": 463, "seek": 346300, "start": 3476.0, "end": 3483.0, "text": " Another thing that matters a lot if you are interested in using these attacks in the real world and defending against them in the real world.", "tokens": [50364, 400, 370, 11, 291, 393, 767, 536, 300, 264, 5102, 13156, 412, 264, 3535, 295, 5705, 1661, 281, 4114, 257, 3035, 466, 341, 293, 456, 311, 2891, 281, 312, 257, 1326, 2357, 456, 311, 257, 2316, 1219, 2806, 1275, 300, 767, 4960, 797, 281, 8460, 17641, 44745, 5110, 337, 40747, 46866, 13, 51014, 51014, 3996, 551, 300, 7001, 257, 688, 498, 291, 366, 3102, 294, 1228, 613, 8122, 294, 264, 957, 1002, 293, 21377, 1970, 552, 294, 264, 957, 1002, 13, 51364, 51364], "temperature": 0.2, "avg_logprob": -0.37869020349839155, "compression_ratio": 1.7179487179487178, "no_speech_prob": 8.284106297651306e-05}, {"id": 464, "seek": 348300, "start": 3483.0, "end": 3498.0, "text": " And so, at the time you don't actually have access to the digital input to a model if you're interested in the perception system for a self-driving car or a robot you probably don't get to actually write to the buffer on the robot itself.", "tokens": [50364, 400, 370, 11, 412, 264, 565, 291, 500, 380, 767, 362, 2105, 281, 264, 4562, 4846, 281, 257, 2316, 498, 291, 434, 3102, 294, 264, 12860, 1185, 337, 257, 2698, 12, 47094, 1032, 420, 257, 7881, 291, 1391, 500, 380, 483, 281, 767, 2464, 281, 264, 21762, 322, 264, 7881, 2564, 13, 51114, 51114, 509, 445, 483, 281, 855, 264, 7881, 6565, 300, 309, 393, 536, 807, 2799, 6765, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.20031691886283257, "compression_ratio": 1.6102564102564103, "no_speech_prob": 6.00811472395435e-05}, {"id": 465, "seek": 348300, "start": 3498.0, "end": 3503.0, "text": " You just get to show the robot objects that it can see through camera lens.", "tokens": [50364, 400, 370, 11, 412, 264, 565, 291, 500, 380, 767, 362, 2105, 281, 264, 4562, 4846, 281, 257, 2316, 498, 291, 434, 3102, 294, 264, 12860, 1185, 337, 257, 2698, 12, 47094, 1032, 420, 257, 7881, 291, 1391, 500, 380, 483, 281, 767, 2464, 281, 264, 21762, 322, 264, 7881, 2564, 13, 51114, 51114, 509, 445, 483, 281, 855, 264, 7881, 6565, 300, 309, 393, 536, 807, 2799, 6765, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.20031691886283257, "compression_ratio": 1.6102564102564103, "no_speech_prob": 6.00811472395435e-05}, {"id": 466, "seek": 350300, "start": 3503.0, "end": 3514.0, "text": " And I'm talking in a semi-benjiu and I wrote a paper where we studied if we can actually fool an object recognition system running on a phone where it perceives the world through a camera.", "tokens": [50364, 400, 286, 478, 1417, 294, 257, 12909, 12, 1799, 4013, 84, 293, 286, 4114, 257, 3035, 689, 321, 9454, 498, 321, 393, 767, 7979, 364, 2657, 11150, 1185, 2614, 322, 257, 2593, 689, 309, 9016, 1539, 264, 1002, 807, 257, 2799, 13, 50914, 50914, 2621, 24850, 390, 534, 15325, 13, 492, 445, 13567, 484, 2940, 5242, 295, 17641, 44745, 5110, 293, 321, 1352, 300, 264, 2657, 11150, 1185, 2614, 322, 264, 2799, 390, 33372, 538, 552, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2119081285264757, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00010798598668770865}, {"id": 467, "seek": 350300, "start": 3514.0, "end": 3526.0, "text": " Our methodology was really straightforward. We just printed out several pictures of adversarial examples and we found that the object recognition system running on the camera was fooled by them.", "tokens": [50364, 400, 286, 478, 1417, 294, 257, 12909, 12, 1799, 4013, 84, 293, 286, 4114, 257, 3035, 689, 321, 9454, 498, 321, 393, 767, 7979, 364, 2657, 11150, 1185, 2614, 322, 257, 2593, 689, 309, 9016, 1539, 264, 1002, 807, 257, 2799, 13, 50914, 50914, 2621, 24850, 390, 534, 15325, 13, 492, 445, 13567, 484, 2940, 5242, 295, 17641, 44745, 5110, 293, 321, 1352, 300, 264, 2657, 11150, 1185, 2614, 322, 264, 2799, 390, 33372, 538, 552, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2119081285264757, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00010798598668770865}, {"id": 468, "seek": 352600, "start": 3526.0, "end": 3536.0, "text": " The camera is actually different from the model that we used to generate the adversarial examples. So we're showing not just transfer across the changes that happen when you use the camera.", "tokens": [50364, 440, 2799, 307, 767, 819, 490, 264, 2316, 300, 321, 1143, 281, 8460, 264, 17641, 44745, 5110, 13, 407, 321, 434, 4099, 406, 445, 5003, 2108, 264, 2962, 300, 1051, 562, 291, 764, 264, 2799, 13, 50864, 50864, 492, 434, 611, 4099, 300, 456, 311, 5003, 2108, 264, 2316, 300, 291, 764, 13, 51064, 51064, 407, 264, 35871, 727, 10413, 592, 1188, 7979, 257, 1185, 300, 311, 17826, 294, 257, 4001, 9461, 11, 754, 498, 436, 500, 380, 362, 2105, 281, 264, 2316, 322, 300, 9461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07874657557560848, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.8069962607114576e-05}, {"id": 469, "seek": 352600, "start": 3536.0, "end": 3540.0, "text": " We're also showing that there's transfer across the model that you use.", "tokens": [50364, 440, 2799, 307, 767, 819, 490, 264, 2316, 300, 321, 1143, 281, 8460, 264, 17641, 44745, 5110, 13, 407, 321, 434, 4099, 406, 445, 5003, 2108, 264, 2962, 300, 1051, 562, 291, 764, 264, 2799, 13, 50864, 50864, 492, 434, 611, 4099, 300, 456, 311, 5003, 2108, 264, 2316, 300, 291, 764, 13, 51064, 51064, 407, 264, 35871, 727, 10413, 592, 1188, 7979, 257, 1185, 300, 311, 17826, 294, 257, 4001, 9461, 11, 754, 498, 436, 500, 380, 362, 2105, 281, 264, 2316, 322, 300, 9461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07874657557560848, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.8069962607114576e-05}, {"id": 470, "seek": 352600, "start": 3540.0, "end": 3549.0, "text": " So the attacker could conceivably fool a system that's deployed in a physical agent, even if they don't have access to the model on that agent.", "tokens": [50364, 440, 2799, 307, 767, 819, 490, 264, 2316, 300, 321, 1143, 281, 8460, 264, 17641, 44745, 5110, 13, 407, 321, 434, 4099, 406, 445, 5003, 2108, 264, 2962, 300, 1051, 562, 291, 764, 264, 2799, 13, 50864, 50864, 492, 434, 611, 4099, 300, 456, 311, 5003, 2108, 264, 2316, 300, 291, 764, 13, 51064, 51064, 407, 264, 35871, 727, 10413, 592, 1188, 7979, 257, 1185, 300, 311, 17826, 294, 257, 4001, 9461, 11, 754, 498, 436, 500, 380, 362, 2105, 281, 264, 2316, 322, 300, 9461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07874657557560848, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.8069962607114576e-05}, {"id": 471, "seek": 354900, "start": 3549.0, "end": 3559.0, "text": " And even if they can't interface directly with the agent, but just modify subtly, model, objects that it can see in its environment.", "tokens": [50364, 400, 754, 498, 436, 393, 380, 9226, 3838, 365, 264, 9461, 11, 457, 445, 16927, 7257, 356, 11, 2316, 11, 6565, 300, 309, 393, 536, 294, 1080, 2823, 13, 50864, 51364], "temperature": 0.0, "avg_logprob": -0.20463332262906161, "compression_ratio": 1.2452830188679245, "no_speech_prob": 1.652020546316635e-05}, {"id": 472, "seek": 355900, "start": 3559.0, "end": 3584.0, "text": " So I think a lot of that comes back to the maps that I showed earlier that if you cross over the boundary into the realm of adversarial examples, they occupy a pretty wide space and they're very densely packed in there.", "tokens": [50364, 407, 286, 519, 257, 688, 295, 300, 1487, 646, 281, 264, 11317, 300, 286, 4712, 3071, 300, 498, 291, 3278, 670, 264, 12866, 666, 264, 15355, 295, 17641, 44745, 5110, 11, 436, 30645, 257, 1238, 4874, 1901, 293, 436, 434, 588, 24505, 736, 13265, 294, 456, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09195697073842965, "compression_ratio": 1.4503311258278146, "no_speech_prob": 5.172477540327236e-05}, {"id": 473, "seek": 358400, "start": 3584.0, "end": 3602.0, "text": " So if you jostle around a little bit, you're not going to recover from the adversarial attack. If the camera noise somehow or other was aligned with the negative gradient of the cost, then the camera could like take a gradient descent step downhill and rescue you from up hill step that the adversary took.", "tokens": [50364, 407, 498, 291, 361, 555, 306, 926, 257, 707, 857, 11, 291, 434, 406, 516, 281, 8114, 490, 264, 17641, 44745, 2690, 13, 759, 264, 2799, 5658, 6063, 420, 661, 390, 17962, 365, 264, 3671, 16235, 295, 264, 2063, 11, 550, 264, 2799, 727, 411, 747, 257, 16235, 23475, 1823, 29929, 293, 13283, 291, 490, 493, 10997, 1823, 300, 264, 48222, 1890, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.14037537930616692, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.830967423506081e-05}, {"id": 474, "seek": 360200, "start": 3602.0, "end": 3628.0, "text": " But probably the camera is taking more or less something that you could model as a random direction. Like clearly when you use the camera more than once, it's going to do the same thing each time. But for the point of view of how that direction relates to the image classification problem, it's more or less a random variable that you sample once, and it seems unlikely to align exactly with the normal to this class boundary.", "tokens": [50364, 583, 1391, 264, 2799, 307, 1940, 544, 420, 1570, 746, 300, 291, 727, 2316, 382, 257, 4974, 3513, 13, 1743, 4448, 562, 291, 764, 264, 2799, 544, 813, 1564, 11, 309, 311, 516, 281, 360, 264, 912, 551, 1184, 565, 13, 583, 337, 264, 935, 295, 1910, 295, 577, 300, 3513, 16155, 281, 264, 3256, 21538, 1154, 11, 309, 311, 544, 420, 1570, 257, 4974, 7006, 300, 291, 6889, 1564, 11, 293, 309, 2544, 17518, 281, 7975, 2293, 365, 264, 2710, 281, 341, 1508, 12866, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10016353478592434, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.459331522521097e-06}, {"id": 475, "seek": 363200, "start": 3633.0, "end": 3645.0, "text": " There's a lot of different defenses that we'd like to build. And you know it's a little bit disappointing that I'm mostly here to tell you about attacks. I'd like to tell you how to make your system's more robust.", "tokens": [50414, 821, 311, 257, 688, 295, 819, 35989, 300, 321, 1116, 411, 281, 1322, 13, 400, 291, 458, 309, 311, 257, 707, 857, 25054, 300, 286, 478, 5240, 510, 281, 980, 291, 466, 8122, 13, 286, 1116, 411, 281, 980, 291, 577, 281, 652, 428, 1185, 311, 544, 13956, 13, 51014, 51014, 583, 1936, 633, 2690, 321, 600, 3031, 575, 7612, 1238, 13425, 13, 400, 294, 1186, 11, 754, 562, 561, 362, 6572, 300, 436, 10727, 34135, 11, 1339, 456, 311, 668, 2940, 10577, 322, 23507, 670, 264, 1036, 2940, 2493, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14672850558632297, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.1298789083957672}, {"id": 476, "seek": 363200, "start": 3645.0, "end": 3661.0, "text": " But basically every attack we've tried has failed pretty badly. And in fact, even when people have published that they successfully defended, while there's been several papers on archive over the last several months,", "tokens": [50414, 821, 311, 257, 688, 295, 819, 35989, 300, 321, 1116, 411, 281, 1322, 13, 400, 291, 458, 309, 311, 257, 707, 857, 25054, 300, 286, 478, 5240, 510, 281, 980, 291, 466, 8122, 13, 286, 1116, 411, 281, 980, 291, 577, 281, 652, 428, 1185, 311, 544, 13956, 13, 51014, 51014, 583, 1936, 633, 2690, 321, 600, 3031, 575, 7612, 1238, 13425, 13, 400, 294, 1186, 11, 754, 562, 561, 362, 6572, 300, 436, 10727, 34135, 11, 1339, 456, 311, 668, 2940, 10577, 322, 23507, 670, 264, 1036, 2940, 2493, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14672850558632297, "compression_ratio": 1.6412213740458015, "no_speech_prob": 0.1298789083957672}, {"id": 477, "seek": 366100, "start": 3661.0, "end": 3668.0, "text": " Nicholas Carlinney at Berkeley just released a paper where he shows that 10 of those defenses are broken.", "tokens": [50364, 22924, 2741, 5045, 2397, 412, 23684, 445, 4736, 257, 3035, 689, 415, 3110, 300, 1266, 295, 729, 35989, 366, 5463, 13, 50714, 50714, 407, 341, 307, 257, 534, 11, 534, 1152, 1154, 13, 509, 393, 380, 445, 652, 309, 352, 1314, 538, 1228, 5164, 3890, 2144, 7512, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.13218135100144607, "compression_ratio": 1.3452380952380953, "no_speech_prob": 1.829565553634893e-05}, {"id": 478, "seek": 366100, "start": 3668.0, "end": 3676.0, "text": " So this is a really, really hard problem. You can't just make it go away by using traditional regularization techniques.", "tokens": [50364, 22924, 2741, 5045, 2397, 412, 23684, 445, 4736, 257, 3035, 689, 415, 3110, 300, 1266, 295, 729, 35989, 366, 5463, 13, 50714, 50714, 407, 341, 307, 257, 534, 11, 534, 1152, 1154, 13, 509, 393, 380, 445, 652, 309, 352, 1314, 538, 1228, 5164, 3890, 2144, 7512, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.13218135100144607, "compression_ratio": 1.3452380952380953, "no_speech_prob": 1.829565553634893e-05}, {"id": 479, "seek": 367600, "start": 3676.0, "end": 3692.0, "text": " And in particular, generative models are not enough to solve the problem. A lot of people say, oh, the problem that's going on here is that you don't know anything about the distribution over the input pixels. If you could just tell whether the input is realistic or not, then you'd be able to resist it.", "tokens": [50364, 400, 294, 1729, 11, 1337, 1166, 5245, 366, 406, 1547, 281, 5039, 264, 1154, 13, 316, 688, 295, 561, 584, 11, 1954, 11, 264, 1154, 300, 311, 516, 322, 510, 307, 300, 291, 500, 380, 458, 1340, 466, 264, 7316, 670, 264, 4846, 18668, 13, 759, 291, 727, 445, 980, 1968, 264, 4846, 307, 12465, 420, 406, 11, 550, 291, 1116, 312, 1075, 281, 4597, 309, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.09231124125735861, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.896671453025192e-05}, {"id": 480, "seek": 369200, "start": 3692.0, "end": 3712.0, "text": " And turns out that what's going on here is what matters more than getting the right distribution over the inputs X is getting the right posterior distribution over the class labels why given inputs X. So just using a generative model is not enough to solve the problem. I think a very carefully designed generative model could possibly do it.", "tokens": [50364, 400, 4523, 484, 300, 437, 311, 516, 322, 510, 307, 437, 7001, 544, 813, 1242, 264, 558, 7316, 670, 264, 15743, 1783, 307, 1242, 264, 558, 33529, 7316, 670, 264, 1508, 16949, 983, 2212, 15743, 1783, 13, 407, 445, 1228, 257, 1337, 1166, 2316, 307, 406, 1547, 281, 5039, 264, 1154, 13, 286, 519, 257, 588, 7500, 4761, 1337, 1166, 2316, 727, 6264, 360, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11098777907235281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0432215276523493e-05}, {"id": 481, "seek": 371200, "start": 3712.0, "end": 3725.0, "text": " Here I showed two different modes of a bimodal distribution and we have two different generative models that try to capture these modes. On the left we have a mixture of two gousians on the right we have a mixture of two leplocians.", "tokens": [50364, 1692, 286, 4712, 732, 819, 14068, 295, 257, 272, 332, 378, 304, 7316, 293, 321, 362, 732, 819, 1337, 1166, 5245, 300, 853, 281, 7983, 613, 14068, 13, 1282, 264, 1411, 321, 362, 257, 9925, 295, 732, 290, 563, 2567, 322, 264, 558, 321, 362, 257, 9925, 295, 732, 476, 564, 905, 2567, 13, 51014, 51014, 509, 393, 406, 534, 980, 264, 2649, 19622, 1296, 264, 7316, 436, 26952, 670, 1783, 293, 264, 2649, 294, 264, 22119, 436, 13279, 281, 264, 3097, 1412, 307, 32570, 964, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1918268308534727, "compression_ratio": 1.8597285067873304, "no_speech_prob": 5.6499931815778837e-05}, {"id": 482, "seek": 371200, "start": 3725.0, "end": 3735.0, "text": " You can not really tell the difference visually between the distribution they impose over X and the difference in the likelihood they assigned to the training data is negligible.", "tokens": [50364, 1692, 286, 4712, 732, 819, 14068, 295, 257, 272, 332, 378, 304, 7316, 293, 321, 362, 732, 819, 1337, 1166, 5245, 300, 853, 281, 7983, 613, 14068, 13, 1282, 264, 1411, 321, 362, 257, 9925, 295, 732, 290, 563, 2567, 322, 264, 558, 321, 362, 257, 9925, 295, 732, 476, 564, 905, 2567, 13, 51014, 51014, 509, 393, 406, 534, 980, 264, 2649, 19622, 1296, 264, 7316, 436, 26952, 670, 1783, 293, 264, 2649, 294, 264, 22119, 436, 13279, 281, 264, 3097, 1412, 307, 32570, 964, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1918268308534727, "compression_ratio": 1.8597285067873304, "no_speech_prob": 5.6499931815778837e-05}, {"id": 483, "seek": 373500, "start": 3735.0, "end": 3748.0, "text": " The posterior distribution they assign over classes is extremely different. On the left we get a logistic regression classifier that has very high confidence out at the tails of the distribution where there is never any training data.", "tokens": [50364, 440, 33529, 7316, 436, 6269, 670, 5359, 307, 4664, 819, 13, 1282, 264, 1411, 321, 483, 257, 3565, 3142, 24590, 1508, 9902, 300, 575, 588, 1090, 6687, 484, 412, 264, 28537, 295, 264, 7316, 689, 456, 307, 1128, 604, 3097, 1412, 13, 51014, 51014, 1282, 264, 558, 365, 257, 476, 564, 905, 952, 7316, 321, 1496, 766, 281, 544, 420, 1570, 2625, 2625, 13, 865, 13, 51364, 51814], "temperature": 0.0, "avg_logprob": -0.1729663712637765, "compression_ratio": 1.595, "no_speech_prob": 9.982466508517973e-06}, {"id": 484, "seek": 373500, "start": 3748.0, "end": 3755.0, "text": " On the right with a leplocian distribution we level off to more or less 50 50. Yeah.", "tokens": [50364, 440, 33529, 7316, 436, 6269, 670, 5359, 307, 4664, 819, 13, 1282, 264, 1411, 321, 483, 257, 3565, 3142, 24590, 1508, 9902, 300, 575, 588, 1090, 6687, 484, 412, 264, 28537, 295, 264, 7316, 689, 456, 307, 1128, 604, 3097, 1412, 13, 51014, 51014, 1282, 264, 558, 365, 257, 476, 564, 905, 952, 7316, 321, 1496, 766, 281, 544, 420, 1570, 2625, 2625, 13, 865, 13, 51364, 51814], "temperature": 0.0, "avg_logprob": -0.1729663712637765, "compression_ratio": 1.595, "no_speech_prob": 9.982466508517973e-06}, {"id": 485, "seek": 375500, "start": 3755.0, "end": 3777.0, "text": " The issue is that it's a non-stationary distribution. So if you train it to recognize one kind of adversarial example then it will become vulnerable to another kind of design to flow its its detector.", "tokens": [50364, 440, 2734, 307, 300, 309, 311, 257, 2107, 12, 19159, 822, 7316, 13, 407, 498, 291, 3847, 309, 281, 5521, 472, 733, 295, 17641, 44745, 1365, 550, 309, 486, 1813, 10955, 281, 1071, 733, 295, 1715, 281, 3095, 1080, 1080, 25712, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.24055733888045602, "compression_ratio": 1.3986013986013985, "no_speech_prob": 0.00019298095139674842}, {"id": 486, "seek": 377700, "start": 3777.0, "end": 3786.0, "text": " That's one of the category of defenses that Nicholas broke in his latest paper that he put out.", "tokens": [50364, 663, 311, 472, 295, 264, 7719, 295, 35989, 300, 22924, 6902, 294, 702, 6792, 3035, 300, 415, 829, 484, 13, 50814, 50814, 407, 510, 1936, 264, 3922, 295, 2293, 264, 1605, 295, 1337, 1166, 2316, 575, 257, 955, 1802, 322, 1968, 264, 33529, 3643, 15957, 3142, 420, 9452, 382, 264, 2316, 48224, 1024, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1019373844409811, "compression_ratio": 1.5402298850574712, "no_speech_prob": 3.649985956144519e-05}, {"id": 487, "seek": 377700, "start": 3786.0, "end": 3799.0, "text": " So here basically the choice of exactly the family of generative model has a big effect on whether the posterior becomes deterministic or uniform as the model extrapolates.", "tokens": [50364, 663, 311, 472, 295, 264, 7719, 295, 35989, 300, 22924, 6902, 294, 702, 6792, 3035, 300, 415, 829, 484, 13, 50814, 50814, 407, 510, 1936, 264, 3922, 295, 2293, 264, 1605, 295, 1337, 1166, 2316, 575, 257, 955, 1802, 322, 1968, 264, 33529, 3643, 15957, 3142, 420, 9452, 382, 264, 2316, 48224, 1024, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1019373844409811, "compression_ratio": 1.5402298850574712, "no_speech_prob": 3.649985956144519e-05}, {"id": 488, "seek": 379900, "start": 3799.0, "end": 3809.0, "text": " So we can design a really rich deep generative model that can generate realistic image net images and also correctly calculate its posterior distribution.", "tokens": [50364, 407, 321, 393, 1715, 257, 534, 4593, 2452, 1337, 1166, 2316, 300, 393, 8460, 12465, 3256, 2533, 5267, 293, 611, 8944, 8873, 1080, 33529, 7316, 13, 50864, 50864, 1396, 1310, 746, 411, 341, 3109, 727, 589, 13, 51014, 51014, 583, 412, 264, 1623, 309, 311, 534, 2252, 281, 483, 604, 295, 729, 31959, 3142, 20448, 3006, 13, 51264, 51264, 400, 437, 2673, 2314, 307, 4079, 2831, 321, 652, 364, 28023, 300, 7700, 264, 33529, 7316, 281, 48224, 473, 588, 43586, 797, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15993385536726132, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00010788599320221692}, {"id": 489, "seek": 379900, "start": 3809.0, "end": 3812.0, "text": " Then maybe something like this approach could work.", "tokens": [50364, 407, 321, 393, 1715, 257, 534, 4593, 2452, 1337, 1166, 2316, 300, 393, 8460, 12465, 3256, 2533, 5267, 293, 611, 8944, 8873, 1080, 33529, 7316, 13, 50864, 50864, 1396, 1310, 746, 411, 341, 3109, 727, 589, 13, 51014, 51014, 583, 412, 264, 1623, 309, 311, 534, 2252, 281, 483, 604, 295, 729, 31959, 3142, 20448, 3006, 13, 51264, 51264, 400, 437, 2673, 2314, 307, 4079, 2831, 321, 652, 364, 28023, 300, 7700, 264, 33529, 7316, 281, 48224, 473, 588, 43586, 797, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15993385536726132, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00010788599320221692}, {"id": 490, "seek": 379900, "start": 3812.0, "end": 3817.0, "text": " But at the moment it's really difficult to get any of those probabilistic calculations correct.", "tokens": [50364, 407, 321, 393, 1715, 257, 534, 4593, 2452, 1337, 1166, 2316, 300, 393, 8460, 12465, 3256, 2533, 5267, 293, 611, 8944, 8873, 1080, 33529, 7316, 13, 50864, 50864, 1396, 1310, 746, 411, 341, 3109, 727, 589, 13, 51014, 51014, 583, 412, 264, 1623, 309, 311, 534, 2252, 281, 483, 604, 295, 729, 31959, 3142, 20448, 3006, 13, 51264, 51264, 400, 437, 2673, 2314, 307, 4079, 2831, 321, 652, 364, 28023, 300, 7700, 264, 33529, 7316, 281, 48224, 473, 588, 43586, 797, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15993385536726132, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00010788599320221692}, {"id": 491, "seek": 379900, "start": 3817.0, "end": 3826.0, "text": " And what usually happens is somewhere rather we make an approximation that causes the posterior distribution to extrapolate very linearly again.", "tokens": [50364, 407, 321, 393, 1715, 257, 534, 4593, 2452, 1337, 1166, 2316, 300, 393, 8460, 12465, 3256, 2533, 5267, 293, 611, 8944, 8873, 1080, 33529, 7316, 13, 50864, 50864, 1396, 1310, 746, 411, 341, 3109, 727, 589, 13, 51014, 51014, 583, 412, 264, 1623, 309, 311, 534, 2252, 281, 483, 604, 295, 729, 31959, 3142, 20448, 3006, 13, 51264, 51264, 400, 437, 2673, 2314, 307, 4079, 2831, 321, 652, 364, 28023, 300, 7700, 264, 33529, 7316, 281, 48224, 473, 588, 43586, 797, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15993385536726132, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00010788599320221692}, {"id": 492, "seek": 382600, "start": 3826.0, "end": 3836.0, "text": " And it's it's been a difficult engineering challenge to build generative models that actually capture these distributions accurately.", "tokens": [50364, 400, 309, 311, 309, 311, 668, 257, 2252, 7043, 3430, 281, 1322, 1337, 1166, 5245, 300, 767, 7983, 613, 37870, 20095, 13, 50864, 50864, 440, 11455, 8542, 1639, 20904, 5112, 505, 300, 2035, 3909, 321, 576, 411, 527, 21538, 2445, 281, 362, 293, 18161, 2533, 300, 311, 955, 1547, 13416, 281, 312, 1075, 281, 2906, 309, 13, 51414, 51414, 467, 311, 364, 1269, 1168, 1968, 321, 393, 3847, 264, 18161, 2533, 281, 362, 300, 2445, 11, 457, 321, 458, 300, 321, 820, 412, 1935, 312, 1075, 281, 976, 264, 558, 3909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06237131357192993, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6003391869599e-05}, {"id": 493, "seek": 382600, "start": 3836.0, "end": 3847.0, "text": " The universal approximator theorem tells us that whatever shape we would like our classification function to have and neural net that's big enough ought to be able to represent it.", "tokens": [50364, 400, 309, 311, 309, 311, 668, 257, 2252, 7043, 3430, 281, 1322, 1337, 1166, 5245, 300, 767, 7983, 613, 37870, 20095, 13, 50864, 50864, 440, 11455, 8542, 1639, 20904, 5112, 505, 300, 2035, 3909, 321, 576, 411, 527, 21538, 2445, 281, 362, 293, 18161, 2533, 300, 311, 955, 1547, 13416, 281, 312, 1075, 281, 2906, 309, 13, 51414, 51414, 467, 311, 364, 1269, 1168, 1968, 321, 393, 3847, 264, 18161, 2533, 281, 362, 300, 2445, 11, 457, 321, 458, 300, 321, 820, 412, 1935, 312, 1075, 281, 976, 264, 558, 3909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06237131357192993, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6003391869599e-05}, {"id": 494, "seek": 382600, "start": 3847.0, "end": 3854.0, "text": " It's an open question whether we can train the neural net to have that function, but we know that we should at least be able to give the right shape.", "tokens": [50364, 400, 309, 311, 309, 311, 668, 257, 2252, 7043, 3430, 281, 1322, 1337, 1166, 5245, 300, 767, 7983, 613, 37870, 20095, 13, 50864, 50864, 440, 11455, 8542, 1639, 20904, 5112, 505, 300, 2035, 3909, 321, 576, 411, 527, 21538, 2445, 281, 362, 293, 18161, 2533, 300, 311, 955, 1547, 13416, 281, 312, 1075, 281, 2906, 309, 13, 51414, 51414, 467, 311, 364, 1269, 1168, 1968, 321, 393, 3847, 264, 18161, 2533, 281, 362, 300, 2445, 11, 457, 321, 458, 300, 321, 820, 412, 1935, 312, 1075, 281, 976, 264, 558, 3909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06237131357192993, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6003391869599e-05}, {"id": 495, "seek": 385400, "start": 3854.0, "end": 3862.0, "text": " So so far we've been getting neural nets that give us these very linear decision functions and we'd like to get something that looks a little bit more like a step function.", "tokens": [50364, 407, 370, 1400, 321, 600, 668, 1242, 18161, 36170, 300, 976, 505, 613, 588, 8213, 3537, 6828, 293, 321, 1116, 411, 281, 483, 746, 300, 1542, 257, 707, 857, 544, 411, 257, 1823, 2445, 13, 50764, 50764, 407, 437, 498, 321, 767, 445, 3847, 322, 633, 17436, 5110, 337, 633, 4846, 2031, 294, 264, 3097, 992, 321, 611, 584, 321, 528, 291, 281, 3847, 2031, 1804, 364, 2690, 281, 4471, 281, 264, 912, 1508, 7645, 382, 264, 3380, 13, 51414, 51414, 467, 4523, 484, 300, 341, 1333, 295, 1985, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09956225943058095, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.3963728633825667e-05}, {"id": 496, "seek": 385400, "start": 3862.0, "end": 3875.0, "text": " So what if we actually just train on every serial examples for every input x in the training set we also say we want you to train x plus an attack to map to the same class label as the original.", "tokens": [50364, 407, 370, 1400, 321, 600, 668, 1242, 18161, 36170, 300, 976, 505, 613, 588, 8213, 3537, 6828, 293, 321, 1116, 411, 281, 483, 746, 300, 1542, 257, 707, 857, 544, 411, 257, 1823, 2445, 13, 50764, 50764, 407, 437, 498, 321, 767, 445, 3847, 322, 633, 17436, 5110, 337, 633, 4846, 2031, 294, 264, 3097, 992, 321, 611, 584, 321, 528, 291, 281, 3847, 2031, 1804, 364, 2690, 281, 4471, 281, 264, 912, 1508, 7645, 382, 264, 3380, 13, 51414, 51414, 467, 4523, 484, 300, 341, 1333, 295, 1985, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09956225943058095, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.3963728633825667e-05}, {"id": 497, "seek": 385400, "start": 3875.0, "end": 3878.0, "text": " It turns out that this sort of works.", "tokens": [50364, 407, 370, 1400, 321, 600, 668, 1242, 18161, 36170, 300, 976, 505, 613, 588, 8213, 3537, 6828, 293, 321, 1116, 411, 281, 483, 746, 300, 1542, 257, 707, 857, 544, 411, 257, 1823, 2445, 13, 50764, 50764, 407, 437, 498, 321, 767, 445, 3847, 322, 633, 17436, 5110, 337, 633, 4846, 2031, 294, 264, 3097, 992, 321, 611, 584, 321, 528, 291, 281, 3847, 2031, 1804, 364, 2690, 281, 4471, 281, 264, 912, 1508, 7645, 382, 264, 3380, 13, 51414, 51414, 467, 4523, 484, 300, 341, 1333, 295, 1985, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09956225943058095, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.3963728633825667e-05}, {"id": 498, "seek": 387800, "start": 3878.0, "end": 3889.0, "text": " You can generally resist the same kind of attack that you train on and an important consideration is making sure that you can run your attack very quickly so that you can train on lots of examples.", "tokens": [50364, 509, 393, 5101, 4597, 264, 912, 733, 295, 2690, 300, 291, 3847, 322, 293, 364, 1021, 12381, 307, 1455, 988, 300, 291, 393, 1190, 428, 2690, 588, 2661, 370, 300, 291, 393, 3847, 322, 3195, 295, 5110, 13, 50914, 50914, 407, 510, 264, 3092, 7605, 412, 264, 588, 1192, 264, 472, 300, 1177, 380, 534, 16333, 709, 412, 439, 300, 311, 264, 1500, 3056, 322, 17641, 44745, 5110, 498, 291, 3847, 322, 2541, 5110, 787, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06755756735801696, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.877030445029959e-05}, {"id": 499, "seek": 387800, "start": 3889.0, "end": 3901.0, "text": " So here the green curve at the very top the one that doesn't really descend much at all that's the test center on adversarial examples if you train on clean examples only.", "tokens": [50364, 509, 393, 5101, 4597, 264, 912, 733, 295, 2690, 300, 291, 3847, 322, 293, 364, 1021, 12381, 307, 1455, 988, 300, 291, 393, 1190, 428, 2690, 588, 2661, 370, 300, 291, 393, 3847, 322, 3195, 295, 5110, 13, 50914, 50914, 407, 510, 264, 3092, 7605, 412, 264, 588, 1192, 264, 472, 300, 1177, 380, 534, 16333, 709, 412, 439, 300, 311, 264, 1500, 3056, 322, 17641, 44745, 5110, 498, 291, 3847, 322, 2541, 5110, 787, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06755756735801696, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.877030445029959e-05}, {"id": 500, "seek": 390100, "start": 3901.0, "end": 3911.0, "text": " The cyan curve that descends more or less diagonally through the middle of the plot that's the tester on adversarial examples if you train on adversarial examples.", "tokens": [50364, 440, 47463, 7605, 300, 7471, 2581, 544, 420, 1570, 17405, 379, 807, 264, 2808, 295, 264, 7542, 300, 311, 264, 36101, 322, 17641, 44745, 5110, 498, 291, 3847, 322, 17641, 44745, 5110, 13, 50864, 50864, 509, 393, 536, 300, 309, 775, 767, 5407, 10591, 309, 2170, 760, 281, 257, 707, 857, 1570, 813, 502, 4, 6713, 13, 51164, 51164, 400, 264, 1021, 551, 281, 1066, 294, 1575, 510, 307, 300, 341, 307, 257, 2370, 16235, 18609, 3170, 17641, 44745, 5110, 309, 311, 709, 6081, 281, 4597, 17138, 1166, 4825, 1823, 17641, 44745, 5110, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11323781889312122, "compression_ratio": 1.7808764940239044, "no_speech_prob": 7.93631170381559e-06}, {"id": 501, "seek": 390100, "start": 3911.0, "end": 3917.0, "text": " You can see that it does actually reduce significantly it gets down to a little bit less than 1% error.", "tokens": [50364, 440, 47463, 7605, 300, 7471, 2581, 544, 420, 1570, 17405, 379, 807, 264, 2808, 295, 264, 7542, 300, 311, 264, 36101, 322, 17641, 44745, 5110, 498, 291, 3847, 322, 17641, 44745, 5110, 13, 50864, 50864, 509, 393, 536, 300, 309, 775, 767, 5407, 10591, 309, 2170, 760, 281, 257, 707, 857, 1570, 813, 502, 4, 6713, 13, 51164, 51164, 400, 264, 1021, 551, 281, 1066, 294, 1575, 510, 307, 300, 341, 307, 257, 2370, 16235, 18609, 3170, 17641, 44745, 5110, 309, 311, 709, 6081, 281, 4597, 17138, 1166, 4825, 1823, 17641, 44745, 5110, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11323781889312122, "compression_ratio": 1.7808764940239044, "no_speech_prob": 7.93631170381559e-06}, {"id": 502, "seek": 390100, "start": 3917.0, "end": 3928.0, "text": " And the important thing to keep in mind here is that this is a fast gradient sine method adversarial examples it's much harder to resist iterative multi step adversarial examples.", "tokens": [50364, 440, 47463, 7605, 300, 7471, 2581, 544, 420, 1570, 17405, 379, 807, 264, 2808, 295, 264, 7542, 300, 311, 264, 36101, 322, 17641, 44745, 5110, 498, 291, 3847, 322, 17641, 44745, 5110, 13, 50864, 50864, 509, 393, 536, 300, 309, 775, 767, 5407, 10591, 309, 2170, 760, 281, 257, 707, 857, 1570, 813, 502, 4, 6713, 13, 51164, 51164, 400, 264, 1021, 551, 281, 1066, 294, 1575, 510, 307, 300, 341, 307, 257, 2370, 16235, 18609, 3170, 17641, 44745, 5110, 309, 311, 709, 6081, 281, 4597, 17138, 1166, 4825, 1823, 17641, 44745, 5110, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11323781889312122, "compression_ratio": 1.7808764940239044, "no_speech_prob": 7.93631170381559e-06}, {"id": 503, "seek": 392800, "start": 3928.0, "end": 3937.0, "text": " We run an optimized for a long time searching for a vulnerability and another thing to keep in mind is that we're testing on the same kind of adversarial examples that we train on.", "tokens": [50364, 492, 1190, 364, 26941, 337, 257, 938, 565, 10808, 337, 257, 24210, 293, 1071, 551, 281, 1066, 294, 1575, 307, 300, 321, 434, 4997, 322, 264, 912, 733, 295, 17641, 44745, 5110, 300, 321, 3847, 322, 13, 50814, 50814, 467, 311, 6081, 281, 2674, 1125, 490, 472, 19618, 9284, 281, 1071, 13, 51064, 51064, 3146, 9660, 498, 291, 574, 412, 264, 498, 291, 574, 412, 437, 2314, 322, 2541, 5110, 264, 3344, 7605, 3110, 437, 2314, 322, 264, 2541, 1500, 992, 6713, 3314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14501902190121738, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.2363670723279938e-05}, {"id": 504, "seek": 392800, "start": 3937.0, "end": 3942.0, "text": " It's harder to generalize from one optimization algorithm to another.", "tokens": [50364, 492, 1190, 364, 26941, 337, 257, 938, 565, 10808, 337, 257, 24210, 293, 1071, 551, 281, 1066, 294, 1575, 307, 300, 321, 434, 4997, 322, 264, 912, 733, 295, 17641, 44745, 5110, 300, 321, 3847, 322, 13, 50814, 50814, 467, 311, 6081, 281, 2674, 1125, 490, 472, 19618, 9284, 281, 1071, 13, 51064, 51064, 3146, 9660, 498, 291, 574, 412, 264, 498, 291, 574, 412, 437, 2314, 322, 2541, 5110, 264, 3344, 7605, 3110, 437, 2314, 322, 264, 2541, 1500, 992, 6713, 3314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14501902190121738, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.2363670723279938e-05}, {"id": 505, "seek": 392800, "start": 3942.0, "end": 3953.0, "text": " By comparison if you look at the if you look at what happens on clean examples the blue curve shows what happens on the clean test set error rate.", "tokens": [50364, 492, 1190, 364, 26941, 337, 257, 938, 565, 10808, 337, 257, 24210, 293, 1071, 551, 281, 1066, 294, 1575, 307, 300, 321, 434, 4997, 322, 264, 912, 733, 295, 17641, 44745, 5110, 300, 321, 3847, 322, 13, 50814, 50814, 467, 311, 6081, 281, 2674, 1125, 490, 472, 19618, 9284, 281, 1071, 13, 51064, 51064, 3146, 9660, 498, 291, 574, 412, 264, 498, 291, 574, 412, 437, 2314, 322, 2541, 5110, 264, 3344, 7605, 3110, 437, 2314, 322, 264, 2541, 1500, 992, 6713, 3314, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14501902190121738, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.2363670723279938e-05}, {"id": 506, "seek": 395300, "start": 3953.0, "end": 3961.0, "text": " If you train only on clean examples the red curve shows what happens if you train on both clean and adversarial examples.", "tokens": [50364, 759, 291, 3847, 787, 322, 2541, 5110, 264, 2182, 7605, 3110, 437, 2314, 498, 291, 3847, 322, 1293, 2541, 293, 17641, 44745, 5110, 13, 50764, 50764, 492, 536, 300, 264, 2182, 7605, 767, 11438, 3126, 813, 264, 3344, 7605, 13, 50964, 50964, 407, 322, 341, 5633, 3097, 322, 17641, 44745, 5110, 767, 4254, 505, 360, 264, 3380, 5633, 1101, 13, 51214, 51214, 639, 307, 570, 322, 264, 3380, 5633, 321, 645, 670, 69, 2414, 3097, 322, 17641, 44745, 5110, 307, 257, 665, 3890, 6545, 13, 51464, 51464, 759, 291, 434, 670, 69, 2414, 309, 393, 652, 291, 670, 6845, 1570, 498, 291, 434, 833, 69, 2414, 309, 603, 445, 652, 291, 833, 6845, 5324, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0935646425776121, "compression_ratio": 2.0441767068273093, "no_speech_prob": 5.610745574813336e-06}, {"id": 507, "seek": 395300, "start": 3961.0, "end": 3965.0, "text": " We see that the red curve actually drops lower than the blue curve.", "tokens": [50364, 759, 291, 3847, 787, 322, 2541, 5110, 264, 2182, 7605, 3110, 437, 2314, 498, 291, 3847, 322, 1293, 2541, 293, 17641, 44745, 5110, 13, 50764, 50764, 492, 536, 300, 264, 2182, 7605, 767, 11438, 3126, 813, 264, 3344, 7605, 13, 50964, 50964, 407, 322, 341, 5633, 3097, 322, 17641, 44745, 5110, 767, 4254, 505, 360, 264, 3380, 5633, 1101, 13, 51214, 51214, 639, 307, 570, 322, 264, 3380, 5633, 321, 645, 670, 69, 2414, 3097, 322, 17641, 44745, 5110, 307, 257, 665, 3890, 6545, 13, 51464, 51464, 759, 291, 434, 670, 69, 2414, 309, 393, 652, 291, 670, 6845, 1570, 498, 291, 434, 833, 69, 2414, 309, 603, 445, 652, 291, 833, 6845, 5324, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0935646425776121, "compression_ratio": 2.0441767068273093, "no_speech_prob": 5.610745574813336e-06}, {"id": 508, "seek": 395300, "start": 3965.0, "end": 3970.0, "text": " So on this task training on adversarial examples actually helped us do the original task better.", "tokens": [50364, 759, 291, 3847, 787, 322, 2541, 5110, 264, 2182, 7605, 3110, 437, 2314, 498, 291, 3847, 322, 1293, 2541, 293, 17641, 44745, 5110, 13, 50764, 50764, 492, 536, 300, 264, 2182, 7605, 767, 11438, 3126, 813, 264, 3344, 7605, 13, 50964, 50964, 407, 322, 341, 5633, 3097, 322, 17641, 44745, 5110, 767, 4254, 505, 360, 264, 3380, 5633, 1101, 13, 51214, 51214, 639, 307, 570, 322, 264, 3380, 5633, 321, 645, 670, 69, 2414, 3097, 322, 17641, 44745, 5110, 307, 257, 665, 3890, 6545, 13, 51464, 51464, 759, 291, 434, 670, 69, 2414, 309, 393, 652, 291, 670, 6845, 1570, 498, 291, 434, 833, 69, 2414, 309, 603, 445, 652, 291, 833, 6845, 5324, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0935646425776121, "compression_ratio": 2.0441767068273093, "no_speech_prob": 5.610745574813336e-06}, {"id": 509, "seek": 395300, "start": 3970.0, "end": 3975.0, "text": " This is because on the original task we were overfitting training on adversarial examples is a good regularizer.", "tokens": [50364, 759, 291, 3847, 787, 322, 2541, 5110, 264, 2182, 7605, 3110, 437, 2314, 498, 291, 3847, 322, 1293, 2541, 293, 17641, 44745, 5110, 13, 50764, 50764, 492, 536, 300, 264, 2182, 7605, 767, 11438, 3126, 813, 264, 3344, 7605, 13, 50964, 50964, 407, 322, 341, 5633, 3097, 322, 17641, 44745, 5110, 767, 4254, 505, 360, 264, 3380, 5633, 1101, 13, 51214, 51214, 639, 307, 570, 322, 264, 3380, 5633, 321, 645, 670, 69, 2414, 3097, 322, 17641, 44745, 5110, 307, 257, 665, 3890, 6545, 13, 51464, 51464, 759, 291, 434, 670, 69, 2414, 309, 393, 652, 291, 670, 6845, 1570, 498, 291, 434, 833, 69, 2414, 309, 603, 445, 652, 291, 833, 6845, 5324, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0935646425776121, "compression_ratio": 2.0441767068273093, "no_speech_prob": 5.610745574813336e-06}, {"id": 510, "seek": 395300, "start": 3975.0, "end": 3981.0, "text": " If you're overfitting it can make you overfit less if you're underfitting it'll just make you underfit worse.", "tokens": [50364, 759, 291, 3847, 787, 322, 2541, 5110, 264, 2182, 7605, 3110, 437, 2314, 498, 291, 3847, 322, 1293, 2541, 293, 17641, 44745, 5110, 13, 50764, 50764, 492, 536, 300, 264, 2182, 7605, 767, 11438, 3126, 813, 264, 3344, 7605, 13, 50964, 50964, 407, 322, 341, 5633, 3097, 322, 17641, 44745, 5110, 767, 4254, 505, 360, 264, 3380, 5633, 1101, 13, 51214, 51214, 639, 307, 570, 322, 264, 3380, 5633, 321, 645, 670, 69, 2414, 3097, 322, 17641, 44745, 5110, 307, 257, 665, 3890, 6545, 13, 51464, 51464, 759, 291, 434, 670, 69, 2414, 309, 393, 652, 291, 670, 6845, 1570, 498, 291, 434, 833, 69, 2414, 309, 603, 445, 652, 291, 833, 6845, 5324, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0935646425776121, "compression_ratio": 2.0441767068273093, "no_speech_prob": 5.610745574813336e-06}, {"id": 511, "seek": 398100, "start": 3981.0, "end": 3987.0, "text": " Other kinds of models besides deep neural nets don't benefit as much from adversarial training.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 512, "seek": 398100, "start": 3987.0, "end": 3993.0, "text": " So when we started this whole topic of study we thought that deep neural nets might be uniquely vulnerable to adversarial examples.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 513, "seek": 398100, "start": 3993.0, "end": 3999.0, "text": " It turns out that actually there are one of the few models that has a clear path to resisting them.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 514, "seek": 398100, "start": 3999.0, "end": 4001.0, "text": " Linear models are just always going to be linear.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 515, "seek": 398100, "start": 4001.0, "end": 4004.0, "text": " They don't have much hope of resisting adversarial examples.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 516, "seek": 398100, "start": 4004.0, "end": 4007.0, "text": " Deep neural nets can be trained to be nonlinear.", "tokens": [50364, 5358, 3685, 295, 5245, 11868, 2452, 18161, 36170, 500, 380, 5121, 382, 709, 490, 17641, 44745, 3097, 13, 50664, 50664, 407, 562, 321, 1409, 341, 1379, 4829, 295, 2979, 321, 1194, 300, 2452, 18161, 36170, 1062, 312, 31474, 10955, 281, 17641, 44745, 5110, 13, 50964, 50964, 467, 4523, 484, 300, 767, 456, 366, 472, 295, 264, 1326, 5245, 300, 575, 257, 1850, 3100, 281, 43940, 552, 13, 51264, 51264, 14670, 289, 5245, 366, 445, 1009, 516, 281, 312, 8213, 13, 51364, 51364, 814, 500, 380, 362, 709, 1454, 295, 43940, 17641, 44745, 5110, 13, 51514, 51514, 14895, 18161, 36170, 393, 312, 8895, 281, 312, 2107, 28263, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09519610533843169, "compression_ratio": 1.8239700374531835, "no_speech_prob": 9.69665234151762e-06}, {"id": 517, "seek": 400700, "start": 4007.0, "end": 4011.0, "text": " And so it seems like there's a path to a solution for them.", "tokens": [50364, 400, 370, 309, 2544, 411, 456, 311, 257, 3100, 281, 257, 3827, 337, 552, 13, 50564, 50564, 2754, 365, 17641, 44745, 3097, 321, 920, 915, 300, 321, 366, 11299, 281, 652, 5245, 689, 498, 291, 19719, 264, 4846, 281, 5784, 281, 819, 5359, 291, 483, 5110, 295, 729, 5359, 13, 51064, 51064, 1692, 286, 722, 484, 365, 257, 383, 19, 3279, 5898, 293, 286, 1261, 309, 666, 1184, 295, 264, 1266, 819, 383, 19, 3279, 5359, 13, 51414, 51414, 1407, 689, 264, 2808, 295, 264, 7542, 291, 393, 536, 264, 5898, 575, 1409, 281, 574, 257, 707, 857, 411, 257, 5255, 13, 51664, 51664, 583, 264, 5255, 1508, 307, 264, 787, 472, 300, 321, 600, 808, 4992, 2651, 8850, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08713719152635144, "compression_ratio": 1.7050847457627119, "no_speech_prob": 2.0005618353025056e-05}, {"id": 518, "seek": 400700, "start": 4011.0, "end": 4021.0, "text": " Even with adversarial training we still find that we are unable to make models where if you optimize the input to belong to different classes you get examples of those classes.", "tokens": [50364, 400, 370, 309, 2544, 411, 456, 311, 257, 3100, 281, 257, 3827, 337, 552, 13, 50564, 50564, 2754, 365, 17641, 44745, 3097, 321, 920, 915, 300, 321, 366, 11299, 281, 652, 5245, 689, 498, 291, 19719, 264, 4846, 281, 5784, 281, 819, 5359, 291, 483, 5110, 295, 729, 5359, 13, 51064, 51064, 1692, 286, 722, 484, 365, 257, 383, 19, 3279, 5898, 293, 286, 1261, 309, 666, 1184, 295, 264, 1266, 819, 383, 19, 3279, 5359, 13, 51414, 51414, 1407, 689, 264, 2808, 295, 264, 7542, 291, 393, 536, 264, 5898, 575, 1409, 281, 574, 257, 707, 857, 411, 257, 5255, 13, 51664, 51664, 583, 264, 5255, 1508, 307, 264, 787, 472, 300, 321, 600, 808, 4992, 2651, 8850, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08713719152635144, "compression_ratio": 1.7050847457627119, "no_speech_prob": 2.0005618353025056e-05}, {"id": 519, "seek": 400700, "start": 4021.0, "end": 4028.0, "text": " Here I start out with a C410 truck and I turn it into each of the 10 different C410 classes.", "tokens": [50364, 400, 370, 309, 2544, 411, 456, 311, 257, 3100, 281, 257, 3827, 337, 552, 13, 50564, 50564, 2754, 365, 17641, 44745, 3097, 321, 920, 915, 300, 321, 366, 11299, 281, 652, 5245, 689, 498, 291, 19719, 264, 4846, 281, 5784, 281, 819, 5359, 291, 483, 5110, 295, 729, 5359, 13, 51064, 51064, 1692, 286, 722, 484, 365, 257, 383, 19, 3279, 5898, 293, 286, 1261, 309, 666, 1184, 295, 264, 1266, 819, 383, 19, 3279, 5359, 13, 51414, 51414, 1407, 689, 264, 2808, 295, 264, 7542, 291, 393, 536, 264, 5898, 575, 1409, 281, 574, 257, 707, 857, 411, 257, 5255, 13, 51664, 51664, 583, 264, 5255, 1508, 307, 264, 787, 472, 300, 321, 600, 808, 4992, 2651, 8850, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08713719152635144, "compression_ratio": 1.7050847457627119, "no_speech_prob": 2.0005618353025056e-05}, {"id": 520, "seek": 400700, "start": 4028.0, "end": 4033.0, "text": " To where the middle of the plot you can see the truck has started to look a little bit like a bird.", "tokens": [50364, 400, 370, 309, 2544, 411, 456, 311, 257, 3100, 281, 257, 3827, 337, 552, 13, 50564, 50564, 2754, 365, 17641, 44745, 3097, 321, 920, 915, 300, 321, 366, 11299, 281, 652, 5245, 689, 498, 291, 19719, 264, 4846, 281, 5784, 281, 819, 5359, 291, 483, 5110, 295, 729, 5359, 13, 51064, 51064, 1692, 286, 722, 484, 365, 257, 383, 19, 3279, 5898, 293, 286, 1261, 309, 666, 1184, 295, 264, 1266, 819, 383, 19, 3279, 5359, 13, 51414, 51414, 1407, 689, 264, 2808, 295, 264, 7542, 291, 393, 536, 264, 5898, 575, 1409, 281, 574, 257, 707, 857, 411, 257, 5255, 13, 51664, 51664, 583, 264, 5255, 1508, 307, 264, 787, 472, 300, 321, 600, 808, 4992, 2651, 8850, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08713719152635144, "compression_ratio": 1.7050847457627119, "no_speech_prob": 2.0005618353025056e-05}, {"id": 521, "seek": 400700, "start": 4033.0, "end": 4036.0, "text": " But the bird class is the only one that we've come anywhere near hitting.", "tokens": [50364, 400, 370, 309, 2544, 411, 456, 311, 257, 3100, 281, 257, 3827, 337, 552, 13, 50564, 50564, 2754, 365, 17641, 44745, 3097, 321, 920, 915, 300, 321, 366, 11299, 281, 652, 5245, 689, 498, 291, 19719, 264, 4846, 281, 5784, 281, 819, 5359, 291, 483, 5110, 295, 729, 5359, 13, 51064, 51064, 1692, 286, 722, 484, 365, 257, 383, 19, 3279, 5898, 293, 286, 1261, 309, 666, 1184, 295, 264, 1266, 819, 383, 19, 3279, 5359, 13, 51414, 51414, 1407, 689, 264, 2808, 295, 264, 7542, 291, 393, 536, 264, 5898, 575, 1409, 281, 574, 257, 707, 857, 411, 257, 5255, 13, 51664, 51664, 583, 264, 5255, 1508, 307, 264, 787, 472, 300, 321, 600, 808, 4992, 2651, 8850, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08713719152635144, "compression_ratio": 1.7050847457627119, "no_speech_prob": 2.0005618353025056e-05}, {"id": 522, "seek": 403600, "start": 4036.0, "end": 4040.0, "text": " So even with adversarial training we're still very far from solving this problem.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 523, "seek": 403600, "start": 4040.0, "end": 4045.0, "text": " When we do adversarial training we rely on having labels for all the examples.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 524, "seek": 403600, "start": 4045.0, "end": 4047.0, "text": " We have an image that's labeled as a bird.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 525, "seek": 403600, "start": 4047.0, "end": 4051.0, "text": " We make a perturbation that's designed to decrease the probability of the bird class.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 526, "seek": 403600, "start": 4051.0, "end": 4054.0, "text": " And we train the model that the image still be a bird.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 527, "seek": 403600, "start": 4054.0, "end": 4056.0, "text": " But what if you don't have labels?", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 528, "seek": 403600, "start": 4056.0, "end": 4059.0, "text": " It turns out that you can actually train with out labels.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 529, "seek": 403600, "start": 4059.0, "end": 4063.0, "text": " You ask the model to predict the label of the first image.", "tokens": [50364, 407, 754, 365, 17641, 44745, 3097, 321, 434, 920, 588, 1400, 490, 12606, 341, 1154, 13, 50564, 50564, 1133, 321, 360, 17641, 44745, 3097, 321, 10687, 322, 1419, 16949, 337, 439, 264, 5110, 13, 50814, 50814, 492, 362, 364, 3256, 300, 311, 21335, 382, 257, 5255, 13, 50914, 50914, 492, 652, 257, 40468, 399, 300, 311, 4761, 281, 11514, 264, 8482, 295, 264, 5255, 1508, 13, 51114, 51114, 400, 321, 3847, 264, 2316, 300, 264, 3256, 920, 312, 257, 5255, 13, 51264, 51264, 583, 437, 498, 291, 500, 380, 362, 16949, 30, 51364, 51364, 467, 4523, 484, 300, 291, 393, 767, 3847, 365, 484, 16949, 13, 51514, 51514, 509, 1029, 264, 2316, 281, 6069, 264, 7645, 295, 264, 700, 3256, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06621524810791016, "compression_ratio": 1.8036363636363637, "no_speech_prob": 3.702600588439964e-05}, {"id": 530, "seek": 406300, "start": 4063.0, "end": 4067.0, "text": " So if you've trained for a little while in your model isn't perfect yet, it might say, oh, maybe this is a bird.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 531, "seek": 406300, "start": 4067.0, "end": 4068.0, "text": " Maybe it's a plane.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 532, "seek": 406300, "start": 4068.0, "end": 4069.0, "text": " There's some blue sky there.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 533, "seek": 406300, "start": 4069.0, "end": 4071.0, "text": " I'm not sure which of these two classes it is.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 534, "seek": 406300, "start": 4071.0, "end": 4076.0, "text": " Then we make an adversarial perturbation that's intended to change the gas.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 535, "seek": 406300, "start": 4076.0, "end": 4079.0, "text": " And we just try to make it say, ah, this is a truck or something like that.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 536, "seek": 406300, "start": 4079.0, "end": 4081.0, "text": " It's not ever you believe it was before.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 537, "seek": 406300, "start": 4081.0, "end": 4087.0, "text": " You can then train it to say that the distribution of our classes should still be the same as it was before.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 538, "seek": 406300, "start": 4087.0, "end": 4090.0, "text": " But this should still be considered probably a bird or a plane.", "tokens": [50364, 407, 498, 291, 600, 8895, 337, 257, 707, 1339, 294, 428, 2316, 1943, 380, 2176, 1939, 11, 309, 1062, 584, 11, 1954, 11, 1310, 341, 307, 257, 5255, 13, 50564, 50564, 2704, 309, 311, 257, 5720, 13, 50614, 50614, 821, 311, 512, 3344, 5443, 456, 13, 50664, 50664, 286, 478, 406, 988, 597, 295, 613, 732, 5359, 309, 307, 13, 50764, 50764, 1396, 321, 652, 364, 17641, 44745, 40468, 399, 300, 311, 10226, 281, 1319, 264, 4211, 13, 51014, 51014, 400, 321, 445, 853, 281, 652, 309, 584, 11, 3716, 11, 341, 307, 257, 5898, 420, 746, 411, 300, 13, 51164, 51164, 467, 311, 406, 1562, 291, 1697, 309, 390, 949, 13, 51264, 51264, 509, 393, 550, 3847, 309, 281, 584, 300, 264, 7316, 295, 527, 5359, 820, 920, 312, 264, 912, 382, 309, 390, 949, 13, 51564, 51564, 583, 341, 820, 920, 312, 4888, 1391, 257, 5255, 420, 257, 5720, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11051237888825245, "compression_ratio": 1.7341389728096677, "no_speech_prob": 2.2496667952509597e-05}, {"id": 539, "seek": 409000, "start": 4090.0, "end": 4093.0, "text": " This technique is called virtual adversarial training.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 540, "seek": 409000, "start": 4093.0, "end": 4095.0, "text": " It was invented by Takay Rumiato.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 541, "seek": 409000, "start": 4095.0, "end": 4099.0, "text": " He was my intern at Google after he did this work.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 542, "seek": 409000, "start": 4099.0, "end": 4106.0, "text": " At Google, we invited him to come and apply his invention to text classification.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 543, "seek": 409000, "start": 4106.0, "end": 4113.0, "text": " Because this ability to learn from unlabeled examples makes it possible to do semi-supervised learning,", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 544, "seek": 409000, "start": 4113.0, "end": 4116.0, "text": " where you learn from both unlabeled and labeled examples.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 545, "seek": 409000, "start": 4116.0, "end": 4119.0, "text": " And there's quite a lot of unlabeled text in the world.", "tokens": [50364, 639, 6532, 307, 1219, 6374, 17641, 44745, 3097, 13, 50514, 50514, 467, 390, 14479, 538, 9118, 320, 497, 17800, 2513, 13, 50614, 50614, 634, 390, 452, 2154, 412, 3329, 934, 415, 630, 341, 589, 13, 50814, 50814, 1711, 3329, 11, 321, 9185, 796, 281, 808, 293, 3079, 702, 22265, 281, 2487, 21538, 13, 51164, 51164, 1436, 341, 3485, 281, 1466, 490, 32118, 18657, 292, 5110, 1669, 309, 1944, 281, 360, 12909, 12, 48172, 24420, 2539, 11, 51514, 51514, 689, 291, 1466, 490, 1293, 32118, 18657, 292, 293, 21335, 5110, 13, 51664, 51664, 400, 456, 311, 1596, 257, 688, 295, 32118, 18657, 292, 2487, 294, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12901289828188783, "compression_ratio": 1.6692015209125475, "no_speech_prob": 2.005519127123989e-05}, {"id": 546, "seek": 411900, "start": 4119.0, "end": 4128.0, "text": " So we were able to bring down the error rate on several different text classification tasks by using this virtual adversarial training.", "tokens": [50364, 407, 321, 645, 1075, 281, 1565, 760, 264, 6713, 3314, 322, 2940, 819, 2487, 21538, 9608, 538, 1228, 341, 6374, 17641, 44745, 3097, 13, 50814, 50814, 6288, 11, 456, 311, 257, 688, 295, 2740, 689, 321, 576, 411, 281, 764, 18161, 36170, 281, 5934, 19618, 13846, 13, 51114, 51114, 759, 321, 528, 281, 652, 257, 588, 11, 588, 2370, 1032, 11, 321, 393, 3811, 257, 18161, 2533, 300, 1542, 412, 264, 888, 23547, 47523, 337, 257, 1032, 293, 6069, 82, 577, 2370, 309, 486, 352, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07769265174865722, "compression_ratio": 1.5755102040816327, "no_speech_prob": 1.6290712665067986e-05}, {"id": 547, "seek": 411900, "start": 4128.0, "end": 4134.0, "text": " Finally, there's a lot of problems where we would like to use neural nets to guide optimization procedures.", "tokens": [50364, 407, 321, 645, 1075, 281, 1565, 760, 264, 6713, 3314, 322, 2940, 819, 2487, 21538, 9608, 538, 1228, 341, 6374, 17641, 44745, 3097, 13, 50814, 50814, 6288, 11, 456, 311, 257, 688, 295, 2740, 689, 321, 576, 411, 281, 764, 18161, 36170, 281, 5934, 19618, 13846, 13, 51114, 51114, 759, 321, 528, 281, 652, 257, 588, 11, 588, 2370, 1032, 11, 321, 393, 3811, 257, 18161, 2533, 300, 1542, 412, 264, 888, 23547, 47523, 337, 257, 1032, 293, 6069, 82, 577, 2370, 309, 486, 352, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07769265174865722, "compression_ratio": 1.5755102040816327, "no_speech_prob": 1.6290712665067986e-05}, {"id": 548, "seek": 411900, "start": 4134.0, "end": 4143.0, "text": " If we want to make a very, very fast car, we can imagine a neural net that looks at the blueprints for a car and predicts how fast it will go.", "tokens": [50364, 407, 321, 645, 1075, 281, 1565, 760, 264, 6713, 3314, 322, 2940, 819, 2487, 21538, 9608, 538, 1228, 341, 6374, 17641, 44745, 3097, 13, 50814, 50814, 6288, 11, 456, 311, 257, 688, 295, 2740, 689, 321, 576, 411, 281, 764, 18161, 36170, 281, 5934, 19618, 13846, 13, 51114, 51114, 759, 321, 528, 281, 652, 257, 588, 11, 588, 2370, 1032, 11, 321, 393, 3811, 257, 18161, 2533, 300, 1542, 412, 264, 888, 23547, 47523, 337, 257, 1032, 293, 6069, 82, 577, 2370, 309, 486, 352, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07769265174865722, "compression_ratio": 1.5755102040816327, "no_speech_prob": 1.6290712665067986e-05}, {"id": 549, "seek": 414300, "start": 4143.0, "end": 4150.0, "text": " If we could then optimize with respect to the input of the neural net and find the blueprint that it predicts would go the fastest,", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 550, "seek": 414300, "start": 4150.0, "end": 4152.0, "text": " we could build an incredibly fast car.", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 551, "seek": 414300, "start": 4152.0, "end": 4156.0, "text": " Unfortunately, what we get right now is not a blueprint for a fast car.", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 552, "seek": 414300, "start": 4156.0, "end": 4160.0, "text": " We get an adversarial example that the model thinks is going to be very fast.", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 553, "seek": 414300, "start": 4160.0, "end": 4166.0, "text": " If we're able to solve the adversarial example problem, we'll be able to solve this model-based optimization problem.", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 554, "seek": 414300, "start": 4166.0, "end": 4171.0, "text": " I like to call model-based optimization the universal engineering machine.", "tokens": [50364, 759, 321, 727, 550, 19719, 365, 3104, 281, 264, 4846, 295, 264, 18161, 2533, 293, 915, 264, 35868, 300, 309, 6069, 82, 576, 352, 264, 14573, 11, 50714, 50714, 321, 727, 1322, 364, 6252, 2370, 1032, 13, 50814, 50814, 8590, 11, 437, 321, 483, 558, 586, 307, 406, 257, 35868, 337, 257, 2370, 1032, 13, 51014, 51014, 492, 483, 364, 17641, 44745, 1365, 300, 264, 2316, 7309, 307, 516, 281, 312, 588, 2370, 13, 51214, 51214, 759, 321, 434, 1075, 281, 5039, 264, 17641, 44745, 1365, 1154, 11, 321, 603, 312, 1075, 281, 5039, 341, 2316, 12, 6032, 19618, 1154, 13, 51514, 51514, 286, 411, 281, 818, 2316, 12, 6032, 19618, 264, 11455, 7043, 3479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03954919179280599, "compression_ratio": 1.879120879120879, "no_speech_prob": 6.518891041196184e-06}, {"id": 555, "seek": 417100, "start": 4171.0, "end": 4178.0, "text": " If we're able to do model-based optimization, we'll be able to write down a function that describes a thing that doesn't exist yet, but we wish that we had.", "tokens": [50364, 759, 321, 434, 1075, 281, 360, 2316, 12, 6032, 19618, 11, 321, 603, 312, 1075, 281, 2464, 760, 257, 2445, 300, 15626, 257, 551, 300, 1177, 380, 2514, 1939, 11, 457, 321, 3172, 300, 321, 632, 13, 50714, 50714, 400, 550, 16235, 23475, 293, 18161, 36170, 486, 2573, 484, 577, 281, 1322, 309, 337, 505, 13, 50914, 50914, 492, 393, 764, 300, 281, 1715, 777, 14424, 293, 777, 13093, 490, 4497, 7766, 13, 51114, 51114, 400, 777, 26354, 281, 652, 18407, 82, 1190, 4663, 293, 721, 411, 300, 13, 51414, 51414, 407, 286, 519, 4787, 12606, 341, 1154, 727, 11634, 257, 688, 295, 3995, 18439, 25297, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07841148032798423, "compression_ratio": 1.6067796610169491, "no_speech_prob": 2.625779052323196e-05}, {"id": 556, "seek": 417100, "start": 4178.0, "end": 4182.0, "text": " And then gradient descent and neural nets will figure out how to build it for us.", "tokens": [50364, 759, 321, 434, 1075, 281, 360, 2316, 12, 6032, 19618, 11, 321, 603, 312, 1075, 281, 2464, 760, 257, 2445, 300, 15626, 257, 551, 300, 1177, 380, 2514, 1939, 11, 457, 321, 3172, 300, 321, 632, 13, 50714, 50714, 400, 550, 16235, 23475, 293, 18161, 36170, 486, 2573, 484, 577, 281, 1322, 309, 337, 505, 13, 50914, 50914, 492, 393, 764, 300, 281, 1715, 777, 14424, 293, 777, 13093, 490, 4497, 7766, 13, 51114, 51114, 400, 777, 26354, 281, 652, 18407, 82, 1190, 4663, 293, 721, 411, 300, 13, 51414, 51414, 407, 286, 519, 4787, 12606, 341, 1154, 727, 11634, 257, 688, 295, 3995, 18439, 25297, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07841148032798423, "compression_ratio": 1.6067796610169491, "no_speech_prob": 2.625779052323196e-05}, {"id": 557, "seek": 417100, "start": 4182.0, "end": 4186.0, "text": " We can use that to design new genes and new molecules from additional drugs.", "tokens": [50364, 759, 321, 434, 1075, 281, 360, 2316, 12, 6032, 19618, 11, 321, 603, 312, 1075, 281, 2464, 760, 257, 2445, 300, 15626, 257, 551, 300, 1177, 380, 2514, 1939, 11, 457, 321, 3172, 300, 321, 632, 13, 50714, 50714, 400, 550, 16235, 23475, 293, 18161, 36170, 486, 2573, 484, 577, 281, 1322, 309, 337, 505, 13, 50914, 50914, 492, 393, 764, 300, 281, 1715, 777, 14424, 293, 777, 13093, 490, 4497, 7766, 13, 51114, 51114, 400, 777, 26354, 281, 652, 18407, 82, 1190, 4663, 293, 721, 411, 300, 13, 51414, 51414, 407, 286, 519, 4787, 12606, 341, 1154, 727, 11634, 257, 688, 295, 3995, 18439, 25297, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07841148032798423, "compression_ratio": 1.6067796610169491, "no_speech_prob": 2.625779052323196e-05}, {"id": 558, "seek": 417100, "start": 4186.0, "end": 4192.0, "text": " And new circuits to make GPUs run faster and things like that.", "tokens": [50364, 759, 321, 434, 1075, 281, 360, 2316, 12, 6032, 19618, 11, 321, 603, 312, 1075, 281, 2464, 760, 257, 2445, 300, 15626, 257, 551, 300, 1177, 380, 2514, 1939, 11, 457, 321, 3172, 300, 321, 632, 13, 50714, 50714, 400, 550, 16235, 23475, 293, 18161, 36170, 486, 2573, 484, 577, 281, 1322, 309, 337, 505, 13, 50914, 50914, 492, 393, 764, 300, 281, 1715, 777, 14424, 293, 777, 13093, 490, 4497, 7766, 13, 51114, 51114, 400, 777, 26354, 281, 652, 18407, 82, 1190, 4663, 293, 721, 411, 300, 13, 51414, 51414, 407, 286, 519, 4787, 12606, 341, 1154, 727, 11634, 257, 688, 295, 3995, 18439, 25297, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07841148032798423, "compression_ratio": 1.6067796610169491, "no_speech_prob": 2.625779052323196e-05}, {"id": 559, "seek": 417100, "start": 4192.0, "end": 4198.0, "text": " So I think overall solving this problem could unlock a lot of potential technological advances.", "tokens": [50364, 759, 321, 434, 1075, 281, 360, 2316, 12, 6032, 19618, 11, 321, 603, 312, 1075, 281, 2464, 760, 257, 2445, 300, 15626, 257, 551, 300, 1177, 380, 2514, 1939, 11, 457, 321, 3172, 300, 321, 632, 13, 50714, 50714, 400, 550, 16235, 23475, 293, 18161, 36170, 486, 2573, 484, 577, 281, 1322, 309, 337, 505, 13, 50914, 50914, 492, 393, 764, 300, 281, 1715, 777, 14424, 293, 777, 13093, 490, 4497, 7766, 13, 51114, 51114, 400, 777, 26354, 281, 652, 18407, 82, 1190, 4663, 293, 721, 411, 300, 13, 51414, 51414, 407, 286, 519, 4787, 12606, 341, 1154, 727, 11634, 257, 688, 295, 3995, 18439, 25297, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07841148032798423, "compression_ratio": 1.6067796610169491, "no_speech_prob": 2.625779052323196e-05}, {"id": 560, "seek": 419800, "start": 4198.0, "end": 4205.0, "text": " In conclusion, attacking machine learning models is extremely easy, and defending them is extremely difficult.", "tokens": [50364, 682, 10063, 11, 15010, 3479, 2539, 5245, 307, 4664, 1858, 11, 293, 21377, 552, 307, 4664, 2252, 13, 50714, 50714, 759, 291, 764, 17641, 44745, 3097, 11, 291, 393, 483, 257, 707, 857, 295, 257, 7654, 11, 457, 456, 311, 920, 867, 11730, 1720, 6615, 365, 300, 7654, 13, 51064, 51064, 1999, 840, 44745, 3097, 293, 6374, 17641, 44745, 3097, 611, 652, 309, 1944, 281, 3890, 1125, 428, 2316, 293, 754, 1466, 490, 32118, 18657, 292, 1412, 13, 51414, 51414, 407, 291, 393, 360, 1101, 322, 3890, 1500, 5110, 11, 754, 498, 291, 434, 406, 5922, 466, 7170, 364, 48222, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06608220509120397, "compression_ratio": 1.788888888888889, "no_speech_prob": 1.6400768799940124e-05}, {"id": 561, "seek": 419800, "start": 4205.0, "end": 4212.0, "text": " If you use adversarial training, you can get a little bit of a defense, but there's still many caveats associated with that defense.", "tokens": [50364, 682, 10063, 11, 15010, 3479, 2539, 5245, 307, 4664, 1858, 11, 293, 21377, 552, 307, 4664, 2252, 13, 50714, 50714, 759, 291, 764, 17641, 44745, 3097, 11, 291, 393, 483, 257, 707, 857, 295, 257, 7654, 11, 457, 456, 311, 920, 867, 11730, 1720, 6615, 365, 300, 7654, 13, 51064, 51064, 1999, 840, 44745, 3097, 293, 6374, 17641, 44745, 3097, 611, 652, 309, 1944, 281, 3890, 1125, 428, 2316, 293, 754, 1466, 490, 32118, 18657, 292, 1412, 13, 51414, 51414, 407, 291, 393, 360, 1101, 322, 3890, 1500, 5110, 11, 754, 498, 291, 434, 406, 5922, 466, 7170, 364, 48222, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06608220509120397, "compression_ratio": 1.788888888888889, "no_speech_prob": 1.6400768799940124e-05}, {"id": 562, "seek": 419800, "start": 4212.0, "end": 4219.0, "text": " Adversarial training and virtual adversarial training also make it possible to regularize your model and even learn from unlabeled data.", "tokens": [50364, 682, 10063, 11, 15010, 3479, 2539, 5245, 307, 4664, 1858, 11, 293, 21377, 552, 307, 4664, 2252, 13, 50714, 50714, 759, 291, 764, 17641, 44745, 3097, 11, 291, 393, 483, 257, 707, 857, 295, 257, 7654, 11, 457, 456, 311, 920, 867, 11730, 1720, 6615, 365, 300, 7654, 13, 51064, 51064, 1999, 840, 44745, 3097, 293, 6374, 17641, 44745, 3097, 611, 652, 309, 1944, 281, 3890, 1125, 428, 2316, 293, 754, 1466, 490, 32118, 18657, 292, 1412, 13, 51414, 51414, 407, 291, 393, 360, 1101, 322, 3890, 1500, 5110, 11, 754, 498, 291, 434, 406, 5922, 466, 7170, 364, 48222, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06608220509120397, "compression_ratio": 1.788888888888889, "no_speech_prob": 1.6400768799940124e-05}, {"id": 563, "seek": 419800, "start": 4219.0, "end": 4224.0, "text": " So you can do better on regular test examples, even if you're not concerned about facing an adversary.", "tokens": [50364, 682, 10063, 11, 15010, 3479, 2539, 5245, 307, 4664, 1858, 11, 293, 21377, 552, 307, 4664, 2252, 13, 50714, 50714, 759, 291, 764, 17641, 44745, 3097, 11, 291, 393, 483, 257, 707, 857, 295, 257, 7654, 11, 457, 456, 311, 920, 867, 11730, 1720, 6615, 365, 300, 7654, 13, 51064, 51064, 1999, 840, 44745, 3097, 293, 6374, 17641, 44745, 3097, 611, 652, 309, 1944, 281, 3890, 1125, 428, 2316, 293, 754, 1466, 490, 32118, 18657, 292, 1412, 13, 51414, 51414, 407, 291, 393, 360, 1101, 322, 3890, 1500, 5110, 11, 754, 498, 291, 434, 406, 5922, 466, 7170, 364, 48222, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06608220509120397, "compression_ratio": 1.788888888888889, "no_speech_prob": 1.6400768799940124e-05}, {"id": 564, "seek": 422400, "start": 4224.0, "end": 4236.0, "text": " And finally, if we're able to solve all of these problems, we'll be able to build a black box model-based optimization system that can solve all kinds of engineering problems that are holding us back in many different fields.", "tokens": [50364, 400, 2721, 11, 498, 321, 434, 1075, 281, 5039, 439, 295, 613, 2740, 11, 321, 603, 312, 1075, 281, 1322, 257, 2211, 2424, 2316, 12, 6032, 19618, 1185, 300, 393, 5039, 439, 3685, 295, 7043, 2740, 300, 366, 5061, 505, 646, 294, 867, 819, 7909, 13, 50964, 50964, 286, 519, 286, 362, 257, 1326, 2077, 1411, 337, 1651, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.056905564807710196, "compression_ratio": 1.5054945054945055, "no_speech_prob": 5.671951385011198e-06}, {"id": 565, "seek": 422400, "start": 4236.0, "end": 4240.0, "text": " I think I have a few minutes left for questions.", "tokens": [50364, 400, 2721, 11, 498, 321, 434, 1075, 281, 5039, 439, 295, 613, 2740, 11, 321, 603, 312, 1075, 281, 1322, 257, 2211, 2424, 2316, 12, 6032, 19618, 1185, 300, 393, 5039, 439, 3685, 295, 7043, 2740, 300, 366, 5061, 505, 646, 294, 867, 819, 7909, 13, 50964, 50964, 286, 519, 286, 362, 257, 1326, 2077, 1411, 337, 1651, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.056905564807710196, "compression_ratio": 1.5054945054945055, "no_speech_prob": 5.671951385011198e-06}, {"id": 566, "seek": 424000, "start": 4240.0, "end": 4256.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.46496299902598065, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0014760061167180538}, {"id": 567, "seek": 427000, "start": 4271.0, "end": 4282.0, "text": " Oh, so there's some determinism to the choice of those 50 directions.", "tokens": [50414, 876, 11, 370, 456, 311, 512, 15957, 1434, 281, 264, 3922, 295, 729, 2625, 11095, 13, 50964, 50964, 876, 11, 558, 13, 865, 13, 407, 18617, 264, 1651, 13, 51114, 51114, 286, 600, 848, 300, 264, 912, 40468, 399, 393, 7979, 867, 819, 5245, 420, 264, 912, 40468, 399, 393, 312, 6456, 281, 867, 819, 2541, 5110, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1449748162300356, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.3285787105560303}, {"id": 568, "seek": 427000, "start": 4282.0, "end": 4285.0, "text": " Oh, right. Yeah. So repeating the questions.", "tokens": [50414, 876, 11, 370, 456, 311, 512, 15957, 1434, 281, 264, 3922, 295, 729, 2625, 11095, 13, 50964, 50964, 876, 11, 558, 13, 865, 13, 407, 18617, 264, 1651, 13, 51114, 51114, 286, 600, 848, 300, 264, 912, 40468, 399, 393, 7979, 867, 819, 5245, 420, 264, 912, 40468, 399, 393, 312, 6456, 281, 867, 819, 2541, 5110, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1449748162300356, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.3285787105560303}, {"id": 569, "seek": 427000, "start": 4285.0, "end": 4292.0, "text": " I've said that the same perturbation can fool many different models or the same perturbation can be applied to many different clean examples.", "tokens": [50414, 876, 11, 370, 456, 311, 512, 15957, 1434, 281, 264, 3922, 295, 729, 2625, 11095, 13, 50964, 50964, 876, 11, 558, 13, 865, 13, 407, 18617, 264, 1651, 13, 51114, 51114, 286, 600, 848, 300, 264, 912, 40468, 399, 393, 7979, 867, 819, 5245, 420, 264, 912, 40468, 399, 393, 312, 6456, 281, 867, 819, 2541, 5110, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1449748162300356, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.3285787105560303}, {"id": 570, "seek": 429200, "start": 4292.0, "end": 4301.0, "text": " I've also said that the subspace of adversarial perturbations is only about 50 dimensional, even if the input dimension is 3,000 dimensional.", "tokens": [50364, 286, 600, 611, 848, 300, 264, 2090, 17940, 295, 17641, 44745, 40468, 763, 307, 787, 466, 2625, 18795, 11, 754, 498, 264, 4846, 10139, 307, 805, 11, 1360, 18795, 13, 50814, 50814, 407, 577, 307, 309, 300, 613, 2090, 79, 2116, 27815, 30, 50964, 50964, 440, 1778, 307, 300, 264, 3922, 295, 264, 2090, 17940, 11095, 307, 406, 2584, 4974, 13, 51264, 51264, 467, 311, 5101, 516, 281, 312, 746, 411, 12166, 490, 472, 1508, 1489, 6490, 281, 1071, 1508, 1489, 6490, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06835839940213609, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.189951399283018e-05}, {"id": 571, "seek": 429200, "start": 4301.0, "end": 4304.0, "text": " So how is it that these subspaces intersect?", "tokens": [50364, 286, 600, 611, 848, 300, 264, 2090, 17940, 295, 17641, 44745, 40468, 763, 307, 787, 466, 2625, 18795, 11, 754, 498, 264, 4846, 10139, 307, 805, 11, 1360, 18795, 13, 50814, 50814, 407, 577, 307, 309, 300, 613, 2090, 79, 2116, 27815, 30, 50964, 50964, 440, 1778, 307, 300, 264, 3922, 295, 264, 2090, 17940, 11095, 307, 406, 2584, 4974, 13, 51264, 51264, 467, 311, 5101, 516, 281, 312, 746, 411, 12166, 490, 472, 1508, 1489, 6490, 281, 1071, 1508, 1489, 6490, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06835839940213609, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.189951399283018e-05}, {"id": 572, "seek": 429200, "start": 4304.0, "end": 4310.0, "text": " The reason is that the choice of the subspace directions is not completely random.", "tokens": [50364, 286, 600, 611, 848, 300, 264, 2090, 17940, 295, 17641, 44745, 40468, 763, 307, 787, 466, 2625, 18795, 11, 754, 498, 264, 4846, 10139, 307, 805, 11, 1360, 18795, 13, 50814, 50814, 407, 577, 307, 309, 300, 613, 2090, 79, 2116, 27815, 30, 50964, 50964, 440, 1778, 307, 300, 264, 3922, 295, 264, 2090, 17940, 11095, 307, 406, 2584, 4974, 13, 51264, 51264, 467, 311, 5101, 516, 281, 312, 746, 411, 12166, 490, 472, 1508, 1489, 6490, 281, 1071, 1508, 1489, 6490, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06835839940213609, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.189951399283018e-05}, {"id": 573, "seek": 429200, "start": 4310.0, "end": 4316.0, "text": " It's generally going to be something like pointing from one class centroid to another class centroid.", "tokens": [50364, 286, 600, 611, 848, 300, 264, 2090, 17940, 295, 17641, 44745, 40468, 763, 307, 787, 466, 2625, 18795, 11, 754, 498, 264, 4846, 10139, 307, 805, 11, 1360, 18795, 13, 50814, 50814, 407, 577, 307, 309, 300, 613, 2090, 79, 2116, 27815, 30, 50964, 50964, 440, 1778, 307, 300, 264, 3922, 295, 264, 2090, 17940, 11095, 307, 406, 2584, 4974, 13, 51264, 51264, 467, 311, 5101, 516, 281, 312, 746, 411, 12166, 490, 472, 1508, 1489, 6490, 281, 1071, 1508, 1489, 6490, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06835839940213609, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.189951399283018e-05}, {"id": 574, "seek": 431600, "start": 4316.0, "end": 4327.0, "text": " And if you look at that vector and visualize it as an image, it might not be meaningful to a human, just because humans aren't very good at imagining what class centroid looks like.", "tokens": [50364, 400, 498, 291, 574, 412, 300, 8062, 293, 23273, 309, 382, 364, 3256, 11, 309, 1062, 406, 312, 10995, 281, 257, 1952, 11, 445, 570, 6255, 3212, 380, 588, 665, 412, 27798, 437, 1508, 1489, 6490, 1542, 411, 13, 50914, 50914, 400, 321, 434, 534, 1578, 412, 27798, 7300, 1296, 1489, 6490, 13, 51064, 51064, 583, 456, 307, 544, 420, 1570, 264, 27249, 1802, 300, 7700, 819, 5245, 281, 1466, 2531, 8213, 6828, 11, 445, 570, 436, 434, 1382, 281, 5039, 264, 912, 5633, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10731453245336359, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.4329555091971997e-06}, {"id": 575, "seek": 431600, "start": 4327.0, "end": 4330.0, "text": " And we're really bad at imagining differences between centroid.", "tokens": [50364, 400, 498, 291, 574, 412, 300, 8062, 293, 23273, 309, 382, 364, 3256, 11, 309, 1062, 406, 312, 10995, 281, 257, 1952, 11, 445, 570, 6255, 3212, 380, 588, 665, 412, 27798, 437, 1508, 1489, 6490, 1542, 411, 13, 50914, 50914, 400, 321, 434, 534, 1578, 412, 27798, 7300, 1296, 1489, 6490, 13, 51064, 51064, 583, 456, 307, 544, 420, 1570, 264, 27249, 1802, 300, 7700, 819, 5245, 281, 1466, 2531, 8213, 6828, 11, 445, 570, 436, 434, 1382, 281, 5039, 264, 912, 5633, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10731453245336359, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.4329555091971997e-06}, {"id": 576, "seek": 433000, "start": 4330.0, "end": 4348.0, "text": " But there is more or less this systematic effect that causes different models to learn similar linear functions, just because they're trying to solve the same task.", "tokens": [50364, 583, 456, 307, 544, 420, 1570, 341, 27249, 1802, 300, 7700, 819, 5245, 281, 1466, 2531, 8213, 6828, 11, 445, 570, 436, 434, 1382, 281, 5039, 264, 912, 5633, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07501009632559384, "compression_ratio": 1.3553719008264462, "no_speech_prob": 1.4040008863958064e-05}, {"id": 577, "seek": 434800, "start": 4348.0, "end": 4363.0, "text": " So the question is, is it possible to identify which layer contributes the most to this issue?", "tokens": [50364, 407, 264, 1168, 307, 11, 307, 309, 1944, 281, 5876, 597, 4583, 32035, 264, 881, 281, 341, 2734, 30, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.18538346497908884, "compression_ratio": 1.1058823529411765, "no_speech_prob": 4.067599002155475e-05}, {"id": 578, "seek": 436300, "start": 4363.0, "end": 4381.0, "text": " It's a very abstracted that's completely robust to adversarial perturbations and can shrink them to be very, very small. And then the last layer is still linear.", "tokens": [50364, 467, 311, 257, 588, 12649, 292, 300, 311, 2584, 13956, 281, 17641, 44745, 40468, 763, 293, 393, 23060, 552, 281, 312, 588, 11, 588, 1359, 13, 400, 550, 264, 1036, 4583, 307, 920, 8213, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.420770987486228, "compression_ratio": 1.319672131147541, "no_speech_prob": 0.0001624013966647908}, {"id": 579, "seek": 438100, "start": 4381.0, "end": 4394.0, "text": " And then you perturb all of the different layers, all the hidden layers as well as the input. In this lecture, I only described perturbing the input because it seems like that's where most of the benefit comes from.", "tokens": [50364, 400, 550, 291, 40468, 439, 295, 264, 819, 7914, 11, 439, 264, 7633, 7914, 382, 731, 382, 264, 4846, 13, 682, 341, 7991, 11, 286, 787, 7619, 40468, 278, 264, 4846, 570, 309, 2544, 411, 300, 311, 689, 881, 295, 264, 5121, 1487, 490, 13, 51014, 51014, 440, 472, 551, 300, 291, 393, 380, 360, 365, 17641, 44745, 3097, 307, 40468, 264, 588, 1036, 4583, 949, 264, 2787, 41167, 570, 300, 8213, 4583, 412, 264, 917, 575, 572, 636, 295, 2539, 281, 4597, 264, 40468, 763, 13, 51314, 51314, 400, 884, 17641, 44745, 3097, 412, 300, 4583, 2673, 445, 9857, 264, 1379, 1399, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17299413681030273, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.0001313737011514604}, {"id": 580, "seek": 438100, "start": 4394.0, "end": 4400.0, "text": " The one thing that you can't do with adversarial training is perturb the very last layer before the softmax because that linear layer at the end has no way of learning to resist the perturbations.", "tokens": [50364, 400, 550, 291, 40468, 439, 295, 264, 819, 7914, 11, 439, 264, 7633, 7914, 382, 731, 382, 264, 4846, 13, 682, 341, 7991, 11, 286, 787, 7619, 40468, 278, 264, 4846, 570, 309, 2544, 411, 300, 311, 689, 881, 295, 264, 5121, 1487, 490, 13, 51014, 51014, 440, 472, 551, 300, 291, 393, 380, 360, 365, 17641, 44745, 3097, 307, 40468, 264, 588, 1036, 4583, 949, 264, 2787, 41167, 570, 300, 8213, 4583, 412, 264, 917, 575, 572, 636, 295, 2539, 281, 4597, 264, 40468, 763, 13, 51314, 51314, 400, 884, 17641, 44745, 3097, 412, 300, 4583, 2673, 445, 9857, 264, 1379, 1399, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17299413681030273, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.0001313737011514604}, {"id": 581, "seek": 438100, "start": 4400.0, "end": 4405.0, "text": " And doing adversarial training at that layer usually just breaks the whole process.", "tokens": [50364, 400, 550, 291, 40468, 439, 295, 264, 819, 7914, 11, 439, 264, 7633, 7914, 382, 731, 382, 264, 4846, 13, 682, 341, 7991, 11, 286, 787, 7619, 40468, 278, 264, 4846, 570, 309, 2544, 411, 300, 311, 689, 881, 295, 264, 5121, 1487, 490, 13, 51014, 51014, 440, 472, 551, 300, 291, 393, 380, 360, 365, 17641, 44745, 3097, 307, 40468, 264, 588, 1036, 4583, 949, 264, 2787, 41167, 570, 300, 8213, 4583, 412, 264, 917, 575, 572, 636, 295, 2539, 281, 4597, 264, 40468, 763, 13, 51314, 51314, 400, 884, 17641, 44745, 3097, 412, 300, 4583, 2673, 445, 9857, 264, 1379, 1399, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17299413681030273, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.0001313737011514604}, {"id": 582, "seek": 440500, "start": 4405.0, "end": 4421.0, "text": " Given that it seems very problem dependent. There's a paper by Sara Sabore and her collaborators called adversarial manipulation of deep representations, where they design adversarial examples that are intended to fool different layers of the net.", "tokens": [50364, 18600, 300, 309, 2544, 588, 1154, 12334, 13, 821, 311, 257, 3035, 538, 18694, 13915, 418, 293, 720, 39789, 1219, 17641, 44745, 26475, 295, 2452, 33358, 11, 689, 436, 1715, 17641, 44745, 5110, 300, 366, 10226, 281, 7979, 819, 7914, 295, 264, 2533, 13, 51164, 51164, 400, 436, 2275, 512, 721, 466, 411, 577, 2416, 295, 257, 40468, 399, 307, 2978, 412, 264, 917, 935, 281, 483, 819, 11602, 295, 40468, 399, 412, 819, 7633, 7914, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14564723733030718, "compression_ratio": 1.6929460580912863, "no_speech_prob": 2.432661858620122e-05}, {"id": 583, "seek": 440500, "start": 4421.0, "end": 4430.0, "text": " And they report some things about like how large of a perturbation is needed at the end point to get different sizes of perturbation at different hidden layers.", "tokens": [50364, 18600, 300, 309, 2544, 588, 1154, 12334, 13, 821, 311, 257, 3035, 538, 18694, 13915, 418, 293, 720, 39789, 1219, 17641, 44745, 26475, 295, 2452, 33358, 11, 689, 436, 1715, 17641, 44745, 5110, 300, 366, 10226, 281, 7979, 819, 7914, 295, 264, 2533, 13, 51164, 51164, 400, 436, 2275, 512, 721, 466, 411, 577, 2416, 295, 257, 40468, 399, 307, 2978, 412, 264, 917, 935, 281, 483, 819, 11602, 295, 40468, 399, 412, 819, 7633, 7914, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14564723733030718, "compression_ratio": 1.6929460580912863, "no_speech_prob": 2.432661858620122e-05}, {"id": 584, "seek": 443000, "start": 4430.0, "end": 4450.0, "text": " And just spec that if you trained the model to resist perturbations at one layer, then another layer would become more vulnerable. I would be like a moving target.", "tokens": [50364, 400, 445, 1608, 300, 498, 291, 8895, 264, 2316, 281, 4597, 40468, 763, 412, 472, 4583, 11, 550, 1071, 4583, 576, 1813, 544, 10955, 13, 286, 576, 312, 411, 257, 2684, 3779, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.26878720360833247, "compression_ratio": 1.3360655737704918, "no_speech_prob": 7.2583115979796275e-06}, {"id": 585, "seek": 445000, "start": 4450.0, "end": 4461.0, "text": " So the question is how many adversarial examples are needed to improve the misclassification rate? Some of our plots include learning curves.", "tokens": [50364, 407, 264, 1168, 307, 577, 867, 17641, 44745, 5110, 366, 2978, 281, 3470, 264, 3346, 11665, 3774, 3314, 30, 2188, 295, 527, 28609, 4090, 2539, 19490, 13, 50914, 50914, 2188, 295, 527, 10577, 4090, 2539, 19490, 13, 407, 291, 393, 767, 536, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.19965086592004655, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.298525062855333e-05}, {"id": 586, "seek": 445000, "start": 4461.0, "end": 4466.0, "text": " Some of our papers include learning curves. So you can actually see.", "tokens": [50364, 407, 264, 1168, 307, 577, 867, 17641, 44745, 5110, 366, 2978, 281, 3470, 264, 3346, 11665, 3774, 3314, 30, 2188, 295, 527, 28609, 4090, 2539, 19490, 13, 50914, 50914, 2188, 295, 527, 10577, 4090, 2539, 19490, 13, 407, 291, 393, 767, 536, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.19965086592004655, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.298525062855333e-05}, {"id": 587, "seek": 446600, "start": 4466.0, "end": 4479.0, "text": " This one here every time we do an epoch we've generated the same number of adversarial examples as there are training examples. So every epoch here is 50,000 adversarial examples.", "tokens": [50364, 639, 472, 510, 633, 565, 321, 360, 364, 30992, 339, 321, 600, 10833, 264, 912, 1230, 295, 17641, 44745, 5110, 382, 456, 366, 3097, 5110, 13, 407, 633, 30992, 339, 510, 307, 2625, 11, 1360, 17641, 44745, 5110, 13, 51014, 51014, 509, 393, 536, 300, 17641, 44745, 3097, 307, 257, 588, 1412, 8067, 1399, 300, 291, 643, 281, 652, 777, 17641, 44745, 5110, 633, 565, 291, 5623, 264, 17443, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13264756589322477, "compression_ratio": 1.7688172043010753, "no_speech_prob": 2.554032789703342e-06}, {"id": 588, "seek": 446600, "start": 4479.0, "end": 4489.0, "text": " You can see that adversarial training is a very data hungry process that you need to make new adversarial examples every time you update the weights.", "tokens": [50364, 639, 472, 510, 633, 565, 321, 360, 364, 30992, 339, 321, 600, 10833, 264, 912, 1230, 295, 17641, 44745, 5110, 382, 456, 366, 3097, 5110, 13, 407, 633, 30992, 339, 510, 307, 2625, 11, 1360, 17641, 44745, 5110, 13, 51014, 51014, 509, 393, 536, 300, 17641, 44745, 3097, 307, 257, 588, 1412, 8067, 1399, 300, 291, 643, 281, 652, 777, 17641, 44745, 5110, 633, 565, 291, 5623, 264, 17443, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13264756589322477, "compression_ratio": 1.7688172043010753, "no_speech_prob": 2.554032789703342e-06}, {"id": 589, "seek": 448900, "start": 4489.0, "end": 4508.0, "text": " And they're constantly changing in reaction to whatever the model has learned most recently.", "tokens": [50364, 400, 436, 434, 6460, 4473, 294, 5480, 281, 2035, 264, 2316, 575, 3264, 881, 3938, 13, 51314, 51314, 876, 11, 264, 2316, 2361, 19618, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.27546428811961204, "compression_ratio": 1.2115384615384615, "no_speech_prob": 6.966487126192078e-05}, {"id": 590, "seek": 448900, "start": 4508.0, "end": 4513.0, "text": " Oh, the model based optimization.", "tokens": [50364, 400, 436, 434, 6460, 4473, 294, 5480, 281, 2035, 264, 2316, 575, 3264, 881, 3938, 13, 51314, 51314, 876, 11, 264, 2316, 2361, 19618, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.27546428811961204, "compression_ratio": 1.2115384615384615, "no_speech_prob": 6.966487126192078e-05}, {"id": 591, "seek": 451300, "start": 4513.0, "end": 4530.0, "text": " The question is just to elaborate further on this problem. So most of the time that we have a machine learning model, it's something like a classifier or a regression model, where we give it an input from the test set and it gives us an output.", "tokens": [50364, 440, 1168, 307, 445, 281, 20945, 3052, 322, 341, 1154, 13, 407, 881, 295, 264, 565, 300, 321, 362, 257, 3479, 2539, 2316, 11, 309, 311, 746, 411, 257, 1508, 9902, 420, 257, 24590, 2316, 11, 689, 321, 976, 309, 364, 4846, 490, 264, 1500, 992, 293, 309, 2709, 505, 364, 5598, 13, 51214, 51214, 400, 2673, 300, 4846, 307, 16979, 18386, 293, 1487, 490, 264, 912, 7316, 382, 264, 3097, 992, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09763082281335608, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00010253390792058781}, {"id": 592, "seek": 451300, "start": 4530.0, "end": 4536.0, "text": " And usually that input is randomly occurring and comes from the same distribution as the training set.", "tokens": [50364, 440, 1168, 307, 445, 281, 20945, 3052, 322, 341, 1154, 13, 407, 881, 295, 264, 565, 300, 321, 362, 257, 3479, 2539, 2316, 11, 309, 311, 746, 411, 257, 1508, 9902, 420, 257, 24590, 2316, 11, 689, 321, 976, 309, 364, 4846, 490, 264, 1500, 992, 293, 309, 2709, 505, 364, 5598, 13, 51214, 51214, 400, 2673, 300, 4846, 307, 16979, 18386, 293, 1487, 490, 264, 912, 7316, 382, 264, 3097, 992, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09763082281335608, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00010253390792058781}, {"id": 593, "seek": 453600, "start": 4536.0, "end": 4546.0, "text": " And we usually just run the model, get its prediction and then we're done with it. Sometimes we have feedback loops like for recommender systems.", "tokens": [50364, 400, 321, 2673, 445, 1190, 264, 2316, 11, 483, 1080, 17630, 293, 550, 321, 434, 1096, 365, 309, 13, 4803, 321, 362, 5824, 16121, 411, 337, 2748, 260, 3652, 13, 50864, 50864, 759, 291, 589, 412, 12778, 293, 291, 2748, 257, 3169, 281, 257, 16767, 11, 550, 436, 434, 544, 3700, 281, 1159, 300, 3169, 293, 550, 3314, 309, 13, 51264, 51264, 400, 550, 456, 311, 516, 281, 312, 544, 24603, 295, 309, 294, 428, 3097, 992, 13, 407, 291, 603, 2748, 309, 281, 544, 561, 294, 264, 2027, 13, 407, 341, 5824, 6367, 490, 264, 5598, 295, 428, 2316, 281, 264, 4846, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11024728527775517, "compression_ratio": 1.7320754716981133, "no_speech_prob": 5.608656647382304e-05}, {"id": 594, "seek": 453600, "start": 4546.0, "end": 4554.0, "text": " If you work at Netflix and you recommend a movie to a viewer, then they're more likely to watch that movie and then rate it.", "tokens": [50364, 400, 321, 2673, 445, 1190, 264, 2316, 11, 483, 1080, 17630, 293, 550, 321, 434, 1096, 365, 309, 13, 4803, 321, 362, 5824, 16121, 411, 337, 2748, 260, 3652, 13, 50864, 50864, 759, 291, 589, 412, 12778, 293, 291, 2748, 257, 3169, 281, 257, 16767, 11, 550, 436, 434, 544, 3700, 281, 1159, 300, 3169, 293, 550, 3314, 309, 13, 51264, 51264, 400, 550, 456, 311, 516, 281, 312, 544, 24603, 295, 309, 294, 428, 3097, 992, 13, 407, 291, 603, 2748, 309, 281, 544, 561, 294, 264, 2027, 13, 407, 341, 5824, 6367, 490, 264, 5598, 295, 428, 2316, 281, 264, 4846, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11024728527775517, "compression_ratio": 1.7320754716981133, "no_speech_prob": 5.608656647382304e-05}, {"id": 595, "seek": 453600, "start": 4554.0, "end": 4561.0, "text": " And then there's going to be more ratings of it in your training set. So you'll recommend it to more people in the future. So this feedback loop from the output of your model to the input.", "tokens": [50364, 400, 321, 2673, 445, 1190, 264, 2316, 11, 483, 1080, 17630, 293, 550, 321, 434, 1096, 365, 309, 13, 4803, 321, 362, 5824, 16121, 411, 337, 2748, 260, 3652, 13, 50864, 50864, 759, 291, 589, 412, 12778, 293, 291, 2748, 257, 3169, 281, 257, 16767, 11, 550, 436, 434, 544, 3700, 281, 1159, 300, 3169, 293, 550, 3314, 309, 13, 51264, 51264, 400, 550, 456, 311, 516, 281, 312, 544, 24603, 295, 309, 294, 428, 3097, 992, 13, 407, 291, 603, 2748, 309, 281, 544, 561, 294, 264, 2027, 13, 407, 341, 5824, 6367, 490, 264, 5598, 295, 428, 2316, 281, 264, 4846, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11024728527775517, "compression_ratio": 1.7320754716981133, "no_speech_prob": 5.608656647382304e-05}, {"id": 596, "seek": 456100, "start": 4561.0, "end": 4569.0, "text": " Most of the time when we build machine vision systems, there's no feedback loop from their output to their input.", "tokens": [50364, 4534, 295, 264, 565, 562, 321, 1322, 3479, 5201, 3652, 11, 456, 311, 572, 5824, 6367, 490, 641, 5598, 281, 641, 4846, 13, 50764, 50764, 759, 321, 3811, 257, 3287, 689, 321, 722, 1228, 364, 19618, 9284, 281, 915, 15743, 300, 19874, 512, 4707, 295, 264, 5598, 13, 51214, 51214, 1743, 498, 321, 362, 257, 2316, 300, 1542, 412, 264, 888, 23547, 47523, 295, 257, 1032, 293, 23930, 264, 5176, 3073, 295, 264, 1032, 11, 550, 321, 727, 764, 16235, 382, 2207, 281, 574, 337, 264, 888, 23547, 47523, 300, 6805, 281, 264, 14573, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10001166179926708, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9477134628687054e-05}, {"id": 597, "seek": 456100, "start": 4569.0, "end": 4578.0, "text": " If we imagine a setting where we start using an optimization algorithm to find inputs that maximize some property of the output.", "tokens": [50364, 4534, 295, 264, 565, 562, 321, 1322, 3479, 5201, 3652, 11, 456, 311, 572, 5824, 6367, 490, 641, 5598, 281, 641, 4846, 13, 50764, 50764, 759, 321, 3811, 257, 3287, 689, 321, 722, 1228, 364, 19618, 9284, 281, 915, 15743, 300, 19874, 512, 4707, 295, 264, 5598, 13, 51214, 51214, 1743, 498, 321, 362, 257, 2316, 300, 1542, 412, 264, 888, 23547, 47523, 295, 257, 1032, 293, 23930, 264, 5176, 3073, 295, 264, 1032, 11, 550, 321, 727, 764, 16235, 382, 2207, 281, 574, 337, 264, 888, 23547, 47523, 300, 6805, 281, 264, 14573, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10001166179926708, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9477134628687054e-05}, {"id": 598, "seek": 456100, "start": 4578.0, "end": 4590.0, "text": " Like if we have a model that looks at the blueprints of a car and outputs the expected speed of the car, then we could use gradient ascent to look for the blueprints that correspond to the fastest.", "tokens": [50364, 4534, 295, 264, 565, 562, 321, 1322, 3479, 5201, 3652, 11, 456, 311, 572, 5824, 6367, 490, 641, 5598, 281, 641, 4846, 13, 50764, 50764, 759, 321, 3811, 257, 3287, 689, 321, 722, 1228, 364, 19618, 9284, 281, 915, 15743, 300, 19874, 512, 4707, 295, 264, 5598, 13, 51214, 51214, 1743, 498, 321, 362, 257, 2316, 300, 1542, 412, 264, 888, 23547, 47523, 295, 257, 1032, 293, 23930, 264, 5176, 3073, 295, 264, 1032, 11, 550, 321, 727, 764, 16235, 382, 2207, 281, 574, 337, 264, 888, 23547, 47523, 300, 6805, 281, 264, 14573, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10001166179926708, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9477134628687054e-05}, {"id": 599, "seek": 459000, "start": 4590.0, "end": 4606.0, "text": " Or for example, if we're designing a medicine, we could look for the molecular structure that we think is most likely to cure some form of cancer or the least likely to cause some kind of liver toxicity effect.", "tokens": [50364, 1610, 337, 1365, 11, 498, 321, 434, 14685, 257, 7195, 11, 321, 727, 574, 337, 264, 19046, 3877, 300, 321, 519, 307, 881, 3700, 281, 13698, 512, 1254, 295, 5592, 420, 264, 1935, 3700, 281, 3082, 512, 733, 295, 15019, 45866, 1802, 13, 51164, 51164, 440, 1154, 307, 300, 1564, 321, 722, 1228, 19618, 281, 574, 337, 613, 15743, 300, 19874, 264, 5598, 295, 264, 2316, 11, 264, 4846, 307, 572, 2854, 364, 6695, 6889, 490, 264, 912, 7316, 382, 321, 764, 300, 264, 2269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15914314058091905, "compression_ratio": 1.728744939271255, "no_speech_prob": 3.8526828575413674e-05}, {"id": 600, "seek": 459000, "start": 4606.0, "end": 4615.0, "text": " The problem is that once we start using optimization to look for these inputs that maximize the output of the model, the input is no longer an independent sample from the same distribution as we use that the channel.", "tokens": [50364, 1610, 337, 1365, 11, 498, 321, 434, 14685, 257, 7195, 11, 321, 727, 574, 337, 264, 19046, 3877, 300, 321, 519, 307, 881, 3700, 281, 13698, 512, 1254, 295, 5592, 420, 264, 1935, 3700, 281, 3082, 512, 733, 295, 15019, 45866, 1802, 13, 51164, 51164, 440, 1154, 307, 300, 1564, 321, 722, 1228, 19618, 281, 574, 337, 613, 15743, 300, 19874, 264, 5598, 295, 264, 2316, 11, 264, 4846, 307, 572, 2854, 364, 6695, 6889, 490, 264, 912, 7316, 382, 321, 764, 300, 264, 2269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15914314058091905, "compression_ratio": 1.728744939271255, "no_speech_prob": 3.8526828575413674e-05}, {"id": 601, "seek": 461500, "start": 4615.0, "end": 4633.0, "text": " The model is now guiding the process that generates the data. So we end up finding essentially adversarial examples.", "tokens": [50364, 440, 2316, 307, 586, 25061, 264, 1399, 300, 23815, 264, 1412, 13, 407, 321, 917, 493, 5006, 4476, 17641, 44745, 5110, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.13768918697650617, "compression_ratio": 1.2210526315789474, "no_speech_prob": 8.415962838626001e-06}, {"id": 602, "seek": 463300, "start": 4633.0, "end": 4645.0, "text": " The model telling us how we can improve the input, what we usually find in practice is that we've got an input that fools the model into thinking that the input corresponds to something great.", "tokens": [50364, 440, 2316, 3585, 505, 577, 321, 393, 3470, 264, 4846, 11, 437, 321, 2673, 915, 294, 3124, 307, 300, 321, 600, 658, 364, 4846, 300, 38625, 264, 2316, 666, 1953, 300, 264, 4846, 23249, 281, 746, 869, 13, 50964, 50964, 407, 321, 1116, 915, 13093, 300, 366, 588, 12786, 11, 457, 264, 2316, 7309, 436, 434, 588, 2107, 12, 1353, 47228, 13, 51214, 51214, 1610, 321, 1116, 915, 5163, 300, 366, 588, 2964, 11, 457, 264, 2316, 7309, 366, 588, 2370, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1484123903162339, "compression_ratio": 1.8205128205128205, "no_speech_prob": 1.6705173038644716e-05}, {"id": 603, "seek": 463300, "start": 4645.0, "end": 4650.0, "text": " So we'd find molecules that are very toxic, but the model thinks they're very non-toxic.", "tokens": [50364, 440, 2316, 3585, 505, 577, 321, 393, 3470, 264, 4846, 11, 437, 321, 2673, 915, 294, 3124, 307, 300, 321, 600, 658, 364, 4846, 300, 38625, 264, 2316, 666, 1953, 300, 264, 4846, 23249, 281, 746, 869, 13, 50964, 50964, 407, 321, 1116, 915, 13093, 300, 366, 588, 12786, 11, 457, 264, 2316, 7309, 436, 434, 588, 2107, 12, 1353, 47228, 13, 51214, 51214, 1610, 321, 1116, 915, 5163, 300, 366, 588, 2964, 11, 457, 264, 2316, 7309, 366, 588, 2370, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1484123903162339, "compression_ratio": 1.8205128205128205, "no_speech_prob": 1.6705173038644716e-05}, {"id": 604, "seek": 465000, "start": 4650.0, "end": 4676.0, "text": " And cars that are very slow, but the model thinks are very fast.", "tokens": [50364, 400, 5163, 300, 366, 588, 2964, 11, 457, 264, 2316, 7309, 366, 588, 2370, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22705138357062088, "compression_ratio": 1.0491803278688525, "no_speech_prob": 8.23283480713144e-05}, {"id": 605, "seek": 467600, "start": 4676.0, "end": 4684.0, "text": " The question is, here the frog class is boosted by going in either the positive or negative adversarial direction.", "tokens": [50364, 440, 1168, 307, 11, 510, 264, 17259, 1508, 307, 9194, 292, 538, 516, 294, 2139, 264, 3353, 420, 3671, 17641, 44745, 3513, 13, 50764, 50764, 400, 294, 512, 295, 264, 661, 9788, 411, 613, 11317, 11, 291, 500, 380, 483, 300, 1802, 689, 16390, 278, 17889, 766, 4728, 9194, 82, 264, 17641, 44745, 1508, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1327402147196107, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0004943745443597436}, {"id": 606, "seek": 467600, "start": 4684.0, "end": 4694.0, "text": " And in some of the other slides like these maps, you don't get that effect where subtracting epsilon off eventually boosts the adversarial class.", "tokens": [50364, 440, 1168, 307, 11, 510, 264, 17259, 1508, 307, 9194, 292, 538, 516, 294, 2139, 264, 3353, 420, 3671, 17641, 44745, 3513, 13, 50764, 50764, 400, 294, 512, 295, 264, 661, 9788, 411, 613, 11317, 11, 291, 500, 380, 483, 300, 1802, 689, 16390, 278, 17889, 766, 4728, 9194, 82, 264, 17641, 44745, 1508, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1327402147196107, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0004943745443597436}, {"id": 607, "seek": 469400, "start": 4694.0, "end": 4710.0, "text": " So what I'm going on is, I think I'm using larger epsilon here, and so you might eventually see that effect if I made these maps wider. I made them maps narrower because it's like quadratic time to build a 2D map and it's linear time to build a 1D cross section.", "tokens": [50364, 407, 437, 286, 478, 516, 322, 307, 11, 286, 519, 286, 478, 1228, 4833, 17889, 510, 11, 293, 370, 291, 1062, 4728, 536, 300, 1802, 498, 286, 1027, 613, 11317, 11842, 13, 286, 1027, 552, 11317, 46751, 570, 309, 311, 411, 37262, 565, 281, 1322, 257, 568, 35, 4471, 293, 309, 311, 8213, 565, 281, 1322, 257, 502, 35, 3278, 3541, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.1909537170872544, "compression_ratio": 1.4886363636363635, "no_speech_prob": 8.66435730131343e-05}, {"id": 608, "seek": 471000, "start": 4710.0, "end": 4725.0, "text": " So I just didn't afford the GPU time to make them maps quite as wide. I also think that this might just be a weird effect that happened randomly on this one example. It's not something that I remember being used to seeing a lot of the time.", "tokens": [50364, 407, 286, 445, 994, 380, 6157, 264, 18407, 565, 281, 652, 552, 11317, 1596, 382, 4874, 13, 286, 611, 519, 300, 341, 1062, 445, 312, 257, 3657, 1802, 300, 2011, 16979, 322, 341, 472, 1365, 13, 467, 311, 406, 746, 300, 286, 1604, 885, 1143, 281, 2577, 257, 688, 295, 264, 565, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.07505528132120769, "compression_ratio": 1.4457831325301205, "no_speech_prob": 2.336397665203549e-05}, {"id": 609, "seek": 472500, "start": 4725.0, "end": 4745.0, "text": " Most things that I observed don't happen perfectly consistently, but if they happen 80% of the time, then I'll put them in my slide. A lot of what we're doing is it's trying to figure out more or less what's going on. And so if we find that something happens 80% of the time, then I consider it to be the dominant phenomenon that we're trying to explain.", "tokens": [50364, 4534, 721, 300, 286, 13095, 500, 380, 1051, 6239, 14961, 11, 457, 498, 436, 1051, 4688, 4, 295, 264, 565, 11, 550, 286, 603, 829, 552, 294, 452, 4137, 13, 316, 688, 295, 437, 321, 434, 884, 307, 309, 311, 1382, 281, 2573, 484, 544, 420, 1570, 437, 311, 516, 322, 13, 400, 370, 498, 321, 915, 300, 746, 2314, 4688, 4, 295, 264, 565, 11, 550, 286, 1949, 309, 281, 312, 264, 15657, 14029, 300, 321, 434, 1382, 281, 2903, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.090520370838254, "compression_ratio": 1.6777251184834123, "no_speech_prob": 4.785145847563399e-06}, {"id": 610, "seek": 474500, "start": 4745.0, "end": 4755.0, "text": " After we've got a better explanation for that, then I might start to try to explain some of the weirder things that happen like the frog happening with negative epsilon.", "tokens": [50364, 2381, 321, 600, 658, 257, 1101, 10835, 337, 300, 11, 550, 286, 1062, 722, 281, 853, 281, 2903, 512, 295, 264, 321, 347, 1068, 721, 300, 1051, 411, 264, 17259, 2737, 365, 3671, 17889, 13, 50864, 51264], "temperature": 0.0, "avg_logprob": -0.17731862190442207, "compression_ratio": 1.3629032258064515, "no_speech_prob": 4.9225145630771294e-05}, {"id": 611, "seek": 475500, "start": 4755.0, "end": 4775.0, "text": " I didn't fully understand the question. It's about the dimensionality of the adversary.", "tokens": [50364, 286, 994, 380, 4498, 1223, 264, 1168, 13, 467, 311, 466, 264, 10139, 1860, 295, 264, 48222, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2768033201044256, "compression_ratio": 1.0875, "no_speech_prob": 0.00037325912853702903}, {"id": 612, "seek": 477500, "start": 4775.0, "end": 4788.0, "text": " Okay, so the question is how is the dimension of the adversary subspace related to the dimension of the input? And my answer is somewhat embarrassing, which is that we've only run this method on two data sets.", "tokens": [50364, 1033, 11, 370, 264, 1168, 307, 577, 307, 264, 10139, 295, 264, 48222, 2090, 17940, 4077, 281, 264, 10139, 295, 264, 4846, 30, 400, 452, 1867, 307, 8344, 17299, 11, 597, 307, 300, 321, 600, 787, 1190, 341, 3170, 322, 732, 1412, 6352, 13, 51014, 51014, 407, 321, 500, 380, 767, 362, 257, 665, 1558, 1939, 11, 457, 309, 311, 286, 519, 309, 311, 746, 1880, 281, 2979, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.17278398226385247, "compression_ratio": 1.5376884422110553, "no_speech_prob": 0.0002895263896789402}, {"id": 613, "seek": 477500, "start": 4788.0, "end": 4794.0, "text": " So we don't actually have a good idea yet, but it's I think it's something interesting to study.", "tokens": [50364, 1033, 11, 370, 264, 1168, 307, 577, 307, 264, 10139, 295, 264, 48222, 2090, 17940, 4077, 281, 264, 10139, 295, 264, 4846, 30, 400, 452, 1867, 307, 8344, 17299, 11, 597, 307, 300, 321, 600, 787, 1190, 341, 3170, 322, 732, 1412, 6352, 13, 51014, 51014, 407, 321, 500, 380, 767, 362, 257, 665, 1558, 1939, 11, 457, 309, 311, 286, 519, 309, 311, 746, 1880, 281, 2979, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.17278398226385247, "compression_ratio": 1.5376884422110553, "no_speech_prob": 0.0002895263896789402}, {"id": 614, "seek": 479400, "start": 4794.0, "end": 4810.0, "text": " If I remember correctly, my co-authors open source code, so you could probably run it on image net without too much trouble. I actually my contribution to that paper was in the week that I was unemployed between working at OpenAI and working at Google.", "tokens": [50364, 759, 286, 1604, 8944, 11, 452, 598, 12, 40198, 830, 1269, 4009, 3089, 11, 370, 291, 727, 1391, 1190, 309, 322, 3256, 2533, 1553, 886, 709, 5253, 13, 286, 767, 452, 13150, 281, 300, 3035, 390, 294, 264, 1243, 300, 286, 390, 34411, 1296, 1364, 412, 7238, 48698, 293, 1364, 412, 3329, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.15715754659552322, "compression_ratio": 1.4566473988439306, "no_speech_prob": 2.636824683577288e-05}, {"id": 615, "seek": 481000, "start": 4810.0, "end": 4825.0, "text": " So I had access to no GPUs, and I ran that experiment on my laptop on CPU. So it's only really small data sets.", "tokens": [50364, 407, 286, 632, 2105, 281, 572, 18407, 82, 11, 293, 286, 5872, 300, 5120, 322, 452, 10732, 322, 13199, 13, 407, 309, 311, 787, 534, 1359, 1412, 6352, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14083674550056458, "compression_ratio": 1.1326530612244898, "no_speech_prob": 2.0731564291054383e-05}, {"id": 616, "seek": 484000, "start": 4841.0, "end": 4848.0, "text": " So the question is, do we end up perturbing low confidence, clean examples to low confidence adversarial examples?", "tokens": [50414, 407, 264, 1168, 307, 11, 360, 321, 917, 493, 13269, 374, 4324, 2295, 6687, 11, 2541, 5110, 281, 2295, 6687, 17641, 44745, 5110, 30, 50764, 50764, 865, 11, 294, 3124, 321, 2673, 915, 300, 321, 393, 483, 588, 1090, 6687, 322, 264, 5598, 5110, 13, 51114, 51114, 1485, 551, 286, 1116, 2152, 300, 311, 257, 707, 857, 29466, 48314, 307, 300, 445, 1242, 264, 1465, 558, 322, 588, 867, 295, 264, 4846, 18668, 307, 1547, 281, 483, 257, 534, 2068, 4134, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1622525370398233, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.1869940459728241}, {"id": 617, "seek": 484000, "start": 4848.0, "end": 4855.0, "text": " Yeah, in practice we usually find that we can get very high confidence on the output examples.", "tokens": [50414, 407, 264, 1168, 307, 11, 360, 321, 917, 493, 13269, 374, 4324, 2295, 6687, 11, 2541, 5110, 281, 2295, 6687, 17641, 44745, 5110, 30, 50764, 50764, 865, 11, 294, 3124, 321, 2673, 915, 300, 321, 393, 483, 588, 1090, 6687, 322, 264, 5598, 5110, 13, 51114, 51114, 1485, 551, 286, 1116, 2152, 300, 311, 257, 707, 857, 29466, 48314, 307, 300, 445, 1242, 264, 1465, 558, 322, 588, 867, 295, 264, 4846, 18668, 307, 1547, 281, 483, 257, 534, 2068, 4134, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1622525370398233, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.1869940459728241}, {"id": 618, "seek": 484000, "start": 4855.0, "end": 4868.0, "text": " One thing I'd mention that's a little bit unintuitive is that just getting the sign right on very many of the input pixels is enough to get a really strong response.", "tokens": [50414, 407, 264, 1168, 307, 11, 360, 321, 917, 493, 13269, 374, 4324, 2295, 6687, 11, 2541, 5110, 281, 2295, 6687, 17641, 44745, 5110, 30, 50764, 50764, 865, 11, 294, 3124, 321, 2673, 915, 300, 321, 393, 483, 588, 1090, 6687, 322, 264, 5598, 5110, 13, 51114, 51114, 1485, 551, 286, 1116, 2152, 300, 311, 257, 707, 857, 29466, 48314, 307, 300, 445, 1242, 264, 1465, 558, 322, 588, 867, 295, 264, 4846, 18668, 307, 1547, 281, 483, 257, 534, 2068, 4134, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1622525370398233, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.1869940459728241}, {"id": 619, "seek": 486800, "start": 4868.0, "end": 4878.0, "text": " So the angle between the weight vector matters a lot more than the exact coordinates in high dimensional systems.", "tokens": [50364, 407, 264, 5802, 1296, 264, 3364, 8062, 7001, 257, 688, 544, 813, 264, 1900, 21056, 294, 1090, 18795, 3652, 13, 50864, 50864, 4402, 300, 652, 1547, 2020, 30, 865, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15228666978723862, "compression_ratio": 1.2758620689655173, "no_speech_prob": 8.246996912930626e-06}, {"id": 620, "seek": 486800, "start": 4878.0, "end": 4890.0, "text": " Does that make enough sense? Yeah.", "tokens": [50364, 407, 264, 5802, 1296, 264, 3364, 8062, 7001, 257, 688, 544, 813, 264, 1900, 21056, 294, 1090, 18795, 3652, 13, 50864, 50864, 4402, 300, 652, 1547, 2020, 30, 865, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15228666978723862, "compression_ratio": 1.2758620689655173, "no_speech_prob": 8.246996912930626e-06}, {"id": 621, "seek": 489000, "start": 4890.0, "end": 4901.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.4668749173482259, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00592405442148447}], "language": "en"}